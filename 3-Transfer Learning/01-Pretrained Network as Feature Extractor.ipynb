{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a789dd2e",
   "metadata": {},
   "source": [
    "# Pretrained Network | Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5de52",
   "metadata": {},
   "source": [
    "El objetivo de esta sección es ver como implementar transfer-learning cuando tenemos una cantidad de datos disponible limitada, pero el dominio del objetivo es similar a la red que queremos usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b9ab3",
   "metadata": {},
   "source": [
    "Congelaremos la parte de extracción de features de la red VGG16 entrenada en el dataset IMAGENET, y agregaremos nuestro propio clasificador, para después reentrenar la red con nuestro pequeño dataset.\n",
    "\n",
    "Otro de los objetivos será mostrar como preprocesar nuestros datos para conseguir entrenar una red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035397b9",
   "metadata": {},
   "source": [
    "Notesé que ya no calculamos MEAN y STD para normalizar aquí, si no que usamos los valores estandard (calculados por la comunidad) para rapidez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bb1b2",
   "metadata": {},
   "source": [
    "Además no usaremos data-augmentation aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bb148fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "[0.485, 0.456, 0.406]\n",
      "[0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "#Obtenemos los pesos de la red VGG16 entrenada en el dataset IMAGENET\n",
    "\n",
    "weights = models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "#Como vemos aqui observamos los valores a los que nuestro dataset debe ser normalizado\n",
    "# y preprocesado para que la red pueda usarlo.\n",
    "\n",
    "preprocess = weights.transforms()\n",
    "print(preprocess)\n",
    "IMAGENET_MEAN = preprocess.mean\n",
    "IMAGENET_STD  = preprocess.std\n",
    "\n",
    "print(IMAGENET_MEAN)\n",
    "print(IMAGENET_STD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb4ec0",
   "metadata": {},
   "source": [
    "Como vemos arriba:\n",
    "\n",
    "### crop_size=[224]\n",
    "\n",
    "Es el tamaño final de la imagen que el modelo usa como entrada: 224×224 píxeles.\n",
    "Se consigue aplicando transforms.CenterCrop(224) después del resize.\n",
    "\n",
    "### resize_size=[256]\n",
    "\n",
    "Antes del recorte, la imagen se redimensiona de forma que su lado más corto mida 256 píxeles, manteniendo su proporción.\n",
    "Esto equivale a transforms.Resize(256)\n",
    "\n",
    "También observamos la media y desviación estandard que usaremos para la normalización.\n",
    "\n",
    "Este pipeline que hemos visto arriba, tenemos que replicarlo en nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b1f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replicamos el pipeline de preprocesado de la red VGG16\n",
    "base_tf = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR), # Redimensiona la imagen\n",
    "    transforms.CenterCrop(224), # Recorta la imagen\n",
    "    transforms.ToTensor(), # Convierte la imagen a un tensor\n",
    "    transforms.Normalize(weights.transforms().mean, weights.transforms().std), # Normaliza la imagen\n",
    "])\n",
    "\n",
    "root = \"./data/01\"\n",
    "train_ds = datasets.ImageFolder(f\"{root}/train\", transform=base_tf)\n",
    "valid_ds = datasets.ImageFolder(f\"{root}/valid\", transform=base_tf)\n",
    "test_ds  = datasets.ImageFolder(f\"{root}/test\",  transform=base_tf)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3b732c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 138,357,544 | Frozen: 0 | Total: 138,357,544\n"
     ]
    }
   ],
   "source": [
    "# Definimos GPU o CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Obtenemos el numero de clases de nuestro dataset\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "# Definimos la base de la red VGG16\n",
    "base = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "\n",
    "#Vamos a imprimir los parametros de la red VGG16\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1355174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 123,642,856 | Frozen: 14,714,688 | Total: 138,357,544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Congelamos los parametros de la red VGG16\n",
    "# Aqui le decimos que no calcule los gradientes para los parametros de la red VGG16\n",
    "# Esto es importante, ya que si no lo hacemos, el modelo se entrenará desde cero\n",
    "# y no aprovechará los pesos ya entrenados de la red VGG16-Imagenet\n",
    "for p in base.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# Vamos a volver a calcular los parametros de la red VGG16, para ver que se han congelado\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cd80f",
   "metadata": {},
   "source": [
    "Como vemos, los parametros entrenables, se han reducido, a continuación, vamos a quedarnos con toda la red, salvo el clasificador, ya que crearemos uno para nuestro caso de uso. (2 clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a13e0",
   "metadata": {},
   "source": [
    "Como veremos en el output, al quitar el clasificador, ya no tenemos parametros entrenables, los que quedan de las capas de convolución estan congelados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff379283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0 | Frozen: 14,714,688 | Total: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.59\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 275.29\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "backbone = nn.Sequential(*(list(base.children())[:-1])) \n",
    "\n",
    "# Vemos que los parametros entrenables se han reducido a 0\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in backbone.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "print(summary(backbone, (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25e01e",
   "metadata": {},
   "source": [
    "Para volver a tener parametros entreables desde 0, tendremos que añadir nuestra propia capa de clasificación a la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definimos la capa de pooling y el clasificador\n",
    "print()\n",
    "\n",
    "head = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512*7*7, 64), # 512 es el numero de features de la ultima capa de convolución y 7*7 es el tamaño de la imagen de entrada.\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(64, num_classes)\n",
    ")\n",
    "\n",
    "# Movemos la red a GPU o CPU\n",
    "model = base.to(device)\n",
    "\n",
    "# Obtenemos el numero de parametros entrenables y totales\n",
    "sum_reqgrad = sum(p.requires_grad for p in model.parameters())\n",
    "sum_total   = sum(1 for _ in model.parameters())\n",
    "print(f\"trainables: {sum_reqgrad}/{sum_total}\")\n",
    "\n",
    "# Definimos la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
