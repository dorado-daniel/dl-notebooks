{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a789dd2e",
   "metadata": {},
   "source": [
    "# Pretrained Network | Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5de52",
   "metadata": {},
   "source": [
    "El objetivo de esta sección es ver como implementar transfer-learning cuando tenemos una cantidad de datos disponible limitada, pero el dominio del objetivo es similar a la red que queremos usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b9ab3",
   "metadata": {},
   "source": [
    "Congelaremos la parte de extracción de features de la red VGG16 entrenada en el dataset IMAGENET, y agregaremos nuestro propio clasificador, para después reentrenar la red con nuestro pequeño dataset.\n",
    "\n",
    "Otro de los objetivos será mostrar como preprocesar nuestros datos para conseguir entrenar una red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035397b9",
   "metadata": {},
   "source": [
    "Notesé que ya no calculamos MEAN y STD para normalizar aquí, si no que usamos los valores estandard (calculados por la comunidad) para rapidez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bb1b2",
   "metadata": {},
   "source": [
    "Además no usaremos data-augmentation aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7bb148fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "[0.485, 0.456, 0.406]\n",
      "[0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "#Obtenemos los pesos de la red VGG16 entrenada en el dataset IMAGENET\n",
    "\n",
    "weights = models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "#Como vemos aqui observamos los valores a los que nuestro dataset debe ser normalizado\n",
    "# y preprocesado para que la red pueda usarlo.\n",
    "\n",
    "preprocess = weights.transforms()\n",
    "print(preprocess)\n",
    "IMAGENET_MEAN = preprocess.mean\n",
    "IMAGENET_STD  = preprocess.std\n",
    "\n",
    "print(IMAGENET_MEAN)\n",
    "print(IMAGENET_STD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb4ec0",
   "metadata": {},
   "source": [
    "Como vemos arriba:\n",
    "\n",
    "### crop_size=[224]\n",
    "\n",
    "Es el tamaño final de la imagen que el modelo usa como entrada: 224×224 píxeles.\n",
    "Se consigue aplicando transforms.CenterCrop(224) después del resize.\n",
    "\n",
    "### resize_size=[256]\n",
    "\n",
    "Antes del recorte, la imagen se redimensiona de forma que su lado más corto mida 256 píxeles, manteniendo su proporción.\n",
    "Esto equivale a transforms.Resize(256)\n",
    "\n",
    "También observamos la media y desviación estandard que usaremos para la normalización.\n",
    "\n",
    "Este pipeline que hemos visto arriba, tenemos que replicarlo en nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94b1f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replicamos el pipeline de preprocesado de la red VGG16\n",
    "base_tf = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR), # Redimensiona la imagen\n",
    "    transforms.CenterCrop(224), # Recorta la imagen\n",
    "    transforms.ToTensor(), # Convierte la imagen a un tensor\n",
    "    transforms.Normalize(weights.transforms().mean, weights.transforms().std), # Normaliza la imagen\n",
    "])\n",
    "\n",
    "root = \"./data/01\"\n",
    "train_ds = datasets.ImageFolder(f\"{root}/train\", transform=base_tf)\n",
    "valid_ds = datasets.ImageFolder(f\"{root}/valid\", transform=base_tf)\n",
    "test_ds  = datasets.ImageFolder(f\"{root}/test\",  transform=base_tf)\n",
    "\n",
    "loader_train = DataLoader(train_ds, batch_size=len(train_ds), shuffle=False)                       \n",
    "loader_val   = DataLoader(valid_ds, batch_size=len(valid_ds), shuffle=False)\n",
    "loader_test  = DataLoader(test_ds,  batch_size=len(test_ds),  shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3b732c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 138,357,544 | Frozen: 0 | Total: 138,357,544\n"
     ]
    }
   ],
   "source": [
    "# Definimos GPU o CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Obtenemos el numero de clases de nuestro dataset\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "# Definimos la base de la red VGG16\n",
    "base = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "\n",
    "#Vamos a imprimir los parametros de la red VGG16\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1355174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 123,642,856 | Frozen: 14,714,688 | Total: 138,357,544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Congelamos los parametros de la red VGG16\n",
    "# Aqui le decimos que no calcule los gradientes para los parametros de la red VGG16\n",
    "# Esto es importante, ya que si no lo hacemos, el modelo se entrenará desde cero\n",
    "# y no aprovechará los pesos ya entrenados de la red VGG16-Imagenet\n",
    "for p in base.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# Vamos a volver a calcular los parametros de la red VGG16, para ver que se han congelado\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cd80f",
   "metadata": {},
   "source": [
    "Como vemos, los parametros entrenables, se han reducido, a continuación, vamos a quedarnos con toda la red, salvo el clasificador, ya que crearemos uno para nuestro caso de uso. (2 clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a13e0",
   "metadata": {},
   "source": [
    "Como veremos en el output, al quitar el clasificador, ya no tenemos parametros entrenables, los que quedan de las capas de convolución estan congelados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff379283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0 | Frozen: 14,714,688 | Total: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.59\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 275.29\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "backbone = nn.Sequential(*(list(base.children())[:-1])) \n",
    "\n",
    "# Vemos que los parametros entrenables se han reducido a 0\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in backbone.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "print(summary(backbone, (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25e01e",
   "metadata": {},
   "source": [
    "Para volver a tener parametros entreables desde 0, tendremos que añadir nuestra propia capa de clasificación a la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "721a7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos el clasificador\n",
    "head = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512*7*7, 64), # 512 es el numero de features de la ultima capa de convolución y 7*7 es el tamaño de la imagen de entrada.\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(64, num_classes)\n",
    "    #No añadimos activacion al final, ya que usaremos CrossEntropyLoss que aplica la activacion softmax.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada15910",
   "metadata": {},
   "source": [
    "Ahora construimos backbone + clasificador\n",
    "\n",
    "Notemos que ahora volvemos a tener parametros entrenables, pero estos son inferiores a la red completa antes de descabezarla, ya que tenemos solo 2 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "859a006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "          Flatten-33                [-1, 25088]               0\n",
      "           Linear-34                   [-1, 64]       1,605,696\n",
      "             ReLU-35                   [-1, 64]               0\n",
      "      BatchNorm1d-36                   [-1, 64]             128\n",
      "          Dropout-37                   [-1, 64]               0\n",
      "           Linear-38                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 16,320,642\n",
      "Trainable params: 1,605,954\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 62.26\n",
      "Estimated Total Size (MB): 281.61\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = VGG16FeatureExtractor(backbone, head).to(device)\n",
    "\n",
    "print(summary(model, (3, 224, 224)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f2afb",
   "metadata": {},
   "source": [
    "Vamos a definir optimizar y funcion de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf255571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16FeatureExtractor(\n",
       "  (backbone): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=25088, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se optimiza solo el clasificador, entramos como parametro.\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "NUM_CLASSES = 2 \n",
    "TENSORBOARD_EXP = f\"runs/vgg16_imgnet_2cls_featextractor_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "EPOCHS = 20\n",
    "\n",
    "#Se optimiza solo el clasificador, entramos como parametro.\n",
    "optimizer = optim.Adam(model.head.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "785de030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/20 | Train Loss: 0.2878 | Train Acc: 0.9307 | Val Loss: 0.3224 | Val Acc: 0.9320\n",
      "Época 2/20 | Train Loss: 0.1145 | Train Acc: 0.9950 | Val Loss: 0.2323 | Val Acc: 0.9612\n",
      "Época 3/20 | Train Loss: 0.0744 | Train Acc: 1.0000 | Val Loss: 0.1772 | Val Acc: 0.9612\n",
      "Época 4/20 | Train Loss: 0.0567 | Train Acc: 1.0000 | Val Loss: 0.1447 | Val Acc: 0.9612\n",
      "Época 5/20 | Train Loss: 0.0404 | Train Acc: 1.0000 | Val Loss: 0.1245 | Val Acc: 0.9612\n",
      "Época 6/20 | Train Loss: 0.0347 | Train Acc: 1.0000 | Val Loss: 0.1121 | Val Acc: 0.9612\n",
      "Época 7/20 | Train Loss: 0.0243 | Train Acc: 1.0000 | Val Loss: 0.1040 | Val Acc: 0.9709\n",
      "Época 8/20 | Train Loss: 0.0207 | Train Acc: 1.0000 | Val Loss: 0.0988 | Val Acc: 0.9709\n",
      "Época 9/20 | Train Loss: 0.0204 | Train Acc: 1.0000 | Val Loss: 0.0954 | Val Acc: 0.9709\n",
      "Época 10/20 | Train Loss: 0.0182 | Train Acc: 1.0000 | Val Loss: 0.0931 | Val Acc: 0.9709\n",
      "Época 11/20 | Train Loss: 0.0184 | Train Acc: 1.0000 | Val Loss: 0.0917 | Val Acc: 0.9709\n",
      "Época 12/20 | Train Loss: 0.0153 | Train Acc: 1.0000 | Val Loss: 0.0908 | Val Acc: 0.9709\n",
      "Época 13/20 | Train Loss: 0.0148 | Train Acc: 1.0000 | Val Loss: 0.0902 | Val Acc: 0.9709\n",
      "Época 14/20 | Train Loss: 0.0128 | Train Acc: 1.0000 | Val Loss: 0.0899 | Val Acc: 0.9709\n",
      "Época 15/20 | Train Loss: 0.0122 | Train Acc: 1.0000 | Val Loss: 0.0900 | Val Acc: 0.9709\n",
      "Época 16/20 | Train Loss: 0.0106 | Train Acc: 1.0000 | Val Loss: 0.0902 | Val Acc: 0.9709\n",
      "Época 17/20 | Train Loss: 0.0101 | Train Acc: 1.0000 | Val Loss: 0.0906 | Val Acc: 0.9612\n",
      "Época 18/20 | Train Loss: 0.0118 | Train Acc: 1.0000 | Val Loss: 0.0910 | Val Acc: 0.9612\n",
      "Época 19/20 | Train Loss: 0.0105 | Train Acc: 1.0000 | Val Loss: 0.0913 | Val Acc: 0.9515\n",
      "Época 20/20 | Train Loss: 0.0095 | Train Acc: 1.0000 | Val Loss: 0.0917 | Val Acc: 0.9515\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "optimizer = torch.optim.Adam(model.head.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset(); recall_metric.reset(); f1_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc); val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Época {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",   val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc, epoch)\n",
    "    writer.add_scalar(\"Acc/val\",    val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()\n",
    "\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(optimizer.param_groups[0]['lr']),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da707d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluamos el modelo en el conjunto de test\n",
    "all_preds, all_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in loader_test:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x).argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "# Calculamos la matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "classes = train_ds.classes  # ['cat', 'dog']\n",
    "\n",
    "# Mostramos la matriz como un mapa de calor\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicción del modelo\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.title(\"Matriz de confusión - Clasificación de gatos y perros\")\n",
    "plt.show()\n",
    "\n",
    "# Interpretación explícita\n",
    "print(\"\\nINTERPRETACIÓN:\")\n",
    "print(f\"{cm[0,0]} gatos clasificados correctamente como gatos (True Positives - cats)\")\n",
    "print(f\"{cm[0,1]} gatos clasificados incorrectamente como perros (False Negatives - cats)\")\n",
    "print(f\"{cm[1,0]} perros clasificados incorrectamente como gatos (False Positives - dogs)\")\n",
    "print(f\"{cm[1,1]} perros clasificados correctamente como perros (True Positives - dogs)\")\n",
    "\n",
    "# Métricas derivadas por clase (para reforzar comprensión)\n",
    "total_cats = cm[0].sum()\n",
    "total_dogs = cm[1].sum()\n",
    "acc_cat = cm[0,0] / total_cats\n",
    "acc_dog = cm[1,1] / total_dogs\n",
    "\n",
    "print(f\"\\nPrecisión gatos: {acc_cat:.3f} ({cm[0,0]}/{total_cats})\")\n",
    "print(f\"Precisión perros: {acc_dog:.3f} ({cm[1,1]}/{total_dogs})\")\n",
    "print(f\"Accuracy global: {(cm.trace() / cm.sum()):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
