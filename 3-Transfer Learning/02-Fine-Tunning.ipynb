{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159fc101",
   "metadata": {},
   "source": [
    "# Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6f321",
   "metadata": {},
   "source": [
    "Cuando tenemos pocos datos, y además no hay una red especifica que comparta mucho dominio con nuestro caso de uso, podemos hacer fine-tuning a una red ya existente.\n",
    "\n",
    "El proceso de finetuning consiste en hacer freeze de ciertas capas, y entrenar otras, además de entrenar la cabeza.\n",
    "\n",
    "Dependiendo de que características queramos quedarnos (mas o menos generales) y del grado de dominio que compartamos entre la red source y nuestro target, haremos freeze de más o menos capas.\n",
    "\n",
    "En general este es un proceso iterativo, prueba y error.\n",
    "\n",
    "Las capas iniciales capturan patrones genéricos (bordes, texturas), útiles para casi cualquier tarea.\n",
    "\n",
    "Las capas profundas capturan rasgos específicos del dataset original, por lo que son las que suele convenir reajustar cuando cambias de dominio.\n",
    "\n",
    "El fine-tuning siempre incluye:\n",
    "\n",
    "-  Sustituir la cabeza (head) por una nueva adaptada al nº de clases objetivo.\n",
    "\n",
    "- Decidir qué capas del backbone se reentrenan y cuáles se congelan.\n",
    "\n",
    "- Usar una tasa de aprendizaje pequeña (a menudo 10× menor que en entrenamiento desde cero) para no destruir pesos    preentrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8244a6",
   "metadata": {},
   "source": [
    "Empecemos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b4aa2",
   "metadata": {},
   "source": [
    "Como hicimos antes, cargamos los pesos de la red VGG16 entranada con IMGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbf31d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "[0.485, 0.456, 0.406]\n",
      "[0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "#Obtenemos los pesos de la red VGG16 entrenada en el dataset IMAGENET\n",
    "\n",
    "weights = models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "#Como vemos aqui observamos los valores a los que nuestro dataset debe ser normalizado\n",
    "# y preprocesado para que la red pueda usarlo.\n",
    "\n",
    "preprocess = weights.transforms()\n",
    "print(preprocess)\n",
    "IMAGENET_MEAN = preprocess.mean\n",
    "IMAGENET_STD  = preprocess.std\n",
    "\n",
    "print(IMAGENET_MEAN)\n",
    "print(IMAGENET_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6118e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replicamos el pipeline de preprocesado de la red VGG16\n",
    "base_tf = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR), # Redimensiona la imagen\n",
    "    transforms.CenterCrop(224), # Recorta la imagen\n",
    "    transforms.ToTensor(), # Convierte la imagen a un tensor\n",
    "    transforms.Normalize(weights.transforms().mean, weights.transforms().std), # Normaliza la imagen\n",
    "])\n",
    "\n",
    "root = \"./data/02\"\n",
    "train_ds = datasets.ImageFolder(f\"{root}/train\", transform=base_tf)\n",
    "valid_ds = datasets.ImageFolder(f\"{root}/valid\", transform=base_tf)\n",
    "test_ds  = datasets.ImageFolder(f\"{root}/test\",  transform=base_tf)\n",
    "\n",
    "loader_train = DataLoader(train_ds, batch_size=64, shuffle=True)                       \n",
    "loader_val   = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
    "loader_test  = DataLoader(test_ds,  batch_size=64,  shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9bc0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Trainable: 138,357,544 | Frozen: 0 | Total: 138,357,544\n"
     ]
    }
   ],
   "source": [
    "# Definimos GPU o CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Obtenemos el numero de clases de nuestro dataset\n",
    "num_classes = len(train_ds.classes)\n",
    "print(num_classes)\n",
    "\n",
    "# Definimos la base de la red VGG16\n",
    "base = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "\n",
    "#Vamos a imprimir los parametros de la red VGG16\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5be0b",
   "metadata": {},
   "source": [
    "Vamos a congelar las capas de la red VGG16, quitar el head y sustituir la última capa avg pool 7×7 por una avg pool 1×1.\n",
    "Esto reduce el número de parámetros de la cabeza y, por tanto, la probabilidad de overfitting.\n",
    "\n",
    "En el ejercicio anterior, sin este ajuste, se observó un leve overfitting; en este caso, al afinar más capas del backbone, aplicamos este cambio para mantener el modelo más controlado y estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "541be12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 123,642,856 | Frozen: 14,714,688 | Total: 138,357,544\n",
      "Trainable: 0 | Frozen: 14,714,688 | Total: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.40\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 275.10\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Congelamos los parametros de la red VGG16\n",
    "# Aqui le decimos que no calcule los gradientes para los parametros de la red VGG16\n",
    "# Esto es importante, ya que si no lo hacemos, el modelo se entrenará desde cero\n",
    "# y no aprovechará los pesos ya entrenados de la red VGG16-Imagenet\n",
    "for p in base.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# Vamos a volver a calcular los parametros de la red VGG16, para ver que se han congelado\n",
    "# Calculamos parámetros\n",
    "trainable_params = sum(p.numel() for p in base.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in base.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "\n",
    "#Quitamos el head de la red VGG16\n",
    "backbone = nn.Sequential(*(list(base.children())[:-1]))\n",
    "\n",
    "# Modificamos la ultima capa de la red para que sea una capa global average pooling\n",
    "backbone[-1] = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# Vemos que los parametros entrenables se han reducido a 0\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in backbone.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "print(summary(backbone, (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b66bf",
   "metadata": {},
   "source": [
    "Vamos a imprimir los indices y los tipos de las capas que tenemos para saber cuales tenemos que descongelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce539900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00: Conv2d               | requires_grad=False\n",
      "01: ReLU                 | requires_grad=False\n",
      "02: Conv2d               | requires_grad=False\n",
      "03: ReLU                 | requires_grad=False\n",
      "04: MaxPool2d            | requires_grad=False\n",
      "05: Conv2d               | requires_grad=False\n",
      "06: ReLU                 | requires_grad=False\n",
      "07: Conv2d               | requires_grad=False\n",
      "08: ReLU                 | requires_grad=False\n",
      "09: MaxPool2d            | requires_grad=False\n",
      "10: Conv2d               | requires_grad=False\n",
      "11: ReLU                 | requires_grad=False\n",
      "12: Conv2d               | requires_grad=False\n",
      "13: ReLU                 | requires_grad=False\n",
      "14: Conv2d               | requires_grad=False\n",
      "15: ReLU                 | requires_grad=False\n",
      "16: MaxPool2d            | requires_grad=False\n",
      "17: Conv2d               | requires_grad=False\n",
      "18: ReLU                 | requires_grad=False\n",
      "19: Conv2d               | requires_grad=False\n",
      "20: ReLU                 | requires_grad=False\n",
      "21: Conv2d               | requires_grad=False\n",
      "22: ReLU                 | requires_grad=False\n",
      "23: MaxPool2d            | requires_grad=False\n",
      "24: Conv2d               | requires_grad=False\n",
      "25: ReLU                 | requires_grad=False\n",
      "26: Conv2d               | requires_grad=False\n",
      "27: ReLU                 | requires_grad=False\n",
      "28: Conv2d               | requires_grad=False\n",
      "29: ReLU                 | requires_grad=False\n",
      "30: MaxPool2d            | requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base.features):\n",
    "    # comprobamos si alguno de los parámetros de la capa tiene gradiente activado\n",
    "    requires_grad = any(p.requires_grad for p in layer.parameters())\n",
    "    print(f\"{i:02d}: {layer.__class__.__name__:<20} | requires_grad={requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da8a76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00: Conv2d               | requires_grad=False\n",
      "01: ReLU                 | requires_grad=False\n",
      "02: Conv2d               | requires_grad=False\n",
      "03: ReLU                 | requires_grad=False\n",
      "04: MaxPool2d            | requires_grad=False\n",
      "05: Conv2d               | requires_grad=False\n",
      "06: ReLU                 | requires_grad=False\n",
      "07: Conv2d               | requires_grad=False\n",
      "08: ReLU                 | requires_grad=False\n",
      "09: MaxPool2d            | requires_grad=False\n",
      "10: Conv2d               | requires_grad=False\n",
      "11: ReLU                 | requires_grad=False\n",
      "12: Conv2d               | requires_grad=False\n",
      "13: ReLU                 | requires_grad=False\n",
      "14: Conv2d               | requires_grad=False\n",
      "15: ReLU                 | requires_grad=False\n",
      "16: MaxPool2d            | requires_grad=False\n",
      "17: Conv2d               | requires_grad=False\n",
      "18: ReLU                 | requires_grad=False\n",
      "19: Conv2d               | requires_grad=False\n",
      "20: ReLU                 | requires_grad=False\n",
      "21: Conv2d               | requires_grad=False\n",
      "22: ReLU                 | requires_grad=False\n",
      "23: MaxPool2d            | requires_grad=False\n",
      "24: Conv2d               | requires_grad=True\n",
      "25: ReLU                 | requires_grad=False\n",
      "26: Conv2d               | requires_grad=True\n",
      "27: ReLU                 | requires_grad=False\n",
      "28: Conv2d               | requires_grad=True\n",
      "29: ReLU                 | requires_grad=False\n",
      "30: MaxPool2d            | requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# Descongelamos las últimas 3 capas convolucionales  (solo esas 3 tienen pesos: block5_conv1-3)\n",
    "for idx in [24, 26, 28]:\n",
    "    for p in backbone[0][idx].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "for i, layer in enumerate(base.features):\n",
    "    # comprobamos si alguno de los parámetros de la capa tiene gradiente activado\n",
    "    requires_grad = any(p.requires_grad for p in layer.parameters())\n",
    "    print(f\"{i:02d}: {layer.__class__.__name__:<20} | requires_grad={requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e66bf624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 7,079,424 | Frozen: 7,635,264 | Total: 14,714,688\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos los parametros de la red VGG16 una vez descongeladas las 3 capas convolucionales\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in backbone.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81b870",
   "metadata": {},
   "source": [
    "Definamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "feea110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos el clasificador\n",
    "head = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, 10), # 512 es el numero de features de la ultima capa de convolución y 1*1 es el tamaño de la imagen de entrada.\n",
    "                        #En este caso no tiene sentido poner 1*1 ya que el resultado sería el mismo.\n",
    "    #No añadimos activacion al final, ya que usaremos CrossEntropyLoss que aplica la activacion softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0fc95",
   "metadata": {},
   "source": [
    "Juntemos ahora el backbone + el head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1ce9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "          Flatten-33                  [-1, 512]               0\n",
      "           Linear-34                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 7,084,554\n",
      "Non-trainable params: 7,635,264\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.40\n",
      "Params size (MB): 56.15\n",
      "Estimated Total Size (MB): 275.13\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class VGG16FineTunning(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = VGG16FineTunning(backbone, head).to(device)\n",
    "\n",
    "print(summary(model, (3, 224, 224)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e082a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 7,084,554 | Frozen: 7,635,264 | Total: 14,719,818\n",
      "['backbone.0.24.weight', 'backbone.0.24.bias', 'backbone.0.26.weight', 'backbone.0.26.bias', 'backbone.0.28.weight', 'backbone.0.28.bias', 'head.1.weight', 'head.1.bias']\n"
     ]
    }
   ],
   "source": [
    "#Se optimiza solo las capas no congeladas + el clasificador.\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 20\n",
    "TENSORBOARD_EXP = f\"runs/vgg16_imgnet_10cls_finetunning_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Seleccionamos los parametros que se van a optimizar, solo required_grad = True\n",
    "params = (p for p in model.parameters() if p.requires_grad==True)\n",
    "\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "frozen_params    = total_params - trainable_params\n",
    "print(f\"Trainable: {trainable_params:,} | Frozen: {frozen_params:,} | Total: {total_params:,}\")\n",
    "\n",
    "print([n for n, p in model.named_parameters() if p.requires_grad][:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "301bf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/20 | Train Loss: 1.5310 | Train Acc: 0.5800 | Val Loss: 0.5654 | Val Acc: 0.9233\n",
      "Época 2/20 | Train Loss: 0.3028 | Train Acc: 0.9352 | Val Loss: 0.2106 | Val Acc: 0.9500\n",
      "Época 3/20 | Train Loss: 0.1056 | Train Acc: 0.9778 | Val Loss: 0.1552 | Val Acc: 0.9733\n",
      "Época 4/20 | Train Loss: 0.0490 | Train Acc: 0.9924 | Val Loss: 0.1108 | Val Acc: 0.9733\n",
      "Época 5/20 | Train Loss: 0.0245 | Train Acc: 0.9965 | Val Loss: 0.1030 | Val Acc: 0.9700\n",
      "Época 6/20 | Train Loss: 0.0143 | Train Acc: 0.9994 | Val Loss: 0.0840 | Val Acc: 0.9900\n",
      "Época 7/20 | Train Loss: 0.0079 | Train Acc: 1.0000 | Val Loss: 0.0891 | Val Acc: 0.9833\n",
      "Época 8/20 | Train Loss: 0.0051 | Train Acc: 1.0000 | Val Loss: 0.0861 | Val Acc: 0.9867\n",
      "Época 9/20 | Train Loss: 0.0036 | Train Acc: 1.0000 | Val Loss: 0.0862 | Val Acc: 0.9900\n",
      "Época 10/20 | Train Loss: 0.0029 | Train Acc: 1.0000 | Val Loss: 0.0880 | Val Acc: 0.9900\n",
      "Época 11/20 | Train Loss: 0.0021 | Train Acc: 1.0000 | Val Loss: 0.0896 | Val Acc: 0.9900\n",
      "Época 12/20 | Train Loss: 0.0017 | Train Acc: 1.0000 | Val Loss: 0.0861 | Val Acc: 0.9900\n",
      "Época 13/20 | Train Loss: 0.0015 | Train Acc: 1.0000 | Val Loss: 0.0856 | Val Acc: 0.9900\n",
      "Época 14/20 | Train Loss: 0.0012 | Train Acc: 1.0000 | Val Loss: 0.0871 | Val Acc: 0.9900\n",
      "Época 15/20 | Train Loss: 0.0010 | Train Acc: 1.0000 | Val Loss: 0.0893 | Val Acc: 0.9900\n",
      "Época 16/20 | Train Loss: 0.0009 | Train Acc: 1.0000 | Val Loss: 0.0868 | Val Acc: 0.9900\n",
      "Época 17/20 | Train Loss: 0.0008 | Train Acc: 1.0000 | Val Loss: 0.0881 | Val Acc: 0.9900\n",
      "Época 18/20 | Train Loss: 0.0007 | Train Acc: 1.0000 | Val Loss: 0.0891 | Val Acc: 0.9900\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m loss = criterion(logits, labels)\n\u001b[32m     34\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m running_loss += loss.item() * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m preds = logits.argmax(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/dlvs/lib/python3.12/site-packages/torch/optim/optimizer.py:502\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28mself\u001b[39m = cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    501\u001b[39m profile_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    505\u001b[39m         _global_optimizer_pre_hooks.values(),\n\u001b[32m    506\u001b[39m         \u001b[38;5;28mself\u001b[39m._optimizer_step_pre_hooks.values(),\n\u001b[32m    507\u001b[39m     ):\n\u001b[32m    508\u001b[39m         result = pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/dlvs/lib/python3.12/site-packages/torch/autograd/profiler.py:780\u001b[39m, in \u001b[36mrecord_function.__init__\u001b[39m\u001b[34m(self, name, args)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28mself\u001b[39m.run_callbacks_on_exit: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# TODO: TorchScript ignores standard type annotation here\u001b[39;00m\n\u001b[32m    778\u001b[39m \u001b[38;5;66;03m# self.record: Optional[\"torch.classes.profiler._RecordFunction\"] = None\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[38;5;28mself\u001b[39m.record = torch.jit.annotate(\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     \u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch.classes.profiler._RecordFunction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    781\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/typing.py:392\u001b[39m, in \u001b[36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    389\u001b[39m _cleanups.append(cache.cache_clear)\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m cache\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    395\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _caches[func](*args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset(); recall_metric.reset(); f1_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc); val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Época {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",   val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc, epoch)\n",
    "    writer.add_scalar(\"Acc/val\",    val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()\n",
    "\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(optimizer.param_groups[0]['lr']),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in loader_test:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x).argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(train_ds.classes))))\n",
    "classes = train_ds.classes\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.title(\"Matriz de confusión - Proyecto 2 (10 clases)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
