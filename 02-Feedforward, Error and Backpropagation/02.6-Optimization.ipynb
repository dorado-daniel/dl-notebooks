{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5956405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formas:\n",
      "X: torch.Size([3, 2]) | W: torch.Size([1, 2]) | b: torch.Size([1]) | y_true: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# === 1. Datos de entrada ===\n",
    "X = torch.tensor([\n",
    "    [2.0, 1.0],   # muestra 1\n",
    "    [5.0, 1.0],   # muestra 2\n",
    "    [1.0, 3.0]    # muestra 3\n",
    "], device=device)\n",
    "\n",
    "# === 2. Etiquetas verdaderas (objetivos) ===\n",
    "# Digamos que la salida esperada (regresión simple) es:\n",
    "y_true = torch.tensor([[5.0], [10.0], [7.0]], device=device)\n",
    "\n",
    "# === 3. Pesos y bias iniciales (entrenables) ===\n",
    "W = torch.tensor([[0.5, 1.0]], device=device, requires_grad=True)  # (1, 2)\n",
    "b = torch.tensor([0.0], device=device, requires_grad=True)          # (1,)\n",
    "\n",
    "# === 4. Definir la función de pérdida ===\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"Formas:\")\n",
    "print(\"X:\", X.shape, \"| W:\", W.shape, \"| b:\", b.shape, \"| y_true:\", y_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 5. Definir el optimizador ===\n",
    "#aqui usamos SGD (gradiente descendente estocástico) \n",
    "# con una tasa de aprendizaje de 0.05\n",
    "# estocastico = aleatorio\n",
    "# el GD que no es estocastico usa todo el dataset para actualizar los pesos\n",
    "# pero es menos eficiente computacionalmente\n",
    "\n",
    "#De todas formas aqui no estamos usando mini-batches,\n",
    "# asi que en este caso SGD y GD serian equivalentes\n",
    "# NO HAY ALEATORIEDAD EN ESTE EJEMPLO\n",
    "#PARA ELLO TENDRIAMOS QUE USAR MINI-BATCHES (MAS ADELANTE)\n",
    "\n",
    "#Se suele usar aleatoriedad porque\n",
    "#se corre el riesgo de quedar atrapado en minimos locales\n",
    "\n",
    "# Usar SGD sin definir mini-batches, es usar GD.\n",
    "\n",
    "#Aqui solo lo definimos, no lo usamos aun\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf295238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida antes de actualizar: 0.0972\n",
      "\n",
      "Gradientes calculados:\n",
      "dL/dW = tensor([[-1.2815, -1.0889]], device='cuda:0')\n",
      "dL/db = tensor([-0.4904], device='cuda:0')\n",
      "\n",
      "Pérdida después de actualizar: 0.0324\n",
      "\n",
      "Nuevos valores de W y b después del paso:\n",
      "W = tensor([[1.6263, 1.6044]], device='cuda:0')\n",
      "b = tensor([0.3634], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# === 6. Un paso de entrenamiento manual (Forward + Backward + Step) ===\n",
    "\n",
    "# 1. Reiniciamos los gradientes acumulados (por seguridad)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 2. Forward: calcular las predicciones actuales\n",
    "y_pred = X @ W.T + b\n",
    "\n",
    "# 3. Calcular la pérdida (error actual)\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(f\"Pérdida antes de actualizar: {loss.item():.4f}\")\n",
    "\n",
    "# 4. Backpropagation: calcular (derivadas, pendientes)\n",
    "# gradientes de la pérdida respecto a W y b\n",
    "# Lo veremos en la siguiente sección en profundidad\n",
    "loss.backward()\n",
    "print(\"\\nGradientes calculados:\")\n",
    "print(\"dL/dW =\", W.grad)\n",
    "print(\"dL/db =\", b.grad)\n",
    "\n",
    "# 5. Step: aplicar la actualización (mover los pesos)\n",
    "# aqui si que ocurre el descenso del gradiente\n",
    "optimizer.step()\n",
    "\n",
    "# 6. Recalcular predicciones y pérdida con los pesos actualizados\n",
    "y_pred_new = X @ W.T + b\n",
    "loss_new = loss_fn(y_pred_new, y_true)\n",
    "print(f\"\\nPérdida después de actualizar: {loss_new.item():.4f}\")\n",
    "\n",
    "# 7. Mostrar nuevos parámetros\n",
    "print(\"\\nNuevos valores de W y b después del paso:\")\n",
    "print(\"W =\", W.data)\n",
    "print(\"b =\", b.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7a0a2",
   "metadata": {},
   "source": [
    "| Etapa               | Acción                        | Código                 |\n",
    "| ------------------- | ----------------------------- | ---------------------- |\n",
    "| **Feedforward**     | Calcular predicciones         | `y_pred = X @ W.T + b` |\n",
    "| **Backpropagation** | Calcular gradientes del error | `loss.backward()`      |\n",
    "| **Actualización**   | Ajustar pesos                 | `optimizer.step()`     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
