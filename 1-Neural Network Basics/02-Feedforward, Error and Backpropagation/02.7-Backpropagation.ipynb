{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6904a5c",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Resumen de lo visto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c023be",
   "metadata": {},
   "source": [
    "| Concepto                      | Exp                                                             | Ejemplo                         |\n",
    "| ----------------------------- | ------------------------------------------------------------------------------------- | --------------------------------------------- |\n",
    "| **Función de pérdida (loss)** | Mide discrepancia entre predicción y objetivo (≥ 0).                                  | `loss_fn = nn.MSELoss()`                      |\n",
    "| **Gradiente**                 | Dirección de cambio más rápido del error respecto a cada peso.                        | `loss.backward()` calcula `W.grad`, `b.grad`. |\n",
    "| **Learning rate**             | Tamaño del paso en la dirección del gradiente.                                        | `lr=0.05`                                     |\n",
    "| **Optimización**              | Proceso de actualizar los pesos usando esos gradientes.                               | `optimizer.step()`                            |\n",
    "| **SGD**                       | Mecanismo de actualización (no un modo en sí). Aleatoriedad solo si tú la introduces. | `torch.optim.SGD([W,b], lr=0.05)`             |\n",
    "| **Backpropagation**           | Mecanismo automático que propaga gradientes hacia capas previas.                      | Ocurre dentro de `loss.backward()`.           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432f913",
   "metadata": {},
   "source": [
    "Ahora veremos en profundidad el concepto de Backpropagation.\n",
    "\n",
    "Backpropagation es simplemente “cómo se reparte la culpa del error”\n",
    "\n",
    "Cuando la red se equivoca, ese error fluye hacia atrás capa por capa.\n",
    "Cada peso recibe una parte proporcional de responsabilidad según cuánto haya contribuido al error.\n",
    "\n",
    "#### 1.Cómo se propaga el error hacia atrás a través de las capas.\n",
    "\n",
    "La regla de la cadena aplicada a funciones compuestas.\n",
    "Cada capa transmite su “culpa” hacia la anterior.\n",
    "\n",
    "#### 2. Por qué necesitamos las derivadas de las funciones de activación (ReLU, Sigmoid, etc.)\n",
    "\n",
    "Son las “válvulas” que controlan cuánta información pasa hacia atrás.\n",
    "\n",
    "Entender dónde pueden “matar” el gradiente (problema del vanishing gradient)\n",
    "\n",
    "#### 3. Qué se guarda en la fase forward para poder hacer backward.\n",
    "\n",
    "Los valores intermedios (z, a) que se necesitan para calcular derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dd4de",
   "metadata": {},
   "source": [
    "### Empezamos.\n",
    "Backpropagation empieza en la salida y empuja el error hacia atrás por la red.\n",
    "Para entenderlo, basta una sola neurona: el flujo de gradientes será idéntico, solo que en redes profundas se encadena capa a capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db0b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formas:\n",
      "x: torch.Size([1, 2]) | W: torch.Size([1, 2]) | b: torch.Size([1]) | y_true: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# === 1. Preparación: Neurona simple con activación Sigmoid ===\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Datos de entrada (una sola muestra con 2 features)\n",
    "x = torch.tensor([[2.0, 1.0]])   # shape (1, 2)\n",
    "\n",
    "# Etiqueta verdadera (target)\n",
    "y_true = torch.tensor([[1.0]])   # salida esperada\n",
    "\n",
    "# Pesos y bias iniciales (entrenables)\n",
    "W = torch.tensor([[0.5, -1.0]], requires_grad=True)  # (1, 2)\n",
    "b = torch.tensor([0.0], requires_grad=True)          # (1,)\n",
    "\n",
    "print(\"Formas:\")\n",
    "print(\"x:\", x.shape, \"| W:\", W.shape, \"| b:\", b.shape, \"| y_true:\", y_true.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e2dfc",
   "metadata": {},
   "source": [
    "La fase forward calcula la salida de la neurona.\n",
    "\n",
    "La fase backward invertirá este flujo: usará cómo cambia la pérdida con respecto a la salida para calcular cómo afecta a los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69474295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (valor pre-activación): tensor([[0.]], grad_fn=<AddBackward0>)\n",
      "a (salida post-Sigmoid): tensor([[0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "Formas → x: torch.Size([1, 2]) | W: torch.Size([1, 2]) | z: torch.Size([1, 1]) | a: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# === 2. Forward: cálculo paso a paso ===\n",
    "\n",
    "# 1. Cálculo lineal: z = x @ W.T + b\n",
    "z = x @ W.T + b\n",
    "print(\"z (valor pre-activación):\", z)\n",
    "\n",
    "# 2. Aplicar función de activación Sigmoid: a = σ(z)\n",
    "a = torch.sigmoid(z)\n",
    "print(\"a (salida post-Sigmoid):\", a)\n",
    "\n",
    "# 3. Mostrar shapes para confirmar coherencia\n",
    "print(\"\\nFormas → x:\", x.shape, \"| W:\", W.shape, \"| z:\", z.shape, \"| a:\", a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5edd6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida BCE: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# === 3. Definir la función de pérdida BCE ===\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definimos la función de pérdida adecuada para salidas en [0, 1]\n",
    "# En este caso usamos BCE debido a que usamos sigmoid como salida\n",
    "bce_lossfn = nn.BCELoss()\n",
    "\n",
    "# Calculamos la pérdida entre la predicción 'a' (ya sigmoid) y la etiqueta 'y_true'\n",
    "loss = bce_lossfn(a, y_true)\n",
    "\n",
    "#La BCE mide cuánto difiere la probabilidad predicha de la real.\n",
    "\n",
    "#Es una medida asimétrica: penaliza más cuando la red está segura y se equivoca\n",
    "\n",
    "#(por ejemplo a≈0.99 cuando y_true=0).\n",
    "\n",
    "print(f\"Pérdida BCE: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0bcb5",
   "metadata": {},
   "source": [
    "Calcularemos ahora el backward (backpropagation) y los gradientes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a537974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradientes intermedios:\n",
      "dL/dz: tensor([[-0.5000]])\n",
      "dL/da: tensor([[-2.]])\n",
      "\n",
      "Gradientes en parámetros:\n",
      "dL/dW: tensor([[-1.0000, -0.5000]])\n",
      "dL/db: tensor([-0.5000])\n"
     ]
    }
   ],
   "source": [
    "# === 4. Backward: gradientes ===\n",
    "\n",
    "# 0) Limpiar gradientes previos (por si re-ejecutas la celda)\n",
    "if W.grad is not None: W.grad.zero_()\n",
    "if b.grad is not None: b.grad.zero_()\n",
    "\n",
    "# 1) Gradiente intermedio: dL/da y dL/dz (regla de la cadena)\n",
    "dL_dz, = torch.autograd.grad(loss, z, retain_graph=True)\n",
    "dL_da, = torch.autograd.grad(loss, a, retain_graph=True)\n",
    "\n",
    "print(\"Gradientes intermedios:\")\n",
    "print(\"dL/dz:\", dL_dz)\n",
    "print(\"dL/da:\", dL_da)\n",
    "\n",
    "# 2) Gradientes respecto a parámetros W y b\n",
    "loss.backward()  # propaga hasta W y b\n",
    "\n",
    "print(\"\\nGradientes en parámetros:\")\n",
    "print(\"dL/dW:\", W.grad)\n",
    "print(\"dL/db:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2634e",
   "metadata": {},
   "source": [
    "5. A vigilar como ingeniero\n",
    "\n",
    "Que los gradientes no se anulen (vanishing gradient, común con Sigmoid).\n",
    "\n",
    "Que los gradientes no exploten (valores enormes → inestabilidad).\n",
    "\n",
    "Que las activaciones y pesos estén en escalas razonables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
