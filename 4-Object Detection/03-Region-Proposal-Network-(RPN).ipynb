{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf734f2b",
   "metadata": {},
   "source": [
    "# RPN Regional Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183e5c4",
   "metadata": {},
   "source": [
    "Una RPN, es una red neuronal orientada a extraer proposals **de un feature map** , al contrario que Selective Search, no las extrae de las imagenes si no directamente del mapa de características. Por lo que nos permite deshacernos de SS y su bottleneck, para obtener directamente propuestas del mapa de caracteristicas de cada imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a1997",
   "metadata": {},
   "source": [
    "- La RPN consume feature maps, no imágenes crudas\n",
    "- Queremos obtener (C, H, W) y el stride efectivo del backbone para imagenes en VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1a4f7",
   "metadata": {},
   "source": [
    "Empecemos por los imports y la configuración inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b49c8e",
   "metadata": {},
   "source": [
    "## 1 Configuración DS Backbone y Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2cc2b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Device: cuda\n",
      "VOC_ROOT: ./data/02\n",
      "Train: 5011\n",
      "Test: 4952\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time, pathlib, shutil, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import selectivesearch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"     \n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"     \n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "VOC_ROOT = os.environ.get(\"VOC_ROOT\", \"./data/02\")\n",
    "print(\"VOC_ROOT:\", VOC_ROOT)\n",
    "\n",
    "#Importamos el modelo de ResNet50\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet_transforms = weights.transforms()\n",
    "\n",
    "\n",
    "\n",
    "#VOC 2007\n",
    "train_root = \"./data/02/VOCtrainval_06-Nov-2007\"\n",
    "test_root  = \"./data/02/VOCtest_06-Nov-2007\"\n",
    "\n",
    "train_ds = VOCDetection(train_root, year=\"2007\", image_set=\"trainval\", download=False)\n",
    "test_ds  = VOCDetection(test_root,  year=\"2007\", image_set=\"test\",     download=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Test:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094e331",
   "metadata": {},
   "source": [
    "Preprocesaremos Imagenes de VOC igual que hicimos en FastRCNN, pero, no trabajaremos las bboxes ni las escalaremos.\n",
    "Solo queremos cargar una imagen, redimensionarla y normalizarla para extraer features del backbone.\n",
    "\n",
    "En RPN la logica es:\n",
    "\n",
    "- Backbone → feature map\n",
    "\n",
    "- Feature map → anchors\n",
    "\n",
    "- Luego se comparan anchors con GT boxes originales o escaladas, pero eso ocurre en otro sitio\n",
    "(no en el preprocesado de imagen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64896c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "# Parámetros de escalado\n",
    "short_side = 600\n",
    "max_side = 1000\n",
    "\n",
    "# Normalización para ResNet (usa tu propio resnet_transforms)\n",
    "R50_MEAN = resnet_transforms.mean\n",
    "R50_STD  = resnet_transforms.std\n",
    "\n",
    "def preprocess_image(img, device=device, short_side=short_side, max_side=max_side):\n",
    "    # Tamaño original\n",
    "    w, h = img.size\n",
    "    orig_size = (h, w)  # (H,W)\n",
    "\n",
    "    # Escalado igual que en Fast R-CNN\n",
    "    actual_short = min(w, h)\n",
    "    actual_long  = max(w, h)\n",
    "\n",
    "    scale = short_side / actual_short\n",
    "    if actual_long * scale > max_side:\n",
    "        scale = max_side / actual_long\n",
    "\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "\n",
    "    new_size = (new_h, new_w)\n",
    "    img_resized = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "    #Convertimos a tensor y normalizamos\n",
    "    img_t = to_tensor(img_resized)\n",
    "    img_t = normalize(img_t, R50_MEAN, R50_STD)\n",
    "    img_t = img_t.unsqueeze(0).to(device)  # [1,3,H,W]\n",
    "\n",
    "    #Devolvemos el tensor, el tamaño original, el nuevo tamaño y el factor de escala\n",
    "    return img_t, orig_size,  new_size, scale, img_resized\n",
    "\n",
    "\n",
    "#Cargamos una imagen de VOC\n",
    "def load_image_voc(dataset, idx):\n",
    "    sample = dataset[idx]\n",
    "    pil_img = sample[\"image\"] if \"image\" in sample else sample[0]  \n",
    "\n",
    "    img_t, orig_size, new_size, scale, img_resized = preprocess_image(pil_img)\n",
    "    return img_t, orig_size, new_size, scale, img_resized\n",
    "\n",
    "    #img_t: tensor imagen redimensionada y normalizada\n",
    "    #orig_size: tamaño original de la imagen (H,W)\n",
    "    #new_size: tamaño nuevo de la imagen (H,W)\n",
    "    #scale: factor de escala \n",
    "    #img_resized: imagen redimensionada (PIL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f56fc4",
   "metadata": {},
   "source": [
    "Ahora, cargaremos el backbone de Resnet50, cortaremos el head, lo pondremos en eval y congelaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7daa4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el modelo de ResNet50\n",
    "backbone = nn.Sequential(*list(models.resnet50(weights=weights).children())).to(device)\n",
    "\n",
    "#En el notebook anterior vimos como calcular el stride 16 (acumulado) y que estaba en el layer 7.\n",
    "BACKBONE_CUT = 7\n",
    "BACKBONE_STRIDE = 16\n",
    "\n",
    "backbone = nn.Sequential(*list(models.resnet50(weights=weights).children())[:BACKBONE_CUT]).to(device)\n",
    "\n",
    "#Congelamos el backbone\n",
    "backbone.eval()\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b5147",
   "metadata": {},
   "source": [
    "Crearemos ahora una funcion para extraer features de cada imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c732e",
   "metadata": {},
   "source": [
    "-Notesé que nuestro stride es fijo (BACKBONE_STRIDE = 16) y que lo devolvemos en extract_features, realmente no seria necesario devolverlo en este caso.\n",
    "\n",
    "Pero lo hacemos porque en arquitecturas más avanzadas como **FPN (Feature Pyramid Netowks)** que son derivadas de RPN, el stride cambia ya que, en ese caso no solo operamos a nivel de una capa (C4) si no de otras más pequeñas, eso ayudará a que, al extraer features a distintos niveles de stride con features maps de distinto tamaño, podamos detectar despues propuestas más pequeñas.\n",
    "\n",
    "Sin embargo, de momento, lo dejamos como BACKBONE_STRIDE = 16 y lo devolvemos para hacer un guiño a los siguientes capitulos, ya veremos como lo hacemos en otro notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01622d74",
   "metadata": {},
   "source": [
    "Para aclarar:\n",
    "\n",
    "Lo que hace una RPN clásica es lo siguiente:\n",
    "\n",
    "    -Se pasa la imagen por el backbone ResNet-50 hasta el bloque C4.\n",
    "\n",
    "    -La salida es un feature map de tamaño [1024, Hf, Wf] y stride ≈16 respecto a la imagen procesada.\n",
    "\n",
    "    Ese feature map es la entrada única de la RPN:\n",
    "\n",
    "        -para colocar anchors centrados en cada celda del grid,\n",
    "\n",
    "        -para predecir objectness y shifts de caja,\n",
    "\n",
    "        -para generar propuestas (ROIs) para Fast R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2eb92288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 38, 50]) 16\n"
     ]
    }
   ],
   "source": [
    "#Funcion para extraer features del backbone\n",
    "def extract_features(img_t):\n",
    "\n",
    "    feat_map = backbone(img_t)\n",
    "\n",
    "    return feat_map, BACKBONE_STRIDE\n",
    "\n",
    "\n",
    "\n",
    "#Cargamos una imagen de VOC\n",
    "img_t, orig_size, new_size, scale, img_resized = load_image_voc(train_ds, 0)\n",
    "\n",
    "#Extraemos features del backbone\n",
    "feat_map, stride = extract_features(img_t)\n",
    "\n",
    "#Imprimimos el shape del feature map y el stride\n",
    "print(feat_map.shape, stride)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8ee0e",
   "metadata": {},
   "source": [
    "## 2. Anchors base y grid sobre el feature map (C4, stride=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d30bff",
   "metadata": {},
   "source": [
    "En RPN no tenemos selective search que nos daba ya las propuestas, por lo que tenemos que definirlas nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ab97e",
   "metadata": {},
   "source": [
    "#### Anchors\n",
    "\n",
    "En RPNs, los anchors son la definición de nuestras propuestas, y estan compuestos por:\n",
    "\n",
    "[x1,y1,x2,y2] en papers suelen definirse como [centroide, w, h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd2d03",
   "metadata": {},
   "source": [
    "En general se definen como scale y aspect ratio.\n",
    "\n",
    "scale = son areas de las propuestas (tamaño)\n",
    "\n",
    "ratio = son las proporciones de las propuestas Ratio = ancho / alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb0c33",
   "metadata": {},
   "source": [
    "Combinando scales y ratios generamos anchors de:\n",
    "\n",
    "- distintos tamaños\n",
    "- diferentes formas\n",
    "- orientados horizontal o vertical\n",
    "\n",
    "Para scale=256:\n",
    "\n",
    "| ratio | forma      | w x h aproximado        | área aproximada |\n",
    "|-------|------------|--------------------------|------------------|\n",
    "| 1     | cuadrado   | 256 × 256                | 65536            |\n",
    "| 0.5   | vertical   | 181 × 362                | 65522            |\n",
    "| 2     | horizontal | 362 × 181                | 65522            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213f963",
   "metadata": {},
   "source": [
    "Definimos SCALES, RATIOS y el valor K, que representa el número total de anchors distintos generados por las combinaciones de escalas y ratios (anchors por celda del feature map).\n",
    "\n",
    "- SCALES → tamaños base de los anchors (aproximadamente la raíz del área por lo que el area w * h es = al cuadrado de scale)\n",
    "\n",
    "w * h = scale²\n",
    "\n",
    "\n",
    "- RATIOS → relación ancho/alto de cada anchor\n",
    "\n",
    "w / h = ratio\n",
    "\n",
    "- K → número total de combinaciones scale × ratio\n",
    "(es decir, el número de anchors diferentes por cada celda del feature map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "23e1fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 9 = 3 * 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ANCHOR_SCALES = [128, 256, 512]\n",
    "ANCHOR_RATIOS = [0.5, 1.0, 2.0]\n",
    "\n",
    "K = len(ANCHOR_SCALES) * len(ANCHOR_RATIOS)\n",
    "\n",
    "\n",
    "print(f\"K = {K} = {len(ANCHOR_SCALES)} * {len(ANCHOR_RATIOS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6b174",
   "metadata": {},
   "source": [
    "### Cálculo de w y h para cada anchor base\n",
    "\n",
    "Dado un `scale` y un `ratio`, queremos definir las dimensiones `(w, h)` del anchor de forma que:\n",
    "\n",
    "- 1) El área quede fija: `w * h = scale²`\n",
    "- 2) La forma venga dada por el ratio: `ratio = w / h`\n",
    "\n",
    "Necesitamos que se cumplan las dos porque queremos 1) el MISMO area siempre 2) para cualquier forma.\n",
    "\n",
    "Esto nos deja un sistema de ecuaciones:\n",
    "\n",
    "[`w * h = scale²` , `ratio = w / h` ]\n",
    "\n",
    "\n",
    "Así que empezamos a despejarlo:\n",
    "\n",
    "De `ratio = w / h` podemos escribir:\n",
    "\n",
    "- `w = ratio * h`\n",
    "\n",
    "y ya tendríamos w definido, sin embargo con h:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Despejar h\n",
    "\n",
    "Partimos de:\n",
    "\n",
    "- `w = ratio * h`\n",
    "- `w * h = scale²`\n",
    "\n",
    "Sustituimos `w` en la ecuación del área:\n",
    "\n",
    "- `(ratio * h) * h = scale²`\n",
    "- `ratio * h² = scale²`\n",
    "\n",
    "Despejamos `h²`:\n",
    "\n",
    "- `h² = scale² / ratio`\n",
    "\n",
    "Para quitar el cuadrado aplicamos raíz cuadrada en ambos lados:\n",
    "\n",
    "- `h = sqrt(scale² / ratio)`\n",
    "\n",
    "Usamos la propiedad `sqrt(A/B) = sqrt(A) / sqrt(B)`:\n",
    "\n",
    "- `h = sqrt(scale²) / sqrt(ratio)`\n",
    "\n",
    "Y como `sqrt(scale²) = scale`:\n",
    "\n",
    "- `h = scale / sqrt(ratio)`\n",
    "\n",
    "Es decir:\n",
    "\n",
    "- `h = scale / sqrt(ratio)`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Calcular w a partir de h\n",
    "\n",
    "Sabemos que:\n",
    "\n",
    "- `w = ratio * h`\n",
    "\n",
    "Sustituimos `h`:\n",
    "\n",
    "- `w = ratio * (scale / sqrt(ratio))`\n",
    "- `w = scale * (ratio / sqrt(ratio))`\n",
    "\n",
    "Ahora usamos que:\n",
    "\n",
    "- `ratio = sqrt(ratio) * sqrt(ratio)`\n",
    "\n",
    "Entonces:\n",
    "\n",
    "- `w = scale * (sqrt(ratio) * sqrt(ratio) / sqrt(ratio))`\n",
    "\n",
    "Podemos simplificar porque:\n",
    "\n",
    "- `(sqrt(ratio) * sqrt(ratio)) / sqrt(ratio) = sqrt(ratio)`\n",
    "\n",
    "Nos queda:\n",
    "\n",
    "- `w = scale * sqrt(ratio)`\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Resultado final\n",
    "\n",
    "Para cada par `(scale, ratio)`:\n",
    "\n",
    "- `w = scale * sqrt(ratio)`\n",
    "- `h = scale / sqrt(ratio)`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Construcción del anchor [x1, y1, x2, y2] centrado en (0,0)\n",
    "\n",
    "Una vez tenemos `w` y `h`, definimos el anchor base centrado en el origen:\n",
    "\n",
    "- `x1 = -w / 2`\n",
    "- `y1 = -h / 2`\n",
    "- `x2 =  w / 2`\n",
    "- `y2 =  h / 2`\n",
    "\n",
    "Cada combinación `(scale, ratio)` produce un anchor `[x1, y1, x2, y2]`.\n",
    "\n",
    "Si tenemos:\n",
    "\n",
    "- `len(SCALES) * len(RATIOS) = K`\n",
    "\n",
    "entonces tendremos un tensor de anchors base de tamaño:\n",
    "\n",
    "- `[K, 4]` donde cada fila es `[x1, y1, x2, y2]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "695cecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_base_anchors(scales, ratios):\n",
    "    anchors = []\n",
    "    for scale in scales:\n",
    "        for ratio in ratios:\n",
    "\n",
    "            #Estas funciones se calculan en la descripcion del punto anterior\n",
    "            w = scale * np.sqrt(ratio)\n",
    "            h = scale / np.sqrt(ratio)\n",
    "\n",
    "            #Aqui generamos los anchors base centrado en el origen, por eso usamos negativos\n",
    "            x1 = -w / 2\n",
    "            y1 = -h / 2\n",
    "            x2 =  w / 2\n",
    "            y2 =  h / 2\n",
    "            \n",
    "            anchors.append([x1, y1, x2, y2])\n",
    "\n",
    "            \n",
    "\n",
    "    return torch.tensor(anchors, dtype=torch.float32)\n",
    "\n",
    "base_anchors = generate_base_anchors(ANCHOR_SCALES, ANCHOR_RATIOS)\n",
    "print(base_anchors.shape)\n",
    "len(base_anchors)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8f0a9",
   "metadata": {},
   "source": [
    "Vamos a visualizar nuestros anchors base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e9cff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "base_anchors = generate_base_anchors(ANCHOR_SCALES, ANCHOR_RATIOS)\n",
    "\n",
    "# valores mínimos y máximos\n",
    "xmin = float(base_anchors[:,0].min())\n",
    "ymin = float(base_anchors[:,1].min())\n",
    "xmax = float(base_anchors[:,2].max())\n",
    "ymax = float(base_anchors[:,3].max())\n",
    "\n",
    "# margen adicional\n",
    "margin = max(xmax - xmin, ymax - ymin) * 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=80)\n",
    "\n",
    "colors = ['red','blue','green','orange','purple','brown','pink','gray','olive']\n",
    "\n",
    "for i, (x1, y1, x2, y2) in enumerate(base_anchors):\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    color = colors[i % len(colors)]\n",
    "    rect = patches.Rectangle((x1, y1), w, h, fill=False, color=color, linewidth=1.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x2, y2, f\"{i}\", fontsize=10, color=color)\n",
    "\n",
    "ax.axhline(0, linestyle='--', linewidth=1)\n",
    "ax.axvline(0, linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_xlim(xmin - margin, xmax + margin)\n",
    "ax.set_ylim(ymin - margin, ymax + margin)\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Base Centered anchors in (0,0)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194268f",
   "metadata": {},
   "source": [
    "Ahora lo que necesitamos es generar estos anchors sobre un grid con stride 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_map: torch.Size([1, 1024, 38, 50]) Hf = 38 Wf = 50 stride = 16\n",
      "shift_x: tensor([  0,  16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
      "        224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432,\n",
      "        448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656,\n",
      "        672, 688, 704, 720, 736, 752, 768, 784], device='cuda:0')\n",
      "shift_x shape: torch.Size([50])\n",
      "shift_y: tensor([  0,  16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
      "        224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432,\n",
      "        448, 464, 480, 496, 512, 528, 544, 560, 576, 592], device='cuda:0')\n",
      "shift_y shape: torch.Size([38])\n",
      "shift_y: tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
      "        [ 16,  16,  16,  ...,  16,  16,  16],\n",
      "        [ 32,  32,  32,  ...,  32,  32,  32],\n",
      "        ...,\n",
      "        [560, 560, 560,  ..., 560, 560, 560],\n",
      "        [576, 576, 576,  ..., 576, 576, 576],\n",
      "        [592, 592, 592,  ..., 592, 592, 592]], device='cuda:0')\n",
      "shift_x: tensor([[  0,  16,  32,  ..., 752, 768, 784],\n",
      "        [  0,  16,  32,  ..., 752, 768, 784],\n",
      "        [  0,  16,  32,  ..., 752, 768, 784],\n",
      "        ...,\n",
      "        [  0,  16,  32,  ..., 752, 768, 784],\n",
      "        [  0,  16,  32,  ..., 752, 768, 784],\n",
      "        [  0,  16,  32,  ..., 752, 768, 784]], device='cuda:0')\n",
      "shift_x shape: torch.Size([38, 50])\n",
      "shift_y shape: torch.Size([38, 50])\n",
      "shifts: tensor([[[  0,   0,   0,   0],\n",
      "         [ 16,   0,  16,   0],\n",
      "         [ 32,   0,  32,   0],\n",
      "         ...,\n",
      "         [752,   0, 752,   0],\n",
      "         [768,   0, 768,   0],\n",
      "         [784,   0, 784,   0]],\n",
      "\n",
      "        [[  0,  16,   0,  16],\n",
      "         [ 16,  16,  16,  16],\n",
      "         [ 32,  16,  32,  16],\n",
      "         ...,\n",
      "         [752,  16, 752,  16],\n",
      "         [768,  16, 768,  16],\n",
      "         [784,  16, 784,  16]],\n",
      "\n",
      "        [[  0,  32,   0,  32],\n",
      "         [ 16,  32,  16,  32],\n",
      "         [ 32,  32,  32,  32],\n",
      "         ...,\n",
      "         [752,  32, 752,  32],\n",
      "         [768,  32, 768,  32],\n",
      "         [784,  32, 784,  32]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0, 560,   0, 560],\n",
      "         [ 16, 560,  16, 560],\n",
      "         [ 32, 560,  32, 560],\n",
      "         ...,\n",
      "         [752, 560, 752, 560],\n",
      "         [768, 560, 768, 560],\n",
      "         [784, 560, 784, 560]],\n",
      "\n",
      "        [[  0, 576,   0, 576],\n",
      "         [ 16, 576,  16, 576],\n",
      "         [ 32, 576,  32, 576],\n",
      "         ...,\n",
      "         [752, 576, 752, 576],\n",
      "         [768, 576, 768, 576],\n",
      "         [784, 576, 784, 576]],\n",
      "\n",
      "        [[  0, 592,   0, 592],\n",
      "         [ 16, 592,  16, 592],\n",
      "         [ 32, 592,  32, 592],\n",
      "         ...,\n",
      "         [752, 592, 752, 592],\n",
      "         [768, 592, 768, 592],\n",
      "         [784, 592, 784, 592]]], device='cuda:0')\n",
      "shifts shape: torch.Size([38, 50, 4])\n",
      "shifts reshaped: tensor([[  0,   0,   0,   0],\n",
      "        [ 16,   0,  16,   0],\n",
      "        [ 32,   0,  32,   0],\n",
      "        ...,\n",
      "        [752, 592, 752, 592],\n",
      "        [768, 592, 768, 592],\n",
      "        [784, 592, 784, 592]], device='cuda:0')\n",
      "shifts reshaped shape: torch.Size([1900, 4])\n",
      "A: 9\n",
      "K: 1900\n"
     ]
    }
   ],
   "source": [
    "#Primero cargamos una imagen de VOC, extraemos las features del backbone\n",
    "\n",
    "img_t, orig_size, new_size, scale, img_resized = load_image_voc(train_ds, 0)\n",
    "\n",
    "feat_map, stride = extract_features(img_t)\n",
    "\n",
    "#Ahora extraemos el shape del feature map y nos quedamos con H y W que son las dimensiones del grid\n",
    "Hf, Wf = feat_map.shape[-2], feat_map.shape[-1]\n",
    "\n",
    "#Ademas obtenemos el stride predefinido en el backbone\n",
    "print(\"feat_map:\", feat_map.shape, \"Hf =\", Hf, \"Wf =\", Wf, \"stride =\", stride)\n",
    "\n",
    "def generate_anchors_on_grid(base_anchors, feat_h, feat_w, stride, device=None):\n",
    "    if device is None:\n",
    "        device = base_anchors.device\n",
    "\n",
    "    base_anchors = base_anchors.to(device)\n",
    "\n",
    "    #Posiciones X de las celdas del grid, usamos arange para generar un array de 0 a feat_w-1\n",
    "    #Multiplicamos por stride para que coincida con el stride del feature map\n",
    "    shift_x = torch.arange(feat_w, device=device) * stride\n",
    "\n",
    "\n",
    "    #Posiciones Y de las celdas del grid, usamos arange para generar un array de 0 a feat_h-1\n",
    "    #Multiplicamos por stride para que coincida con el stride del feature map\n",
    "    shift_y = torch.arange(feat_h, device=device) * stride\n",
    "  \n",
    "    #Generamos las posiciones X y Y del grid, usamos meshgrid para que sean coordenadas cartesianas\n",
    "    #Lo que hace meshgrid es generar todas las combinaciones de shift_x y shift_y para cada posicion del grid\n",
    "    shift_y, shift_x = torch.meshgrid(shift_y, shift_x, indexing=\"ij\")\n",
    "\n",
    "    #Creamos un tensor con las coordenadas de las celdas del grid y las stackamos para poder sumarlas a los anchors base\n",
    "    shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=-1)\n",
    " \n",
    "    #Ahora tenemos un tensor de shape (Hf * Wf, 4) con las coordenadas de las celdas del grid\n",
    "    shifts = shifts.reshape(-1, 4)\n",
    " \n",
    "    A = base_anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    print(\"A:\", A)\n",
    "    print(\"K:\", K)\n",
    "\n",
    "    #Ahora generamos los anchors desplazados por las coordenadas del grid\n",
    "    anchors = base_anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n",
    "    anchors = anchors.view(K * A, 4)\n",
    "\n",
    "    return anchors\n",
    "\n",
    "all_anchors = generate_anchors_on_grid(base_anchors, Hf, Wf, BACKBONE_STRIDE, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
