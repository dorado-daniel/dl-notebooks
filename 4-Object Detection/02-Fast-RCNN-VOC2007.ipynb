{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d96abbc",
   "metadata": {},
   "source": [
    "# Fast RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257e05f",
   "metadata": {},
   "source": [
    "**Fast R-CNN** es una mejora directa de la R-CNN clásica.  \n",
    "\n",
    "En este caso, las regiones de interés (*ROIs*) ya no se recortan ni se procesan individualmente para extraer características.  \n",
    "En su lugar, la imagen completa pasa una sola vez por la red convolucional, generando un **feature map**.  \n",
    "\n",
    "Después, se obtienen las propuestas de **Selective Search** y se proyectan sus coordenadas sobre ese mapa de características para extraer las regiones correspondientes.  \n",
    "\n",
    "Dado que cada ROI tiene un tamaño distinto, se aplica **ROI Pooling** para normalizarlas a una forma fija, de modo que puedan pasar por las capas fully-connected del *head*.  (En implementaciones modernas usamos ROI Align)\n",
    "\n",
    "La red se entrena optimizando simultáneamente:\n",
    "- la **pérdida de clasificación** (qué objeto hay en cada ROI), y  \n",
    "- la **pérdida de regresión** (ajuste fino de los *bounding boxes*).  \n",
    "\n",
    "Durante el entrenamiento **no se aplica NMS**, pero **sí se usa en la inferencia** para eliminar detecciones redundantes.  \n",
    "\n",
    "Esta arquitectura mejora notablemente los tiempos respecto a R-CNN, aunque **Selective Search sigue siendo el cuello de botella principal**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8f41c",
   "metadata": {},
   "source": [
    "Diferencias:\n",
    "\n",
    "| Etapa     | R-CNN clásica                  | Fast-RCNN                   |\n",
    "| --------- | ------------------------------ | ------------------------------------------- |\n",
    "| Proposals | Selective Search               | Selective Search                 |\n",
    "| ROIs      | No hay como tal                | Subconjunto de proposals usado por ROIPooling / Align |\n",
    "| NMS       | Final (sobre predicciones SVM) | Final (sobre predicciones del head)         |\n",
    "\n",
    "\n",
    "Conceptos:\n",
    "\n",
    "\n",
    "| Concepto      | Qué es                                                    | Cómo se obtiene                                        | Cuántos hay              | Cómo se usa                                                |\n",
    "| ------------- | --------------------------------------------------------- | ------------------------------------------------------ | ------------------------ | ---------------------------------------------------------- |\n",
    "| **Proposals** | Candidatas “donde podría haber algo”                      | De un algoritmo como **Selective Search** o un **RPN** | Miles (≈2000 por imagen) | Entrada bruta del detector                                 |\n",
    "| **ROIs**      | Subconjunto de proposals elegidas para entrenar o inferir | Se **muestrean** (pos/neg) de las proposals            | Pocas (≈128 por imagen)  | Alimentan **ROIAlign + head** para clasificación/regresión |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4fd84",
   "metadata": {},
   "source": [
    "Vamos a ver su implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ceee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Device: cuda\n",
      "VOC_ROOT: ./data/02\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time, pathlib, shutil, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "\n",
    "VOC_ROOT = os.environ.get(\"VOC_ROOT\", \"./data/02\")\n",
    "print(\"VOC_ROOT:\", VOC_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c07cc",
   "metadata": {},
   "source": [
    "#### Nos saltamos la parte de descargar el dataset, lo vimos en el cuaderno anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbba32",
   "metadata": {},
   "source": [
    "Pasamos directamente a su carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6dda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5011\n",
      "Test: 4952\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import VOCDetection\n",
    "\n",
    "train_root = \"./data/02/VOCtrainval_06-Nov-2007\"\n",
    "test_root  = \"./data/02/VOCtest_06-Nov-2007\"\n",
    "\n",
    "train_ds = VOCDetection(train_root, year=\"2007\", image_set=\"trainval\", download=False)\n",
    "test_ds  = VOCDetection(test_root,  year=\"2007\", image_set=\"test\",     download=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a126f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
      "Total: 21\n"
     ]
    }
   ],
   "source": [
    "# === Extracción automática de clases del dataset VOC ===\n",
    "\n",
    "def extract_voc_classes(dataset):\n",
    "    classes = set()\n",
    "    for i in range(min(500, len(dataset))):  # escanea solo 500 imágenes para acelerar\n",
    "        ann = dataset[i][1]['annotation']\n",
    "        objs = ann.get('object', [])\n",
    "        if isinstance(objs, dict):  # si solo hay un objeto\n",
    "            objs = [objs]\n",
    "        for obj in objs:\n",
    "            name = obj['name']\n",
    "            classes.add(name)\n",
    "    return sorted(list(classes))\n",
    "\n",
    "# Obtener clases del dataset de entrenamiento\n",
    "voc_classes = extract_voc_classes(train_ds)\n",
    "\n",
    "#Añadimos background porque es la clase 0, que no está en voc_classes, lo necesitamos para el loss. \n",
    "#Tendremos 21 clases: 20 de VOC + 1 de background.\n",
    "CLASSES = ['background'] + voc_classes\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(\"Clases detectadas:\", voc_classes)\n",
    "print(\"Total:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1a99d",
   "metadata": {},
   "source": [
    "Tenemos unas 2000 proposals por imagen\n",
    "Estas salen de Selective Search.\n",
    "La mayoría no contiene ningún objeto útil; algunas sí cubren bien GT.\n",
    "El objetivo es quedarse con 128 ROIs representativas para esa imagen.\n",
    "\n",
    "Para ello calculamos el IoU entre cada proposal y todas las cajas GT.\n",
    "\n",
    "Luego clasificamos cada proposal en Pos y Neg en base a los thresholds del IoU y poderamos las imagenes que nos queremos quedar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num clases: 21\n",
      "Sampler: 128 (FG 32, BG 96)\n",
      "PosIoU≥0.5, NegIoU≤0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# IoU thresholds\n",
    "POS_IOU_THRESH = 0.5\n",
    "NEG_IOU_THRESH = 0.3\n",
    "\n",
    "# Sampler config\n",
    "# Tomamos 128 ROIs por imagen.\n",
    "ROIS_PER_IMG = 128\n",
    "# 25% de esas ROIs son positivas (foreground) y el resto 75% negativas (background).\n",
    "# Usamos esta distribución porque es lo que da mejores resultados en la literatura.\n",
    "FG_FRACTION = 0.25  # ~32 pos + 96 neg \n",
    "\n",
    "# BBox delta normalization\n",
    "# Normalizamos las coordenadas de los bounding boxes para que sean más estables.\n",
    "#Son valores que usamos fijos para todo el entrenamiento y que se han obtenido empíricamente.\n",
    "#Son un estándar en la literatura.\n",
    "BBOX_MEANS = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
    "BBOX_STDS  = torch.tensor([0.1, 0.1, 0.2, 0.2], dtype=torch.float32)\n",
    "\n",
    "# Proposals\n",
    "# Tomamos hasta 2000 propuestas por imagen.\n",
    "MAX_PROPOSALS_PER_IMG = 2000\n",
    "# Las propuestas mínimas tienen 16px de lado.\n",
    "MIN_SIZE = 16\n",
    "\n",
    "print(f\"Num clases: {NUM_CLASSES}\")\n",
    "print(f\"Sampler: {ROIS_PER_IMG} (FG {int(ROIS_PER_IMG*FG_FRACTION)}, BG {ROIS_PER_IMG-int(ROIS_PER_IMG*FG_FRACTION)})\")\n",
    "print(f\"PosIoU≥{POS_IOU_THRESH}, NegIoU≤{NEG_IOU_THRESH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
