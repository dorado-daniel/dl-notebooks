{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d96abbc",
   "metadata": {},
   "source": [
    "# Fast RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257e05f",
   "metadata": {},
   "source": [
    "**Fast R-CNN** es una mejora directa de la R-CNN clásica.  \n",
    "\n",
    "En este caso, las regiones de interés (*ROIs*) ya no se recortan ni se procesan individualmente para extraer características.  \n",
    "En su lugar, la imagen completa pasa una sola vez por la red convolucional, generando un **feature map**.  \n",
    "\n",
    "Después, se obtienen las propuestas de **Selective Search** y se proyectan sus coordenadas sobre ese mapa de características para extraer las regiones correspondientes.  \n",
    "\n",
    "Dado que cada ROI tiene un tamaño distinto, se aplica **ROI Pooling** para normalizarlas a una forma fija, de modo que puedan pasar por las capas fully-connected del *head*.  (En implementaciones modernas usamos ROI Align)\n",
    "\n",
    "La red se entrena optimizando simultáneamente:\n",
    "- la **pérdida de clasificación** (qué objeto hay en cada ROI), y  \n",
    "- la **pérdida de regresión** (ajuste fino de los *bounding boxes*).  \n",
    "\n",
    "Durante el entrenamiento **no se aplica NMS**, pero **sí se usa en la inferencia** para eliminar detecciones redundantes.  \n",
    "\n",
    "Esta arquitectura mejora notablemente los tiempos respecto a R-CNN, aunque **Selective Search sigue siendo el cuello de botella principal**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8f41c",
   "metadata": {},
   "source": [
    "Diferencias:\n",
    "\n",
    "| Etapa     | R-CNN clásica                  | Fast-RCNN                   |\n",
    "| --------- | ------------------------------ | ------------------------------------------- |\n",
    "| Proposals | Selective Search               | Selective Search                 |\n",
    "| ROIs      | No hay como tal                | Subconjunto de proposals usado por ROIPooling / Align |\n",
    "| NMS       | Final (sobre predicciones SVM) | Final (sobre predicciones del head)         |\n",
    "\n",
    "\n",
    "Conceptos:\n",
    "\n",
    "\n",
    "| Concepto      | Qué es                                                    | Cómo se obtiene                                        | Cuántos hay              | Cómo se usa                                                |\n",
    "| ------------- | --------------------------------------------------------- | ------------------------------------------------------ | ------------------------ | ---------------------------------------------------------- |\n",
    "| **Proposals** | Candidatas “donde podría haber algo”                      | De un algoritmo como **Selective Search** o un **RPN** | Miles (≈2000 por imagen) | Entrada bruta del detector                                 |\n",
    "| **ROIs**      | Subconjunto de proposals elegidas para entrenar o inferir | Se **muestrean** (pos/neg) de las proposals            | Pocas (≈128 por imagen)  | Alimentan **ROIAlign + head** para clasificación/regresión |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4fd84",
   "metadata": {},
   "source": [
    "Vamos a ver su implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee5717",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time, pathlib, shutil, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import selectivesearch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "#Train Mode\n",
    "trainFreeze = True\n",
    "trainFineTune = False\n",
    "\n",
    "#Checkpoint de modelo\n",
    "CKPT_PATH_FROZEN = \"./checkpoints_fast_rcnn/fast_rcnn_frozen_best_01.pth\"\n",
    "\n",
    "#Proposals cache, para no tener que calcularlos de nuevo\n",
    "PROPOSALS_PATH = Path(\"./checkpoints_fast_rcnn/proposals_cache_01.pkl\")\n",
    "if PROPOSALS_PATH.exists():\n",
    "    with open(PROPOSALS_PATH, \"rb\") as f:\n",
    "        PROPOSALS_CACHE = pickle.load(f)\n",
    "    print(f\"[CACHE] Loaded proposals cache ({len(PROPOSALS_CACHE)} images)\")\n",
    "else:\n",
    "    PROPOSALS_CACHE = {}\n",
    "    print(\"[CACHE] Starting with empty proposals cache\")\n",
    "# ===============================\n",
    "\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet_transforms = weights.transforms()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "VOC_ROOT = os.environ.get(\"VOC_ROOT\", \"./data/02\")\n",
    "print(\"VOC_ROOT:\", VOC_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c07cc",
   "metadata": {},
   "source": [
    "#### Nos saltamos la parte de descargar el dataset, lo vimos en el cuaderno anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbba32",
   "metadata": {},
   "source": [
    "Pasamos directamente a su carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6dda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5011\n",
      "Test: 4952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_root = \"./data/02/VOCtrainval_06-Nov-2007\"\n",
    "test_root  = \"./data/02/VOCtest_06-Nov-2007\"\n",
    "\n",
    "train_ds = VOCDetection(train_root, year=\"2007\", image_set=\"trainval\", download=False)\n",
    "test_ds  = VOCDetection(test_root,  year=\"2007\", image_set=\"test\",     download=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a126f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
      "Total: 21\n"
     ]
    }
   ],
   "source": [
    "# === Extracción automática de clases del dataset VOC ===\n",
    "\n",
    "def extract_voc_classes(dataset):\n",
    "    classes = set()\n",
    "    for i in range(min(500, len(dataset))):  # escanea solo 500 imágenes para acelerar\n",
    "        ann = dataset[i][1]['annotation']\n",
    "        objs = ann.get('object', [])\n",
    "        if isinstance(objs, dict):  # si solo hay un objeto\n",
    "            objs = [objs]\n",
    "        for obj in objs:\n",
    "            name = obj['name']\n",
    "            classes.add(name)\n",
    "    return sorted(list(classes))\n",
    "\n",
    "# Obtener clases del dataset de entrenamiento\n",
    "voc_classes = extract_voc_classes(train_ds)\n",
    "\n",
    "#Añadimos background porque es la clase 0, que no está en voc_classes, lo necesitamos para el loss. \n",
    "#Tendremos 21 clases: 20 de VOC + 1 de background.\n",
    "CLASSES = ['background'] + voc_classes\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(\"Clases detectadas:\", voc_classes)\n",
    "print(\"Total:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1a99d",
   "metadata": {},
   "source": [
    "Vamos a setear algunas constantes.\n",
    "\n",
    "Tenemos unas 2000 proposals por imagen\n",
    "Estas salen de Selective Search.\n",
    "La mayoría no contiene ningún objeto útil; algunas sí cubren bien GT.\n",
    "El objetivo es quedarse con 128 ROIs representativas para esa imagen.\n",
    "\n",
    "Para ello calcularemos el IoU entre cada proposal y todas las cajas GT. (en otra celda, mas adelante)\n",
    "\n",
    "Luego clasificamos cada proposal en Pos y Neg en base a los thresholds del IoU y poderamos las imagenes que nos queremos quedar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c0a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num clases: 21\n",
      "Sampler: 256 (FG 128, BG 128)\n",
      "PosIoU≥0.3, NegIoU≤0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# IoU thresholds\n",
    "POS_IOU_THRESH = 0.3\n",
    "NEG_IOU_THRESH = 0.3\n",
    "\n",
    "# Sampler config\n",
    "# Tomamos 128 ROIs por imagen.\n",
    "ROIS_PER_IMG = 256\n",
    "# 25% de esas ROIs son positivas (foreground) y el resto 75% negativas (background).\n",
    "# Usamos esta distribución porque es lo que da mejores resultados en la literatura.\n",
    "FG_FRACTION = 0.5# ~32 pos + 96 neg \n",
    "\n",
    "# BBox delta normalization\n",
    "# Normalizamos las coordenadas de los bounding boxes para que sean más estables.\n",
    "#Son valores que usamos fijos para todo el entrenamiento y que se han obtenido empíricamente.\n",
    "#Son un estándar en la literatura.\n",
    "BBOX_MEANS = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
    "BBOX_STDS  = torch.tensor([0.1, 0.1, 0.2, 0.2], dtype=torch.float32)\n",
    "\n",
    "# Proposals\n",
    "# Tomamos hasta 2000 propuestas por imagen.\n",
    "MAX_PROPOSALS_PER_IMG = 2000\n",
    "# Las propuestas mínimas tienen 16px de lado.\n",
    "MIN_SIZE = 16\n",
    "\n",
    "print(f\"Num clases: {NUM_CLASSES}\")\n",
    "print(f\"Sampler: {ROIS_PER_IMG} (FG {int(ROIS_PER_IMG*FG_FRACTION)}, BG {ROIS_PER_IMG-int(ROIS_PER_IMG*FG_FRACTION)})\")\n",
    "print(f\"PosIoU≥{POS_IOU_THRESH}, NegIoU≤{NEG_IOU_THRESH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327891d",
   "metadata": {},
   "source": [
    "Ahora necesitamos redimensionar las imagenes, mantendremos un tamaño fijo para el lado más corto (600) y uno variable de maximo 1000 para el lado largo, de desta manera podemos mantener el factor de escala, este facto de escala tendremos que aplicarlo también a los BBs para que la proyección sea coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476f11b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "#Definimos el tamaño fijo para el lado más corto y el máximo para el lado largo\n",
    "short_side = 600\n",
    "max_side = 1000\n",
    "\n",
    "#Obtenemos la media y la desviación estándar de ResNet para la normalización\n",
    "R50_MEAN = resnet_transforms.mean\n",
    "R50_STD = resnet_transforms.std\n",
    "print(R50_MEAN, R50_STD)\n",
    "\n",
    "#Llamaremos a esta función para preprocesar las imágenes\n",
    "def preprocess_image(img, boxes, device=device, short_side=short_side, max_side=max_side):\n",
    "    \n",
    "    #Objeto PIL de la imagen\n",
    "    w, h = img.size #Ej (500, 333)\n",
    "    #Obtenemos el tamaño más corto y el más largo de la imagen\n",
    "    actual_short = min(w, h) #Ej 333\n",
    "    actual_long  = max(w, h) #Ej 500\n",
    "\n",
    "    scale_multiplier = short_side / actual_short\n",
    "    \n",
    "    target_long = actual_long * scale_multiplier\n",
    "\n",
    "    if target_long > max_side:\n",
    "        scale_multiplier = max_side / actual_long #porque max_side = actual_long * scale_multiplier\n",
    "\n",
    "    #Calculamos el nuevo tamaño de la imagen\n",
    "    w = int(w * scale_multiplier)\n",
    "    h = int(h * scale_multiplier)\n",
    "\n",
    "    #Redimensionamos la imagen\n",
    "    img_resized = img.resize((w, h), Image.BILINEAR)\n",
    "\n",
    "    #Ajustamos los BBs\n",
    "    #Los BBs vienen en formato (xmin, ymin, xmax, ymax)\n",
    "    #Es un tensor de 4 columnas, cada fila es un BB\n",
    "    #Multiplicamos cada columna por el factor de escala\n",
    "    boxes = boxes * scale_multiplier\n",
    "\n",
    "    #Convertimos la imagen a tensor\n",
    "    img_t = to_tensor(img_resized)\n",
    "\n",
    "    #Normalizamos la imagen \n",
    "    img_t_norm = normalize(img_t, R50_MEAN, R50_STD)\n",
    "\n",
    "    #Los boxes NO se normalizan, se mantienen en las coordenadas originales porque son relativas a la imagen.\n",
    "    \n",
    "    return img_t_norm, boxes, scale_multiplier, img_resized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a37215",
   "metadata": {},
   "source": [
    "Bien, ahora, necesitamos una funcion que reciba una imagen  y extraiga las bbs de esa imagen en un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59004ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "def extract_gt_boxes_and_labels(annotation):\n",
    "    objs = annotation[\"annotation\"][\"object\"]\n",
    "    if isinstance(objs, dict):\n",
    "        objs = [objs]\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for o in objs:\n",
    "        bb = o[\"bndbox\"]\n",
    "        xmin = float(bb[\"xmin\"])\n",
    "        ymin = float(bb[\"ymin\"])\n",
    "        xmax = float(bb[\"xmax\"])\n",
    "        ymax = float(bb[\"ymax\"])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        cls_name = o[\"name\"]\n",
    "        labels.append(CLASS_TO_IDX[cls_name])\n",
    "\n",
    "    return (\n",
    "        torch.tensor(boxes, dtype=torch.float32),\n",
    "        torch.tensor(labels, dtype=torch.int64)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd17c9d",
   "metadata": {},
   "source": [
    "Ahora tendremos que crear una funcion que calcule los proposals con selective search de una imagen ya preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13a9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_selective_proposals(img_pil, max_proposals=MAX_PROPOSALS_PER_IMG, min_size=MIN_SIZE):\n",
    "    img_np = np.array(img_pil)\n",
    "\n",
    "    #Llamamos a selective search\n",
    "    _, regions = selectivesearch.selective_search(\n",
    "        img_np,\n",
    "        scale=500,\n",
    "        sigma=0.9,\n",
    "        min_size=10\n",
    "    )\n",
    "\n",
    "    #Evitamos duplicados\n",
    "    seen = set()\n",
    "    #Guardamos las propuestas\n",
    "    proposals = []\n",
    "\n",
    "    #Recorremos las regiones\n",
    "    for r in regions:\n",
    "        #Obtenemos las coordenadas de la región\n",
    "        x, y, w, h = r[\"rect\"]\n",
    "\n",
    "        #Evitamos duplicados\n",
    "        if (x, y, w, h) in seen:\n",
    "            continue\n",
    "\n",
    "        #Añadimos la región a las ya vistas\n",
    "        seen.add((x, y, w, h))\n",
    "\n",
    "        #Evitamos regiones demasiado pequeñas\n",
    "        if w < min_size or h < min_size:\n",
    "            continue\n",
    "\n",
    "        #Añadimos la región a las propuestas\n",
    "        proposals.append([x, y, x + w, y + h])\n",
    "\n",
    "        #Si tenemos más propuestas que el máximo, salimos\n",
    "        if len(proposals) >= max_proposals:\n",
    "            break\n",
    "\n",
    "    #Si no hay propuestas, devolvemos un tensor de 0 filas y 4 columnas\n",
    "    # A veces ss no encuentra ninguna región, por lo que devolvemos un tensor de 0 filas y 4 columnas\n",
    "    # Si no lo hicieramos, explotaría el pipeline.\n",
    "    if len(proposals) == 0:\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    #Devolvemos las propuestas\n",
    "    return torch.tensor(proposals, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1837f0dd",
   "metadata": {},
   "source": [
    "Implementamos IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decedaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_iou(box1, box2):\n",
    "\n",
    "    inter_x1 = max(box1[0], box2[0]) \n",
    "    inter_y1 = max(box1[1], box2[1]) \n",
    "    inter_x2 = min(box1[2], box2[2]) \n",
    "    inter_y2 = min(box1[3], box2[3])     \n",
    "\n",
    "    inter_w = max(0, inter_x2 - inter_x1)\n",
    "    inter_h = max(0, inter_y2 - inter_y1)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # áreas de las cajas\n",
    "    area1 = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    area2 = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - inter_area\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return inter_area / union #IoU\n",
    "\n",
    "\n",
    "def iou_with_many(box, gt_boxes):\n",
    "    ious = []\n",
    "\n",
    "    for gt_box in gt_boxes:\n",
    "        ious.append(get_iou(box, gt_box))\n",
    "    return torch.tensor(ious)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8eeca6",
   "metadata": {},
   "source": [
    "Implementamos una funcion para poder computar la matriz de proposals y gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d905c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposals: tensor [P,4] en coords de la imagen reescalada\n",
    "# boxes_t:  tensor [K,4] GT reescaladas (lo que ya tenías)\n",
    "\n",
    "def compute_ious_matrix(proposals, gt_boxes):\n",
    "    if proposals.numel() == 0 or gt_boxes.numel() == 0:\n",
    "        return torch.zeros((proposals.shape[0], gt_boxes.shape[0]), dtype=torch.float32, device=proposals.device)\n",
    "\n",
    "    proposals = proposals.to(gt_boxes.device)\n",
    "\n",
    "    p = proposals[:, None, :]   # [P,1,4]\n",
    "    g = gt_boxes[None, :, :]    # [1,K,4]\n",
    "\n",
    "    inter_x1 = torch.maximum(p[..., 0], g[..., 0])\n",
    "    inter_y1 = torch.maximum(p[..., 1], g[..., 1])\n",
    "    inter_x2 = torch.minimum(p[..., 2], g[..., 2])\n",
    "    inter_y2 = torch.minimum(p[..., 3], g[..., 3])\n",
    "\n",
    "    inter_w = (inter_x2 - inter_x1).clamp(min=0)\n",
    "    inter_h = (inter_y2 - inter_y1).clamp(min=0)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    area_p = ((p[..., 2] - p[..., 0]).clamp(min=0) *\n",
    "              (p[..., 3] - p[..., 1]).clamp(min=0))\n",
    "    area_g = ((g[..., 2] - g[..., 0]).clamp(min=0) *\n",
    "              (g[..., 3] - g[..., 1]).clamp(min=0))\n",
    "\n",
    "    union = area_p + area_g - inter_area\n",
    "    ious = inter_area / union.clamp(min=1e-6)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7e567",
   "metadata": {},
   "source": [
    "Ahora toca: clasificar proposals como FG/BG en base al IoU y ver cuántas tienes de cada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060fb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fg_bg_inds(maxiou, proposals=None, verbose=False):\n",
    "    #Esto compara cada proposal con cada GT y crea un tensor de booleanos.\n",
    "    fg_mask = maxiou >= POS_IOU_THRESH\n",
    "    bg_mask = maxiou <  NEG_IOU_THRESH\n",
    "    ig_mask = (maxiou >= NEG_IOU_THRESH) & (maxiou < POS_IOU_THRESH)\n",
    "\n",
    "    #Esto obtiene los índices de los proposals que cumplen la máscara, hacemos squeeze(1) para que sea un tensor 1D, ya que torch.nonzero() devuelve un tensor 2D.\n",
    "    fg_inds = torch.nonzero(fg_mask).squeeze(1)\n",
    "    bg_inds = torch.nonzero(bg_mask).squeeze(1)\n",
    "    ig_inds = torch.nonzero(ig_mask).squeeze(1)\n",
    "\n",
    "\n",
    "    return fg_inds, bg_inds, ig_inds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba6a49",
   "metadata": {},
   "source": [
    " Vamos a preparar nuestro backbone; cortaremos en layer3 para quedarnos con un stride de 16, lo elegimos porque es el valor estandar para Fast RCNN (mejor RoiPooling)\n",
    "\n",
    "El stride lo podemos calcular dividiendo la dimensión espacial de la imagen de entrada entre la dimensión espacial del feature map resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 64, 256, 256]) stride acumulado = 2.0\n",
      "1 torch.Size([1, 64, 256, 256]) stride acumulado = 2.0\n",
      "2 torch.Size([1, 64, 256, 256]) stride acumulado = 2.0\n",
      "3 torch.Size([1, 64, 128, 128]) stride acumulado = 4.0\n",
      "4 torch.Size([1, 256, 128, 128]) stride acumulado = 4.0\n",
      "5 torch.Size([1, 512, 64, 64]) stride acumulado = 8.0\n",
      "6 torch.Size([1, 1024, 32, 32]) stride acumulado = 16.0\n",
      "16.0 7\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "#Importamos el modelo de ResNet50\n",
    "backbone = nn.Sequential(*list(models.resnet50(weights=weights).children())).to(device)\n",
    "#Creamos un tensor de prueba, usamos \n",
    "x = torch.randn(1, 3, 512, 512).to(device)\n",
    "feat = x.clone()\n",
    "\n",
    "#Calculamos el stride de cada capa\n",
    "for name, module in backbone.named_children():\n",
    "    feat = module(feat)\n",
    "    current_stride = 512 / feat.shape[2]\n",
    "    print(name, feat.shape, \"stride acumulado =\", current_stride)\n",
    "    if ( current_stride == 16):\n",
    "        break\n",
    "BACKBONE_CUT = int(name) + 1 #Sumamos 1 por que empieza en 0\n",
    "print(current_stride, BACKBONE_CUT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda14cb",
   "metadata": {},
   "source": [
    "Ahora vamos a cortar la red por donde vimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cortamos el backbone en el layer3 que es donde tenemos stride 16 que vimos antes.\n",
    "\n",
    "backbone = nn.Sequential(*list(models.resnet50(weights=weights).children())[:BACKBONE_CUT]).to(device)\n",
    "\n",
    "\n",
    "# Para una primera fase de entrenamiento congelamos los parametros del backbone\n",
    "if trainFreeze:\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Descongelamos solo los últimos 2 módulos del backbone truncado\n",
    "    # Ajustar [-1:], [-3:], segun cuantas capas queremos descongelar\n",
    "    for m in list(backbone.children())[-2:]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "#Cuando queramos reentrenar el modelo, tenemos que descongelar algunas capas del backbone\n",
    "elif trainFineTune:\n",
    "    # Primero congelamos todo\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Descongelamos solo los últimos 2 módulos del backbone truncado\n",
    "    # Ajustar [-1:], [-3:], segun cuantas capas queremos descongelar\n",
    "    for m in list(backbone.children())[-2:]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0221bc",
   "metadata": {},
   "source": [
    "### ROIAlign: qué hace y por qué se usa\n",
    "\n",
    "En Fast R-CNN original se usaba ROI Pooling, que tenía un problema importante: **redondeaba las coordenadas** de las ROIs al proyectarlas sobre el feature map. Ese redondeo generaba errores de alineamiento y hacía que las features no correspondieran exactamente con la región real del objeto.\n",
    "\n",
    "**ROIAlign** elimina ese problema.\n",
    "\n",
    "Dado un feature map de la CNN:\n",
    "[C, Hf, Wf]\n",
    "\n",
    "y una ROI en coordenadas de imagen:\n",
    "(x1, y1, x2, y2)\n",
    "\n",
    "ROIAlign sigue estos pasos:\n",
    "\n",
    "1. Proyecta la ROI al feature map dividiendo por el stride (≈16 en ResNet50-C4):\n",
    "   (x1/16, y1/16, x2/16, y2/16)\n",
    "\n",
    "2. Recorta exactamente esa región del feature map **sin redondear**.\n",
    "\n",
    "3. Remuestrea la región usando **interpolación bilineal** para mantener continuidad espacial.\n",
    "\n",
    "4. Devuelve un tensor de tamaño fijo, por ejemplo:\n",
    "   [C, 7, 7]\n",
    "\n",
    "El resultado final de procesar N ROIs es:\n",
    "[N, C, 7, 7]\n",
    "\n",
    "Esto permite alimentar capas fully connected con bloques de tamaño constante, independientemente del tamaño original de cada ROI, y garantiza alineamiento preciso entre la imagen y las features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db48581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from torchvision.ops import roi_align\n",
    "\n",
    "ROI_OUTPUT_SIZE = (7, 7)\n",
    "SPATIAL_SCALE = 1.0 / BACKBONE_STRIDE\n",
    "ROI_SAMPLING_RATIO = 2\n",
    "\n",
    "def build_rois_from_proposals(proposals, batch_idx=0):\n",
    "    if not torch.is_tensor(proposals):\n",
    "        proposals = torch.as_tensor(proposals, dtype=torch.float32)\n",
    "    batch_inds = torch.full((proposals.shape[0], 1), batch_idx, dtype=proposals.dtype, device=proposals.device)\n",
    "    rois = torch.cat([batch_inds, proposals], dim=1)\n",
    "    return rois\n",
    "\n",
    "def extract_roi_features(feat_map, rois):\n",
    "    return roi_align(\n",
    "        feat_map,\n",
    "        rois,\n",
    "        output_size=ROI_OUTPUT_SIZE,\n",
    "        spatial_scale=SPATIAL_SCALE,\n",
    "        sampling_ratio=ROI_SAMPLING_RATIO,\n",
    "        aligned=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8bbe3",
   "metadata": {},
   "source": [
    "Vamos a crear la funcion que crea los bbox targets\n",
    "\n",
    "Los bbtargets son la operación que hay que aplicar a la proposal para que se convierta en su GT correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bb5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bbox_targets(proposals, gt_boxes, means, stds):\n",
    "    p = proposals\n",
    "    g = gt_boxes\n",
    "\n",
    "    px = (p[:, 0] + p[:, 2]) / 2\n",
    "    py = (p[:, 1] + p[:, 3]) / 2\n",
    "    pw = (p[:, 2] - p[:, 0]).clamp(min=1e-6)\n",
    "    ph = (p[:, 3] - p[:, 1]).clamp(min=1e-6)\n",
    "\n",
    "    gx = (g[:, 0] + g[:, 2]) / 2\n",
    "    gy = (g[:, 1] + g[:, 3]) / 2\n",
    "    gw = (g[:, 2] - g[:, 0]).clamp(min=1e-6)\n",
    "    gh = (g[:, 3] - g[:, 1]).clamp(min=1e-6)\n",
    "\n",
    "    tx = (gx - px) / pw\n",
    "    ty = (gy - py) / ph\n",
    "    tw = torch.log(gw / pw)\n",
    "    th = torch.log(gh / ph)\n",
    "\n",
    "    deltas = torch.stack([tx, ty, tw, th], dim=1)\n",
    "\n",
    "    if isinstance(means, torch.Tensor):\n",
    "        means_t = means.to(deltas.device, deltas.dtype)\n",
    "    else:\n",
    "        means_t = torch.tensor(means, device=deltas.device, dtype=deltas.dtype)\n",
    "\n",
    "    if isinstance(stds, torch.Tensor):\n",
    "        stds_t = stds.to(deltas.device, deltas.dtype)\n",
    "    else:\n",
    "        stds_t = torch.tensor(stds, device=deltas.device, dtype=deltas.dtype)\n",
    "\n",
    "    return (deltas - means_t) / stds_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadc076",
   "metadata": {},
   "source": [
    "Vamos a construir nuestro pipeline usando los metodos definidos, procesará una imagen por vez\n",
    " antes de pasar a la RCNN como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a804a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_boxes(proposals, bbox_deltas, means=BBOX_MEANS, stds=BBOX_STDS):\n",
    "    \"\"\"\n",
    "    Convierte las transformaciones predichas (bbox_deltas) en coordenadas absolutas de cajas.\n",
    "    \n",
    "    Es la operación INVERSA de compute_bbox_targets.\n",
    "    \n",
    "    Args:\n",
    "        proposals: tensor [N, 4] con formato (x1, y1, x2, y2) - cajas originales\n",
    "        bbox_deltas: tensor [N, 4] con formato (dx, dy, dw, dh) - transformaciones predichas por el modelo\n",
    "        means: tensor [4] - medias usadas en la normalización\n",
    "        stds: tensor [4] - desviaciones estándar usadas en la normalización\n",
    "        \n",
    "    Returns:\n",
    "        tensor [N, 4] con formato (x1, y1, x2, y2) - cajas finales predichas\n",
    "    \"\"\"\n",
    "    # Denormalizar los deltas\n",
    "    if isinstance(means, torch.Tensor):\n",
    "        means_t = means.to(bbox_deltas.device, bbox_deltas.dtype)\n",
    "    else:\n",
    "        means_t = torch.tensor(means, device=bbox_deltas.device, dtype=bbox_deltas.dtype)\n",
    "    \n",
    "    if isinstance(stds, torch.Tensor):\n",
    "        stds_t = stds.to(bbox_deltas.device, bbox_deltas.dtype)\n",
    "    else:\n",
    "        stds_t = torch.tensor(stds, device=bbox_deltas.device, dtype=bbox_deltas.dtype)\n",
    "    \n",
    "    # Revertir la normalización: deltas_real = deltas_norm * std + mean\n",
    "    bbox_deltas = bbox_deltas * stds_t + means_t\n",
    "    \n",
    "    # Extraer centro y dimensiones de las proposals\n",
    "    px = (proposals[:, 0] + proposals[:, 2]) / 2\n",
    "    py = (proposals[:, 1] + proposals[:, 3]) / 2\n",
    "    pw = (proposals[:, 2] - proposals[:, 0]).clamp(min=1e-6)\n",
    "    ph = (proposals[:, 3] - proposals[:, 1]).clamp(min=1e-6)\n",
    "    \n",
    "    # Extraer los deltas predichos\n",
    "    dx = bbox_deltas[:, 0]\n",
    "    dy = bbox_deltas[:, 1]\n",
    "    dw = bbox_deltas[:, 2]\n",
    "    dh = bbox_deltas[:, 3]\n",
    "    \n",
    "    # Aplicar las transformaciones inversas:\n",
    "    # tx = (gx - px) / pw  -->  gx = px + tx * pw\n",
    "    # ty = (gy - py) / ph  -->  gy = py + ty * ph\n",
    "    # tw = log(gw / pw)    -->  gw = pw * exp(tw)\n",
    "    # th = log(gh / ph)    -->  gh = ph * exp(th)\n",
    "    \n",
    "    pred_cx = px + dx * pw\n",
    "    pred_cy = py + dy * ph\n",
    "    pred_w = pw * torch.exp(dw)\n",
    "    pred_h = ph * torch.exp(dh)\n",
    "    \n",
    "    # Convertir de (cx, cy, w, h) a (x1, y1, x2, y2)\n",
    "    x1 = pred_cx - pred_w / 2\n",
    "    y1 = pred_cy - pred_h / 2\n",
    "    x2 = pred_cx + pred_w / 2\n",
    "    y2 = pred_cy + pred_h / 2\n",
    "    \n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4685ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(img, ann):\n",
    "    gt_boxes, gt_labels = extract_gt_boxes_and_labels(ann)\n",
    "\n",
    "    img_t, gt_boxes_t, scale, img_resized = preprocess_image(img, gt_boxes)\n",
    "\n",
    "    filename = ann[\"annotation\"][\"filename\"]\n",
    "\n",
    "    if filename in PROPOSALS_CACHE:\n",
    "        proposals_t = PROPOSALS_CACHE[filename].to(device)\n",
    "    else:\n",
    "        proposals = get_selective_proposals(img_resized)\n",
    "        proposals_t = torch.as_tensor(proposals, dtype=torch.float32)\n",
    "        PROPOSALS_CACHE[filename] = proposals_t.cpu()\n",
    "        proposals_t = proposals_t.to(device)\n",
    "\n",
    "    ious = compute_ious_matrix(proposals_t, gt_boxes_t.to(device))\n",
    "    max_iou, gt_idx = ious.max(dim=1)\n",
    "\n",
    "    fg_inds, bg_inds, ig_inds = get_fg_bg_inds(max_iou, proposals_t, verbose=False)\n",
    "\n",
    "    num_fg_total = fg_inds.numel()\n",
    "    num_bg_total = bg_inds.numel()\n",
    "\n",
    "    num_fg = min(int(ROIS_PER_IMG * FG_FRACTION), num_fg_total)\n",
    "    num_bg = min(ROIS_PER_IMG - num_fg, num_bg_total)\n",
    "\n",
    "    if num_fg > 0:\n",
    "        fg_sampled = fg_inds[torch.randperm(num_fg_total, device=device)[:num_fg]]\n",
    "    else:\n",
    "        fg_sampled = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "    if num_bg > 0:\n",
    "        bg_sampled = bg_inds[torch.randperm(num_bg_total, device=device)[:num_bg]]\n",
    "    else:\n",
    "        bg_sampled = torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "    if fg_sampled.numel() == 0 and bg_sampled.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    sampled_inds = torch.cat([fg_sampled, bg_sampled], dim=0)\n",
    "    proposals_sampled = proposals_t[sampled_inds]\n",
    "\n",
    "    rois = build_rois_from_proposals(proposals_sampled, batch_idx=0)\n",
    "\n",
    "    img_t = img_t.unsqueeze(0).to(device)\n",
    "\n",
    "    if trainFreeze:\n",
    "        feat_map = backbone(img_t)\n",
    "        roi_feats = extract_roi_features(feat_map, rois)\n",
    "    elif trainFineTune:\n",
    "        feat_map = backbone(img_t)\n",
    "        roi_feats = extract_roi_features(feat_map, rois)\n",
    "\n",
    "    gt_idx_sampled = gt_idx[sampled_inds]\n",
    "\n",
    "    return roi_feats, proposals_sampled, gt_idx_sampled, gt_labels, gt_boxes_t, num_fg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b1a85",
   "metadata": {},
   "source": [
    "Hagamos un resumen antes de definir el pipeline completo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1442a9a",
   "metadata": {},
   "source": [
    "- **roi_feats**  \n",
    "  Features de cada ROI después de ROIAlign.  \n",
    "  Shape: `[N_rois, C, 7, 7]`.  \n",
    "  Orden: primero todas las ROIs FG, luego todas las BG.\n",
    "\n",
    "- **proposals_sampled**  \n",
    "  Cajas de esas mismas ROIs, en coordenadas de la imagen preprocesada.  \n",
    "  Shape: `[N_rois, 4]`.  \n",
    "  Mismo orden que `roi_feats`.\n",
    "\n",
    "- **gt_idx_sampled**  \n",
    "  Para cada ROI, índice de la GT con mayor IoU.  \n",
    "  Shape: `[N_rois]`.  \n",
    "  Mismo orden que `roi_feats`.  \n",
    "  Las ROIs BG también tienen un `gt_idx_sampled[i]`, aunque solo se usa realmente para FG.\n",
    "\n",
    "- **gt_labels**  \n",
    "  Clases de cada GT box.  \n",
    "  Shape: `[num_gt]`.  \n",
    "  Valores típicos: `1..NUM_CLASSES-1` (sin background).\n",
    "\n",
    "- **gt_boxes_t**  \n",
    "  Cajas GT reescaladas a la imagen preprocesada.  \n",
    "  Shape: `[num_gt, 4]`.\n",
    "\n",
    "- **num_fg**  \n",
    "  Número de ROIs positivas (FG) dentro de esos `N_rois`.  \n",
    "  Interpretación: índices `[0 .. num_fg-1]` → FG; índices `[num_fg .. N_rois-1]` → BG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f12b26",
   "metadata": {},
   "source": [
    "Ahora vamos as definir nuestra HEAD para clasificar las features que sacamos antes\n",
    "Como tenemos dos salidas (regresion de las bboxes y clasificacion de los scores)\n",
    "Necesitaremos dos tipos de calculo de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa43a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class FastRCNNHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parte común de la red\n",
    "        self.fc1 = nn.Linear(in_channels * 7 * 7, 1024) \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "\n",
    "        # Parte de clasificación de los labels de las proposiciones\n",
    "        self.cls_score = nn.Linear(1024, num_classes)\n",
    "\n",
    "        # Parte de regresión de las bounding boxes\n",
    "        self.bbox_pred = nn.Linear(1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #Forward de la clasificación\n",
    "        scores = self.cls_score(x)\n",
    "        #Forward de la regresión\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "        return scores, bbox_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20916e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea1b985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CKPT] No frozen checkpoint found, fine-tune will start from random head\n"
     ]
    }
   ],
   "source": [
    "IN_CHANNELS = 1024\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rcnn_head = FastRCNNHead(IN_CHANNELS, NUM_CLASSES).to(device)\n",
    "\n",
    "#Cargamos el checkpoint si existe\n",
    "if trainFineTune:\n",
    "    ckpt = torch.load(CKPT_PATH_FROZEN, map_location=device)\n",
    "    rcnn_head.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    print(f\"[CKPT] Loaded frozen head from epoch {ckpt['epoch']} (best_val_loss={ckpt['best_val_loss']:.4f})\")\n",
    "else:\n",
    "    print(\"[CKPT] No frozen checkpoint found, fine-tune will start from random head\")\n",
    "\n",
    "\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "reg_criterion = nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "if trainFreeze:\n",
    "    params_backbone = [p for p in backbone.parameters() if p.requires_grad]\n",
    "    params_head = list(rcnn_head.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": params_backbone, \"lr\": 1e-5},\n",
    "            {\"params\": params_head, \"lr\": 1e-4},\n",
    "        ]\n",
    "    )\n",
    "elif trainFineTune:\n",
    "    params_backbone = [p for p in backbone.parameters() if p.requires_grad]\n",
    "    params_head = list(rcnn_head.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": params_backbone, \"lr\": 1e-5},\n",
    "            {\"params\": params_head, \"lr\": 1e-4},\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aea296",
   "metadata": {},
   "source": [
    "Ahora definimos el loop general para procesar todas las imagenes de train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4914dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def voc_collate(batch):\n",
    "    img, ann = batch[0]\n",
    "    return img, ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8947797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=voc_collate,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "def TrainPipeline(max_iters=None):\n",
    "    running_cls = 0.0\n",
    "    running_reg = 0.0\n",
    "    running_total = 0.0\n",
    "    step = 0\n",
    "\n",
    "    for i, (img, ann) in enumerate(train_loader, start=1):\n",
    "        if max_iters is not None and i > max_iters:\n",
    "            break\n",
    "\n",
    "        out = build_sample(img, ann)\n",
    "        if out is None:\n",
    "            continue\n",
    "\n",
    "        roi_feats, proposals_sampled, gt_idx_sampled, gt_labels, gt_boxes_t, num_fg = out\n",
    "\n",
    "        device_ = proposals_sampled.device\n",
    "\n",
    "        gt_labels = gt_labels.to(device_)\n",
    "        gt_idx_sampled = gt_idx_sampled.to(device_)\n",
    "        gt_boxes_t = gt_boxes_t.to(device_)\n",
    "        proposals_sampled = proposals_sampled.to(device_)\n",
    "        roi_feats = roi_feats.to(device_)\n",
    "\n",
    "        roi_labels = torch.zeros_like(gt_idx_sampled, dtype=torch.long)\n",
    "        if num_fg > 0:\n",
    "            roi_labels[:num_fg] = gt_labels[gt_idx_sampled[:num_fg]]\n",
    "\n",
    "        bbox_targets = torch.zeros_like(proposals_sampled, dtype=torch.float32)\n",
    "        if num_fg > 0:\n",
    "            fg_props = proposals_sampled[:num_fg]\n",
    "            fg_gt = gt_boxes_t[gt_idx_sampled[:num_fg]]\n",
    "            bbox_targets[:num_fg] = compute_bbox_targets(\n",
    "                fg_props,\n",
    "                fg_gt,\n",
    "                BBOX_MEANS,\n",
    "                BBOX_STDS\n",
    "            )\n",
    "\n",
    "        scores, bbox_deltas = rcnn_head(roi_feats)\n",
    "\n",
    "        loss_cls = cls_criterion(scores, roi_labels)\n",
    "        if num_fg > 0:\n",
    "            loss_reg = reg_criterion(bbox_deltas[:num_fg], bbox_targets[:num_fg])\n",
    "        else:\n",
    "            loss_reg = torch.tensor(0.0, device=device_)\n",
    "\n",
    "        loss_total = loss_cls + loss_reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cls += loss_cls.item()\n",
    "        running_reg += loss_reg.item()\n",
    "        running_total += loss_total.item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"[TRAIN] iter={i} step={step} cls={loss_cls.item():.4f} reg={loss_reg.item():.4f} total={loss_total.item():.4f}\")\n",
    "\n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            mean_cls = running_cls / step\n",
    "            mean_reg = running_reg / step\n",
    "            mean_total = running_total / step\n",
    "\n",
    "            print(\n",
    "                f\"{RED}[LOG] step={step} | mean_cls={mean_cls:.4f} | mean_reg={mean_reg:.4f} | mean_total={mean_total:.4f}{RESET}\"\n",
    "            )\n",
    "\n",
    "    if step > 0:\n",
    "        mean_cls = running_cls / step\n",
    "        mean_reg = running_reg / step\n",
    "        mean_total = running_total / step\n",
    "        print(\n",
    "            f\"{RED}[EPOCH] steps={step} | mean_cls={mean_cls:.4f} | mean_reg={mean_reg:.4f} | mean_total={mean_total:.4f}{RESET}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab358c58",
   "metadata": {},
   "source": [
    "Definimos un pipeline similar para validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eed9aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=voc_collate,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "def EvalPipeline(dataloader=test_loader, max_iters=None):\n",
    "    rcnn_head.eval()\n",
    "    backbone.eval()\n",
    "\n",
    "    running_cls = 0.0\n",
    "    running_reg = 0.0\n",
    "    running_total = 0.0\n",
    "    step = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, ann) in enumerate(dataloader, start=1):\n",
    "            if max_iters is not None and i > max_iters:\n",
    "                break\n",
    "\n",
    "            out = build_sample(img, ann)\n",
    "            if out is None:\n",
    "                continue\n",
    "\n",
    "            roi_feats, proposals_sampled, gt_idx_sampled, gt_labels, gt_boxes_t, num_fg = out\n",
    "\n",
    "            device_ = proposals_sampled.device\n",
    "\n",
    "            gt_labels = gt_labels.to(device_)\n",
    "            gt_idx_sampled = gt_idx_sampled.to(device_)\n",
    "            gt_boxes_t = gt_boxes_t.to(device_)\n",
    "            proposals_sampled = proposals_sampled.to(device_)\n",
    "            roi_feats = roi_feats.to(device_)\n",
    "\n",
    "            roi_labels = torch.zeros_like(gt_idx_sampled, dtype=torch.long)\n",
    "            if num_fg > 0:\n",
    "                roi_labels[:num_fg] = gt_labels[gt_idx_sampled[:num_fg]]\n",
    "\n",
    "            bbox_targets = torch.zeros_like(proposals_sampled, dtype=torch.float32)\n",
    "            if num_fg > 0:\n",
    "                fg_props = proposals_sampled[:num_fg]\n",
    "                fg_gt = gt_boxes_t[gt_idx_sampled[:num_fg]]\n",
    "                bbox_targets[:num_fg] = compute_bbox_targets(\n",
    "                    fg_props,\n",
    "                    fg_gt,\n",
    "                    BBOX_MEANS,\n",
    "                    BBOX_STDS\n",
    "                )\n",
    "\n",
    "            scores, bbox_deltas = rcnn_head(roi_feats)\n",
    "\n",
    "            loss_cls = cls_criterion(scores, roi_labels)\n",
    "\n",
    "            if num_fg > 0:\n",
    "                bbox_deltas_fg = bbox_deltas[:num_fg]\n",
    "                bbox_targets_fg = bbox_targets[:num_fg]\n",
    "                loss_reg = reg_criterion(bbox_deltas_fg, bbox_targets_fg)\n",
    "            else:\n",
    "                loss_reg = torch.tensor(0.0, device=device_)\n",
    "\n",
    "            loss_total = loss_cls + loss_reg\n",
    "\n",
    "            running_cls += loss_cls.item()\n",
    "            running_reg += loss_reg.item()\n",
    "            running_total += loss_total.item()\n",
    "            step += 1\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"[VAL-STEP] iter={i} step={step}\")\n",
    "\n",
    "    rcnn_head.train()\n",
    "    backbone.train()\n",
    "\n",
    "    if step == 0:\n",
    "        return {\"val_loss_cls\": None, \"val_loss_reg\": None, \"val_loss_total\": None}\n",
    "\n",
    "    mean_cls = running_cls / step\n",
    "    mean_reg = running_reg / step\n",
    "    mean_total = running_total / step\n",
    "\n",
    "    print(\n",
    "        f\"{BLUE}[VAL] steps={step} | val_cls={mean_cls:.4f} | val_reg={mean_reg:.4f} | val_total={mean_total:.4f}{RESET}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"val_loss_cls\": mean_cls,\n",
    "        \"val_loss_reg\": mean_reg,\n",
    "        \"val_loss_total\": mean_total\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "YELLOW = \"\\033[93m\"\n",
    "RESET = \"\\033[0m\"\n",
    "import pickle\n",
    "#Entrenamos el modelo congelado si trainFreeze es True\n",
    "if trainFreeze:\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"{YELLOW}\\n=== Epoch {epoch+1}/{num_epochs} ==={RESET}\")\n",
    "\n",
    "        TrainPipeline()  # sin max_iters → una época completa sobre train_ds\n",
    "\n",
    "        val_metrics = EvalPipeline(max_iters=50)\n",
    "        print(val_metrics)\n",
    "\n",
    "        val_total = val_metrics[\"val_loss_total\"]\n",
    "        if val_total is not None and val_total < best_val_loss:\n",
    "            best_val_loss = val_total\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"backbone_state_dict\": backbone.state_dict(),\n",
    "                    \"rcnn_head_state_dict\": rcnn_head.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                },\n",
    "                CKPT_PATH_FROZEN,\n",
    "            )\n",
    "            print(f\"{BLUE}[CKPT] Nuevo mejor modelo en epoch {best_epoch} con val_total={val_total:.4f}{RESET}\")\n",
    "        \n",
    "        #Guardamos el cache de proposals para no tener que calcularlo de nuevo\n",
    "        with open(\"./checkpoints_fast_rcnn/proposals_cache.pkl\", \"wb\") as f:\n",
    "            pickle.dump(PROPOSALS_CACHE, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4856c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"./checkpoints_fast_rcnn/fast_rcnn_head_refined.pth\", map_location=device)\n",
    "backbone.load_state_dict(ckpt[\"backbone_state_dict\"])\n",
    "rcnn_head.load_state_dict(ckpt[\"rcnn_head_state_dict\"])\n",
    "\n",
    "# congelar backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# solo entrenar head\n",
    "for p in rcnn_head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    rcnn_head.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=1,\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "total_epochs = 8\n",
    "\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    TrainPipeline()\n",
    "\n",
    "    val_loss_total = val_metrics[\"val_loss_total\"]\n",
    "    print(f\"[HEAD-ONLY] epoch={epoch} val_loss_total={val_loss_total:.4f}\")\n",
    "\n",
    "    scheduler.step(val_loss_total)\n",
    "\n",
    "    if val_loss_total < best_val_loss:\n",
    "        best_val_loss = val_loss_total\n",
    "        torch.save(\n",
    "            {\n",
    "                \"backbone_state_dict\": backbone.state_dict(),\n",
    "                \"rcnn_head_state_dict\": rcnn_head.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "            },\n",
    "            \"./checkpoints_fast_rcnn/fast_rcnn_head_refined_2.pth\",\n",
    "        )\n",
    "        print(\"[CKPT] nuevo mejor HEAD-ONLY\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
