{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d96abbc",
   "metadata": {},
   "source": [
    "# Fast RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257e05f",
   "metadata": {},
   "source": [
    "**Fast R-CNN** es una mejora directa de la R-CNN clásica.  \n",
    "\n",
    "En este caso, las regiones de interés (*ROIs*) ya no se recortan ni se procesan individualmente para extraer características.  \n",
    "En su lugar, la imagen completa pasa una sola vez por la red convolucional, generando un **feature map**.  \n",
    "\n",
    "Después, se obtienen las propuestas de **Selective Search** y se proyectan sus coordenadas sobre ese mapa de características para extraer las regiones correspondientes.  \n",
    "\n",
    "Dado que cada ROI tiene un tamaño distinto, se aplica **ROI Pooling** para normalizarlas a una forma fija, de modo que puedan pasar por las capas fully-connected del *head*.  (En implementaciones modernas usamos ROI Align)\n",
    "\n",
    "La red se entrena optimizando simultáneamente:\n",
    "- la **pérdida de clasificación** (qué objeto hay en cada ROI), y  \n",
    "- la **pérdida de regresión** (ajuste fino de los *bounding boxes*).  \n",
    "\n",
    "Durante el entrenamiento **no se aplica NMS**, pero **sí se usa en la inferencia** para eliminar detecciones redundantes.  \n",
    "\n",
    "Esta arquitectura mejora notablemente los tiempos respecto a R-CNN, aunque **Selective Search sigue siendo el cuello de botella principal**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8f41c",
   "metadata": {},
   "source": [
    "Diferencias:\n",
    "\n",
    "| Etapa     | R-CNN clásica                  | Fast-RCNN                   |\n",
    "| --------- | ------------------------------ | ------------------------------------------- |\n",
    "| Proposals | Selective Search               | Selective Search                 |\n",
    "| ROIs      | No hay como tal                | Subconjunto de proposals usado por ROIPooling / Align |\n",
    "| NMS       | Final (sobre predicciones SVM) | Final (sobre predicciones del head)         |\n",
    "\n",
    "\n",
    "Conceptos:\n",
    "\n",
    "\n",
    "| Concepto      | Qué es                                                    | Cómo se obtiene                                        | Cuántos hay              | Cómo se usa                                                |\n",
    "| ------------- | --------------------------------------------------------- | ------------------------------------------------------ | ------------------------ | ---------------------------------------------------------- |\n",
    "| **Proposals** | Candidatas “donde podría haber algo”                      | De un algoritmo como **Selective Search** o un **RPN** | Miles (≈2000 por imagen) | Entrada bruta del detector                                 |\n",
    "| **ROIs**      | Subconjunto de proposals elegidas para entrenar o inferir | Se **muestrean** (pos/neg) de las proposals            | Pocas (≈128 por imagen)  | Alimentan **ROIAlign + head** para clasificación/regresión |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4fd84",
   "metadata": {},
   "source": [
    "Vamos a ver su implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42ceee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Device: cuda\n",
      "VOC_ROOT: ./data/02\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time, pathlib, shutil, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import selectivesearch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet_transforms = weights.transforms()\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "\n",
    "VOC_ROOT = os.environ.get(\"VOC_ROOT\", \"./data/02\")\n",
    "print(\"VOC_ROOT:\", VOC_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c07cc",
   "metadata": {},
   "source": [
    "#### Nos saltamos la parte de descargar el dataset, lo vimos en el cuaderno anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbba32",
   "metadata": {},
   "source": [
    "Pasamos directamente a su carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e6dda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5011\n",
      "Test: 4952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_root = \"./data/02/VOCtrainval_06-Nov-2007\"\n",
    "test_root  = \"./data/02/VOCtest_06-Nov-2007\"\n",
    "\n",
    "train_ds = VOCDetection(train_root, year=\"2007\", image_set=\"trainval\", download=False)\n",
    "test_ds  = VOCDetection(test_root,  year=\"2007\", image_set=\"test\",     download=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a126f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
      "Total: 21\n"
     ]
    }
   ],
   "source": [
    "# === Extracción automática de clases del dataset VOC ===\n",
    "\n",
    "def extract_voc_classes(dataset):\n",
    "    classes = set()\n",
    "    for i in range(min(500, len(dataset))):  # escanea solo 500 imágenes para acelerar\n",
    "        ann = dataset[i][1]['annotation']\n",
    "        objs = ann.get('object', [])\n",
    "        if isinstance(objs, dict):  # si solo hay un objeto\n",
    "            objs = [objs]\n",
    "        for obj in objs:\n",
    "            name = obj['name']\n",
    "            classes.add(name)\n",
    "    return sorted(list(classes))\n",
    "\n",
    "# Obtener clases del dataset de entrenamiento\n",
    "voc_classes = extract_voc_classes(train_ds)\n",
    "\n",
    "#Añadimos background porque es la clase 0, que no está en voc_classes, lo necesitamos para el loss. \n",
    "#Tendremos 21 clases: 20 de VOC + 1 de background.\n",
    "CLASSES = ['background'] + voc_classes\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(\"Clases detectadas:\", voc_classes)\n",
    "print(\"Total:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1a99d",
   "metadata": {},
   "source": [
    "Tenemos unas 2000 proposals por imagen\n",
    "Estas salen de Selective Search.\n",
    "La mayoría no contiene ningún objeto útil; algunas sí cubren bien GT.\n",
    "El objetivo es quedarse con 128 ROIs representativas para esa imagen.\n",
    "\n",
    "Para ello calculamos el IoU entre cada proposal y todas las cajas GT.\n",
    "\n",
    "Luego clasificamos cada proposal en Pos y Neg en base a los thresholds del IoU y poderamos las imagenes que nos queremos quedar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19c0a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num clases: 21\n",
      "Sampler: 128 (FG 32, BG 96)\n",
      "PosIoU≥0.5, NegIoU≤0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# IoU thresholds\n",
    "POS_IOU_THRESH = 0.5\n",
    "NEG_IOU_THRESH = 0.3\n",
    "\n",
    "# Sampler config\n",
    "# Tomamos 128 ROIs por imagen.\n",
    "ROIS_PER_IMG = 128\n",
    "# 25% de esas ROIs son positivas (foreground) y el resto 75% negativas (background).\n",
    "# Usamos esta distribución porque es lo que da mejores resultados en la literatura.\n",
    "FG_FRACTION = 0.25  # ~32 pos + 96 neg \n",
    "\n",
    "# BBox delta normalization\n",
    "# Normalizamos las coordenadas de los bounding boxes para que sean más estables.\n",
    "#Son valores que usamos fijos para todo el entrenamiento y que se han obtenido empíricamente.\n",
    "#Son un estándar en la literatura.\n",
    "BBOX_MEANS = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
    "BBOX_STDS  = torch.tensor([0.1, 0.1, 0.2, 0.2], dtype=torch.float32)\n",
    "\n",
    "# Proposals\n",
    "# Tomamos hasta 2000 propuestas por imagen.\n",
    "MAX_PROPOSALS_PER_IMG = 2000\n",
    "# Las propuestas mínimas tienen 16px de lado.\n",
    "MIN_SIZE = 16\n",
    "\n",
    "print(f\"Num clases: {NUM_CLASSES}\")\n",
    "print(f\"Sampler: {ROIS_PER_IMG} (FG {int(ROIS_PER_IMG*FG_FRACTION)}, BG {ROIS_PER_IMG-int(ROIS_PER_IMG*FG_FRACTION)})\")\n",
    "print(f\"PosIoU≥{POS_IOU_THRESH}, NegIoU≤{NEG_IOU_THRESH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327891d",
   "metadata": {},
   "source": [
    "Ahora necesitamos redimensionar las imagenes, mantendremos un tamaño fijo para el lado más corto (600) y uno variable de maximo 1000 para el lado largo, de desta manera podemos mantener el factor de escala, este facto de escala tendremos que aplicarlo también a los BBs para que la proyección sea coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "476f11b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "#Definimos el tamaño fijo para el lado más corto y el máximo para el lado largo\n",
    "short_side = 600\n",
    "max_side = 1000\n",
    "\n",
    "#Obtenemos la media y la desviación estándar de ResNet para la normalización\n",
    "R50_MEAN = resnet_transforms.mean\n",
    "R50_STD = resnet_transforms.std\n",
    "print(R50_MEAN, R50_STD)\n",
    "\n",
    "#Llamaremos a esta función para preprocesar las imágenes\n",
    "def preprocess_image(img, boxes, device=device, short_side=short_side, max_side=max_side):\n",
    "    \n",
    "    #Objeto PIL de la imagen\n",
    "    w, h = img.size #Ej (500, 333)\n",
    "    #Obtenemos el tamaño más corto y el más largo de la imagen\n",
    "    actual_short = min(w, h) #Ej 333\n",
    "    actual_long  = max(w, h) #Ej 500\n",
    "\n",
    "    scale_multiplier = short_side / actual_short\n",
    "    \n",
    "    target_long = actual_long * scale_multiplier\n",
    "\n",
    "    if target_long > max_side:\n",
    "        scale_multiplier = max_side / actual_long #porque max_side = actual_long * scale_multiplier\n",
    "\n",
    "    #Calculamos el nuevo tamaño de la imagen\n",
    "    w = int(w * scale_multiplier)\n",
    "    h = int(h * scale_multiplier)\n",
    "\n",
    "    #Redimensionamos la imagen\n",
    "    img = img.resize((w, h), Image.BILINEAR)\n",
    "\n",
    "    #Ajustamos los BBs\n",
    "    #Los BBs vienen en formato (xmin, ymin, xmax, ymax)\n",
    "    #Es un tensor de 4 columnas, cada fila es un BB\n",
    "    #Multiplicamos cada columna por el factor de escala\n",
    "    boxes = boxes * scale_multiplier\n",
    "\n",
    "    #Convertimos la imagen a tensor\n",
    "    img = to_tensor(img)\n",
    "\n",
    "    #Normalizamos la imagen \n",
    "    img = normalize(img, R50_MEAN, R50_STD)\n",
    "\n",
    "    #Los boxes NO se normalizan, se mantienen en las coordenadas originales porque son relativas a la imagen.\n",
    "    \n",
    "    return img, boxes, scale_multiplier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a37215",
   "metadata": {},
   "source": [
    "Bien, ahora, necesitamos una funcion que reciba una imagen  y extraiga las bbs de esa imagen en un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59004ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto es puro parseo de XML, no hay nada especial.\n",
    "def extract_gt_boxes(annotation):\n",
    "    objs = annotation[\"annotation\"][\"object\"]\n",
    "    \n",
    "    if isinstance(objs, dict):\n",
    "        objs = [objs]  \n",
    "\n",
    "    boxes = []\n",
    "    for o in objs:\n",
    "        bb = o[\"bndbox\"]\n",
    "        xmin = float(bb[\"xmin\"])\n",
    "        ymin = float(bb[\"ymin\"])\n",
    "        xmax = float(bb[\"xmax\"])\n",
    "        ymax = float(bb[\"ymax\"])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105b50e",
   "metadata": {},
   "source": [
    "Vamos a probar con una unica imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6cde7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño original: (500, 375)\n",
      "Boxes originales: tensor([[263., 211., 324., 339.],\n",
      "        [165., 264., 253., 372.],\n",
      "        [  5., 244.,  67., 374.],\n",
      "        [241., 194., 295., 299.],\n",
      "        [277., 186., 312., 220.]])\n",
      "Escala aplicada: 1.6\n",
      "Tamaño nuevo: torch.Size([3, 600, 800])\n",
      "Boxes reescaladas: tensor([[420.8000, 337.6000, 518.4000, 542.4000],\n",
      "        [264.0000, 422.4000, 404.8000, 595.2000],\n",
      "        [  8.0000, 390.4000, 107.2000, 598.4000],\n",
      "        [385.6000, 310.4000, 472.0000, 478.4000],\n",
      "        [443.2000, 297.6000, 499.2000, 352.0000]])\n"
     ]
    }
   ],
   "source": [
    "img, ann = train_ds[0]\n",
    "\n",
    "#Extraemos las bbs de la imagen\n",
    "boxes = extract_gt_boxes(ann)\n",
    "\n",
    "print(\"Tamaño original:\", img.size)\n",
    "print(\"Boxes originales:\", boxes)\n",
    "\n",
    "#Preprocesamos la imagen y los boxes\n",
    "img_t, boxes_t, scale = preprocess_image(img, boxes)\n",
    "\n",
    "print(\"Escala aplicada:\", scale)\n",
    "print(\"Tamaño nuevo:\", img_t.shape)\n",
    "print(\"Boxes reescaladas:\", boxes_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd17c9d",
   "metadata": {},
   "source": [
    "Ahora tendremos que crear una funcion que calcule los proposals con selective search de una imagen ya preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f13a9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_selective_proposals(img_pil, max_proposals=MAX_PROPOSALS_PER_IMG, min_size=MIN_SIZE):\n",
    "    img_np = np.array(img_pil)\n",
    "\n",
    "    _, regions = selectivesearch.selective_search(\n",
    "        img_np,\n",
    "        scale=500,\n",
    "        sigma=0.9,\n",
    "        min_size=10\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    proposals = []\n",
    "\n",
    "    for r in regions:\n",
    "        x, y, w, h = r[\"rect\"]\n",
    "\n",
    "        if (x, y, w, h) in seen:\n",
    "            continue\n",
    "        seen.add((x, y, w, h))\n",
    "\n",
    "        if w < min_size or h < min_size:\n",
    "            continue\n",
    "\n",
    "        proposals.append([x, y, x + w, y + h])\n",
    "\n",
    "        if len(proposals) >= max_proposals:\n",
    "            break\n",
    "\n",
    "    if len(proposals) == 0:\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    return torch.tensor(proposals, dtype=torch.float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
