{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1545485",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbc871",
   "metadata": {},
   "source": [
    "ResNet y VGG comparten la idea de bloques repetitivos, pero difieren en lo que hacen dentro del bloque y en cómo conectan los bloques entre sí.\n",
    "\n",
    "ResNet se basa en \"bloques residuales\".\n",
    "Cada bloque tiene un atajo (“skip connection”) que suma la entrada original del bloque (x) con la salida procesada (F(x)) para tratar de solucionar el problema de desvanecimiento del gradiente.\n",
    "\n",
    "De esta forma, el gradiente puede fluir hacia atras sin perderse en redes mucho más profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13539e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False, drop_last=True)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fabfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([-4.5197e-06, -2.3309e-06, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "    \n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21f65c",
   "metadata": {},
   "source": [
    "Vamos a implementar una Clase con un bloque residual simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8833b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "# ===========================================================\n",
    "# NoSkip: Conv3x3(stride s) → BN → ReLU → Conv3x3(stride 1) → BN\n",
    "# Skip: Identity o Conv1x1(stride s)\n",
    "# Resultado final: NoSkip + ReLU o Skip + ReLU\n",
    "# ===========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleResidualBlock(nn.Module):\n",
    "    \"\"\"Conv-BN-ReLU -> Conv-BN -> +skip -> ReLU\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ===========================================================\n",
    "        # Camino principal (F(x))\n",
    "        # ===========================================================\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "\n",
    "        # ===========================================================\n",
    "        # Skip)\n",
    "        # ===========================================================\n",
    "        # Si el stride es distinto de 1 o cambian los canales,\n",
    "        # creamos una conv1x1 para igualar resolución y profundidad.\n",
    "        # Si no, simplemente usamos Identity (no toca nada).\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "        # ReLU final (post-suma)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    # ===========================================================\n",
    "    # Forward: el flujo real de datos\n",
    "    # ===========================================================\n",
    "    def forward(self, x):\n",
    "        # Calculamos el camino principal y el atajo en paralelo\n",
    "        out = self.main(x)\n",
    "        skip = self.skip(x)\n",
    "        # Sumamos ambos caminos\n",
    "        out = out + skip\n",
    "        # Aplicamos ReLU final\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Helper para crear una capa con varios bloques seguidos\n",
    "# ===========================================================\n",
    "def _make_layer(block, in_ch, out_ch, num_blocks, first_stride):\n",
    "    layers = []\n",
    "    # Primer bloque: puede reducir resolución y/o cambiar canales\n",
    "    layers.append(block(in_ch, out_ch, stride=first_stride))\n",
    "    # Resto de bloques: stride=1, canales constantes\n",
    "    for _ in range(1, num_blocks):\n",
    "        layers.append(block(out_ch, out_ch, stride=1))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d942d28",
   "metadata": {},
   "source": [
    "Ahora implementaremos el resto de la red e incluiremos los bloques residuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d9ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet20(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-20 para CIFAR-10\n",
    "    Estructura: 1 conv inicial + 3 grupos de 3 bloques residuales + avg pool + FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Capa convolucional inicial (no reduce resolución en CIFAR10)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 3 Grupos Residuales\n",
    "        # Grupo 1: 16 canales, 32x32 -> 32x32 (stride=1)\n",
    "        self.layer1 = _make_layer(SimpleResidualBlock, 16, 16, num_blocks=3, first_stride=1)\n",
    "        \n",
    "        # Grupo 2: 32 canales, 32x32 -> 16x16 (stride=2 en el primer bloque)\n",
    "        self.layer2 = _make_layer(SimpleResidualBlock, 16, 32, num_blocks=3, first_stride=2)\n",
    "        \n",
    "        # Grupo 3: 64 canales, 16x16 -> 8x8 (stride=2 en el primer bloque)\n",
    "        self.layer3 = _make_layer(SimpleResidualBlock, 32, 64, num_blocks=3, first_stride=2)\n",
    "        \n",
    "        # Clasificador\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv inicial\n",
    "        x = self.conv1(x)           # [B, 16, 32, 32]\n",
    "        \n",
    "        # Bloques residuales\n",
    "        x = self.layer1(x)          # [B, 16, 32, 32]\n",
    "        x = self.layer2(x)          # [B, 32, 16, 16]\n",
    "        x = self.layer3(x)          # [B, 64, 8, 8]\n",
    "        \n",
    "        # Clasificación\n",
    "        x = self.avgpool(x)         # [B, 64, 1, 1]\n",
    "        x = x.view(x.size(0), -1)   # [B, 64]\n",
    "        x = self.fc(x)              # [B, 10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5170afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/dlvs/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([4, 3, 32, 32]) | Output: torch.Size([4, 10])\n",
      "Params totales: 272474\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "          Identity-9           [-1, 16, 32, 32]               0\n",
      "             ReLU-10           [-1, 16, 32, 32]               0\n",
      "SimpleResidualBlock-11           [-1, 16, 32, 32]               0\n",
      "           Conv2d-12           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-13           [-1, 16, 32, 32]              32\n",
      "             ReLU-14           [-1, 16, 32, 32]               0\n",
      "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
      "         Identity-17           [-1, 16, 32, 32]               0\n",
      "             ReLU-18           [-1, 16, 32, 32]               0\n",
      "SimpleResidualBlock-19           [-1, 16, 32, 32]               0\n",
      "           Conv2d-20           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-21           [-1, 16, 32, 32]              32\n",
      "             ReLU-22           [-1, 16, 32, 32]               0\n",
      "           Conv2d-23           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-24           [-1, 16, 32, 32]              32\n",
      "         Identity-25           [-1, 16, 32, 32]               0\n",
      "             ReLU-26           [-1, 16, 32, 32]               0\n",
      "SimpleResidualBlock-27           [-1, 16, 32, 32]               0\n",
      "           Conv2d-28           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-29           [-1, 32, 16, 16]              64\n",
      "             ReLU-30           [-1, 32, 16, 16]               0\n",
      "           Conv2d-31           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
      "           Conv2d-33           [-1, 32, 16, 16]             512\n",
      "      BatchNorm2d-34           [-1, 32, 16, 16]              64\n",
      "             ReLU-35           [-1, 32, 16, 16]               0\n",
      "SimpleResidualBlock-36           [-1, 32, 16, 16]               0\n",
      "           Conv2d-37           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
      "             ReLU-39           [-1, 32, 16, 16]               0\n",
      "           Conv2d-40           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-41           [-1, 32, 16, 16]              64\n",
      "         Identity-42           [-1, 32, 16, 16]               0\n",
      "             ReLU-43           [-1, 32, 16, 16]               0\n",
      "SimpleResidualBlock-44           [-1, 32, 16, 16]               0\n",
      "           Conv2d-45           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-46           [-1, 32, 16, 16]              64\n",
      "             ReLU-47           [-1, 32, 16, 16]               0\n",
      "           Conv2d-48           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
      "         Identity-50           [-1, 32, 16, 16]               0\n",
      "             ReLU-51           [-1, 32, 16, 16]               0\n",
      "SimpleResidualBlock-52           [-1, 32, 16, 16]               0\n",
      "           Conv2d-53             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-54             [-1, 64, 8, 8]             128\n",
      "             ReLU-55             [-1, 64, 8, 8]               0\n",
      "           Conv2d-56             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-57             [-1, 64, 8, 8]             128\n",
      "           Conv2d-58             [-1, 64, 8, 8]           2,048\n",
      "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
      "             ReLU-60             [-1, 64, 8, 8]               0\n",
      "SimpleResidualBlock-61             [-1, 64, 8, 8]               0\n",
      "           Conv2d-62             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-63             [-1, 64, 8, 8]             128\n",
      "             ReLU-64             [-1, 64, 8, 8]               0\n",
      "           Conv2d-65             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-66             [-1, 64, 8, 8]             128\n",
      "         Identity-67             [-1, 64, 8, 8]               0\n",
      "             ReLU-68             [-1, 64, 8, 8]               0\n",
      "SimpleResidualBlock-69             [-1, 64, 8, 8]               0\n",
      "           Conv2d-70             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-71             [-1, 64, 8, 8]             128\n",
      "             ReLU-72             [-1, 64, 8, 8]               0\n",
      "           Conv2d-73             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
      "         Identity-75             [-1, 64, 8, 8]               0\n",
      "             ReLU-76             [-1, 64, 8, 8]               0\n",
      "SimpleResidualBlock-77             [-1, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-78             [-1, 64, 1, 1]               0\n",
      "           Linear-79                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 272,474\n",
      "Trainable params: 272,474\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.72\n",
      "Params size (MB): 1.04\n",
      "Estimated Total Size (MB): 6.77\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "from torchsummary import summary\n",
    "# Instancia + sanity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet20().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb = torch.randn(4, 3, 32, 32, device=device)\n",
    "    yb = model(xb)\n",
    "print(\"Input:\", xb.shape, \"| Output:\", yb.shape)  \n",
    "print(\"Params totales:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(summary(model, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eaac445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet20(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): SimpleResidualBlock(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "\n",
    "EPOCHS = 80  \n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "weight_decay=1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, threshold=1e-3)\n",
    "lrs = []\n",
    "\n",
    "model.train()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56240ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.8012 | Train Acc: 0.3816 | Val Loss: 1.6156 | Val Acc: 0.4941\n",
      "Época 2/80 | Train Loss: 1.4291 | Train Acc: 0.5758 | Val Loss: 1.6421 | Val Acc: 0.5116\n",
      "Época 3/80 | Train Loss: 1.2547 | Train Acc: 0.6620 | Val Loss: 1.4193 | Val Acc: 0.6135\n",
      "Época 4/80 | Train Loss: 1.1454 | Train Acc: 0.7169 | Val Loss: 1.1744 | Val Acc: 0.7097\n",
      "Época 5/80 | Train Loss: 1.0748 | Train Acc: 0.7526 | Val Loss: 1.1247 | Val Acc: 0.7310\n",
      "Época 6/80 | Train Loss: 1.0211 | Train Acc: 0.7744 | Val Loss: 1.1674 | Val Acc: 0.7307\n",
      "Época 7/80 | Train Loss: 0.9845 | Train Acc: 0.7937 | Val Loss: 1.3552 | Val Acc: 0.6611\n",
      "Época 8/80 | Train Loss: 0.9583 | Train Acc: 0.8025 | Val Loss: 1.0896 | Val Acc: 0.7602\n",
      "Época 9/80 | Train Loss: 0.9316 | Train Acc: 0.8145 | Val Loss: 0.9916 | Val Acc: 0.7981\n",
      "Época 10/80 | Train Loss: 0.9164 | Train Acc: 0.8226 | Val Loss: 1.0613 | Val Acc: 0.7552\n",
      "Época 11/80 | Train Loss: 0.8981 | Train Acc: 0.8304 | Val Loss: 0.9809 | Val Acc: 0.8015\n",
      "Época 12/80 | Train Loss: 0.8829 | Train Acc: 0.8377 | Val Loss: 0.9950 | Val Acc: 0.7867\n",
      "Época 13/80 | Train Loss: 0.8710 | Train Acc: 0.8429 | Val Loss: 0.9667 | Val Acc: 0.8063\n",
      "Época 14/80 | Train Loss: 0.8541 | Train Acc: 0.8492 | Val Loss: 0.9742 | Val Acc: 0.8013\n",
      "Época 15/80 | Train Loss: 0.8479 | Train Acc: 0.8535 | Val Loss: 0.9382 | Val Acc: 0.8175\n",
      "Época 16/80 | Train Loss: 0.8392 | Train Acc: 0.8557 | Val Loss: 0.9447 | Val Acc: 0.8160\n",
      "Época 17/80 | Train Loss: 0.8295 | Train Acc: 0.8617 | Val Loss: 0.9245 | Val Acc: 0.8197\n",
      "Época 18/80 | Train Loss: 0.8254 | Train Acc: 0.8627 | Val Loss: 0.8983 | Val Acc: 0.8366\n",
      "Época 19/80 | Train Loss: 0.8132 | Train Acc: 0.8688 | Val Loss: 0.9267 | Val Acc: 0.8221\n",
      "Época 20/80 | Train Loss: 0.8111 | Train Acc: 0.8694 | Val Loss: 0.9446 | Val Acc: 0.8206\n",
      "Época 21/80 | Train Loss: 0.8037 | Train Acc: 0.8730 | Val Loss: 0.9078 | Val Acc: 0.8300\n",
      "Época 22/80 | Train Loss: 0.7950 | Train Acc: 0.8761 | Val Loss: 0.8933 | Val Acc: 0.8281\n",
      "Época 23/80 | Train Loss: 0.7890 | Train Acc: 0.8774 | Val Loss: 0.8778 | Val Acc: 0.8477\n",
      "Época 24/80 | Train Loss: 0.7854 | Train Acc: 0.8806 | Val Loss: 0.9984 | Val Acc: 0.7976\n",
      "Época 25/80 | Train Loss: 0.7795 | Train Acc: 0.8830 | Val Loss: 0.8874 | Val Acc: 0.8363\n",
      "Época 26/80 | Train Loss: 0.7753 | Train Acc: 0.8838 | Val Loss: 0.9293 | Val Acc: 0.8234\n",
      "Época 27/80 | Train Loss: 0.7718 | Train Acc: 0.8875 | Val Loss: 0.9507 | Val Acc: 0.8157\n",
      "Época 28/80 | Train Loss: 0.7697 | Train Acc: 0.8877 | Val Loss: 0.9681 | Val Acc: 0.8128\n",
      "Época 29/80 | Train Loss: 0.7646 | Train Acc: 0.8876 | Val Loss: 0.8754 | Val Acc: 0.8407\n",
      "Época 30/80 | Train Loss: 0.7606 | Train Acc: 0.8912 | Val Loss: 0.8615 | Val Acc: 0.8546\n",
      "Época 31/80 | Train Loss: 0.7609 | Train Acc: 0.8936 | Val Loss: 0.9882 | Val Acc: 0.7978\n",
      "Época 32/80 | Train Loss: 0.7549 | Train Acc: 0.8947 | Val Loss: 0.8993 | Val Acc: 0.8372\n",
      "Época 33/80 | Train Loss: 0.7505 | Train Acc: 0.8947 | Val Loss: 0.8793 | Val Acc: 0.8416\n",
      "Época 34/80 | Train Loss: 0.7474 | Train Acc: 0.8975 | Val Loss: 0.9800 | Val Acc: 0.8100\n",
      "Época 35/80 | Train Loss: 0.7421 | Train Acc: 0.8986 | Val Loss: 0.9109 | Val Acc: 0.8321\n",
      "Época 36/80 | Train Loss: 0.7441 | Train Acc: 0.8990 | Val Loss: 0.9841 | Val Acc: 0.8137\n",
      "Época 37/80 | Train Loss: 0.7388 | Train Acc: 0.9015 | Val Loss: 0.8864 | Val Acc: 0.8420\n",
      "Época 38/80 | Train Loss: 0.7329 | Train Acc: 0.9026 | Val Loss: 0.9081 | Val Acc: 0.8306\n",
      "Época 39/80 | Train Loss: 0.7353 | Train Acc: 0.9019 | Val Loss: 0.8816 | Val Acc: 0.8439\n",
      "Época 40/80 | Train Loss: 0.6826 | Train Acc: 0.9276 | Val Loss: 0.7690 | Val Acc: 0.8885\n",
      "Época 41/80 | Train Loss: 0.6675 | Train Acc: 0.9340 | Val Loss: 0.8330 | Val Acc: 0.8661\n",
      "Época 42/80 | Train Loss: 0.6657 | Train Acc: 0.9351 | Val Loss: 0.8262 | Val Acc: 0.8695\n",
      "Época 43/80 | Train Loss: 0.6637 | Train Acc: 0.9364 | Val Loss: 0.8058 | Val Acc: 0.8753\n",
      "Época 44/80 | Train Loss: 0.6632 | Train Acc: 0.9360 | Val Loss: 0.7968 | Val Acc: 0.8806\n",
      "Época 45/80 | Train Loss: 0.6597 | Train Acc: 0.9376 | Val Loss: 0.7898 | Val Acc: 0.8834\n",
      "Época 46/80 | Train Loss: 0.6547 | Train Acc: 0.9387 | Val Loss: 0.8003 | Val Acc: 0.8768\n",
      "Época 47/80 | Train Loss: 0.6588 | Train Acc: 0.9372 | Val Loss: 0.7854 | Val Acc: 0.8829\n",
      "Época 48/80 | Train Loss: 0.6562 | Train Acc: 0.9378 | Val Loss: 0.7961 | Val Acc: 0.8805\n",
      "Época 49/80 | Train Loss: 0.6553 | Train Acc: 0.9385 | Val Loss: 0.8118 | Val Acc: 0.8745\n",
      "Época 50/80 | Train Loss: 0.6224 | Train Acc: 0.9531 | Val Loss: 0.7541 | Val Acc: 0.8972\n",
      "Época 51/80 | Train Loss: 0.6128 | Train Acc: 0.9591 | Val Loss: 0.7535 | Val Acc: 0.8969\n",
      "Época 52/80 | Train Loss: 0.6107 | Train Acc: 0.9588 | Val Loss: 0.7575 | Val Acc: 0.8950\n",
      "Época 53/80 | Train Loss: 0.6049 | Train Acc: 0.9625 | Val Loss: 0.7629 | Val Acc: 0.8969\n",
      "Época 54/80 | Train Loss: 0.6065 | Train Acc: 0.9615 | Val Loss: 0.7678 | Val Acc: 0.8913\n",
      "Época 55/80 | Train Loss: 0.6037 | Train Acc: 0.9623 | Val Loss: 0.7852 | Val Acc: 0.8863\n",
      "Época 56/80 | Train Loss: 0.6031 | Train Acc: 0.9634 | Val Loss: 0.7661 | Val Acc: 0.8933\n",
      "Época 57/80 | Train Loss: 0.6019 | Train Acc: 0.9629 | Val Loss: 0.7678 | Val Acc: 0.8930\n",
      "Época 58/80 | Train Loss: 0.6014 | Train Acc: 0.9632 | Val Loss: 0.7598 | Val Acc: 0.8934\n",
      "Época 59/80 | Train Loss: 0.6006 | Train Acc: 0.9634 | Val Loss: 0.7699 | Val Acc: 0.8954\n",
      "Época 60/80 | Train Loss: 0.5819 | Train Acc: 0.9728 | Val Loss: 0.7437 | Val Acc: 0.9031\n",
      "Época 61/80 | Train Loss: 0.5790 | Train Acc: 0.9743 | Val Loss: 0.7443 | Val Acc: 0.9026\n",
      "Época 62/80 | Train Loss: 0.5778 | Train Acc: 0.9740 | Val Loss: 0.7421 | Val Acc: 0.9044\n",
      "Época 63/80 | Train Loss: 0.5739 | Train Acc: 0.9764 | Val Loss: 0.7445 | Val Acc: 0.9031\n",
      "Época 64/80 | Train Loss: 0.5734 | Train Acc: 0.9768 | Val Loss: 0.7449 | Val Acc: 0.9051\n",
      "Época 65/80 | Train Loss: 0.5728 | Train Acc: 0.9768 | Val Loss: 0.7567 | Val Acc: 0.8991\n",
      "Época 66/80 | Train Loss: 0.5710 | Train Acc: 0.9782 | Val Loss: 0.7499 | Val Acc: 0.9019\n",
      "Época 67/80 | Train Loss: 0.5712 | Train Acc: 0.9780 | Val Loss: 0.7488 | Val Acc: 0.9005\n",
      "Época 68/80 | Train Loss: 0.5723 | Train Acc: 0.9774 | Val Loss: 0.7505 | Val Acc: 0.9019\n",
      "Época 69/80 | Train Loss: 0.5690 | Train Acc: 0.9788 | Val Loss: 0.7488 | Val Acc: 0.9014\n",
      "Época 70/80 | Train Loss: 0.5658 | Train Acc: 0.9802 | Val Loss: 0.7528 | Val Acc: 0.9010\n",
      "Época 71/80 | Train Loss: 0.5663 | Train Acc: 0.9793 | Val Loss: 0.7509 | Val Acc: 0.9013\n",
      "Época 72/80 | Train Loss: 0.5589 | Train Acc: 0.9830 | Val Loss: 0.7421 | Val Acc: 0.9066\n",
      "Época 73/80 | Train Loss: 0.5573 | Train Acc: 0.9831 | Val Loss: 0.7473 | Val Acc: 0.9045\n",
      "Época 74/80 | Train Loss: 0.5564 | Train Acc: 0.9842 | Val Loss: 0.7465 | Val Acc: 0.9025\n",
      "Época 75/80 | Train Loss: 0.5556 | Train Acc: 0.9845 | Val Loss: 0.7467 | Val Acc: 0.9038\n",
      "Época 76/80 | Train Loss: 0.5547 | Train Acc: 0.9852 | Val Loss: 0.7418 | Val Acc: 0.9058\n",
      "Época 77/80 | Train Loss: 0.5539 | Train Acc: 0.9851 | Val Loss: 0.7469 | Val Acc: 0.9059\n",
      "Época 78/80 | Train Loss: 0.5528 | Train Acc: 0.9860 | Val Loss: 0.7484 | Val Acc: 0.9023\n",
      "Época 79/80 | Train Loss: 0.5541 | Train Acc: 0.9857 | Val Loss: 0.7436 | Val Acc: 0.9064\n",
      "Época 80/80 | Train Loss: 0.5517 | Train Acc: 0.9867 | Val Loss: 0.7419 | Val Acc: 0.9060\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abdb9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado correctamente a cifar10cnn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4042/1074930037.py:8: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "# Exportar modelo a ONNX (compatibilidad PyTorch <=2.2)\n",
    "import torch.onnx\n",
    "\n",
    "model.eval()\n",
    "model_cpu = model.to('cpu').to(memory_format=torch.contiguous_format)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    dummy_input,\n",
    "    \"cifar10cnn_resnet.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    dynamo=False,  # fuerza el exportador estable\n",
    "    training=torch.onnx.TrainingMode.EVAL\n",
    ")\n",
    "\n",
    "print(\"Modelo exportado correctamente a cifar10cnn.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
