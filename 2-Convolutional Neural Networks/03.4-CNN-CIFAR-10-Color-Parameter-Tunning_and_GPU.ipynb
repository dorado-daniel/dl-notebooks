{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad4aa8",
   "metadata": {},
   "source": [
    "## Parameter Tunning\n",
    "\n",
    "Vamos a mejorar el entrenamiento del anterior todavía más, hasta conseguir una convergencia que nos sirva.\n",
    "\n",
    "Aqui experimentaremos:\n",
    "\n",
    "- Subiendo EPOCHS\n",
    "\n",
    "- Aumentando el tamaño de LR\n",
    "\n",
    "- Tocando step_size y gamma de StepLR\n",
    "\n",
    "- Manteniendo Data Agumentation\n",
    "\n",
    "\n",
    "Además introducimos aqui Tensorboard para visualización de lr, scalars y prepararemos correctamente para hacer test final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e462f3c",
   "metadata": {},
   "source": [
    "Comenzamos igual que en la sección anterior, lo que cambianmos aqui serán parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceb06d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d9a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69f483",
   "metadata": {},
   "source": [
    "Vamos ahora a coger el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4857847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loader_train))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66939dd6",
   "metadata": {},
   "source": [
    "# Declaración de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ba618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #Primera capa de convolucion, entra una imagen de 3 canales y sale una imagen de 32 features\n",
    "            #El kernel es de 3x3 y el padding es 1 (same padding)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Segunda capa de convolucion, entra una imagen de 16 features y sale una imagen de 32 features\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Tercera capa de convolucion, entra una imagen de 32 features y sale una imagen de 64 features\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "             nn.Dropout(p=0.3), #Aplicamos dropout para evitar el overfitting\n",
    "        )\n",
    "\n",
    "        n_flat = get_flatten_size(self.features)\n",
    "\n",
    "        \n",
    "\n",
    "        #Añadimos una capa lineal para clasificar\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), #Aplanamos la imagen para que sea un vector\n",
    "          \n",
    "            \n",
    "            nn.Linear(in_features=n_flat, out_features=500), #Una capa lineal con n_flat entradas y 500 salidas\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.Dropout(p=0.4), #Aplicamos dropout para evitar el overfitting\n",
    "            \n",
    "            nn.Linear(in_features=500, out_features=10), #Una capa lineal con 500 entradas y 10 salidas\n",
    "            # No aplicamos softmax, ya que la funcion de perdida que usamos (CrossEntropyLoss) lo aplica por defecto\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instancia\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = CIFAR10CNN()\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bd307e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "              ReLU-2           [-1, 16, 32, 32]               0\n",
      "         MaxPool2d-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
      "              ReLU-5           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-6             [-1, 32, 8, 8]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          18,496\n",
      "              ReLU-8             [-1, 64, 8, 8]               0\n",
      "         MaxPool2d-9             [-1, 64, 4, 4]               0\n",
      "          Dropout-10             [-1, 64, 4, 4]               0\n",
      "          Flatten-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 500]         512,500\n",
      "             ReLU-13                  [-1, 500]               0\n",
      "          Dropout-14                  [-1, 500]               0\n",
      "           Linear-15                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 541,094\n",
      "Trainable params: 541,094\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.52\n",
      "Params size (MB): 2.06\n",
      "Estimated Total Size (MB): 2.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45588c0e",
   "metadata": {},
   "source": [
    "### Vamos a declarar el bucle de entramiento\n",
    "\n",
    "Vamos a recordar los parametros tipicos de SDG\n",
    "\n",
    "| Parámetro  | Rol                                                      | Valor típico |\n",
    "| ---------- | -------------------------------------------------------- | ------------ |\n",
    "| `lr`       | tamaño del paso (cuánto cambian los pesos por gradiente) | 0.01–0.1     |\n",
    "| `momentum` | cuánto “recuerda” del gradiente anterior                 | 0.8–0.95     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a0e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.05\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86551f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.8531 | Train Acc: 0.3125 | Val Loss: 1.5294 | Val Acc: 0.4452\n",
      "Época 2/80 | Train Loss: 1.5156 | Train Acc: 0.4422 | Val Loss: 1.2902 | Val Acc: 0.5369\n",
      "Época 3/80 | Train Loss: 1.3809 | Train Acc: 0.4970 | Val Loss: 1.1832 | Val Acc: 0.5670\n",
      "Época 4/80 | Train Loss: 1.2903 | Train Acc: 0.5374 | Val Loss: 1.1002 | Val Acc: 0.6113\n",
      "Época 5/80 | Train Loss: 1.2507 | Train Acc: 0.5541 | Val Loss: 1.0489 | Val Acc: 0.6266\n",
      "Época 6/80 | Train Loss: 1.2046 | Train Acc: 0.5700 | Val Loss: 1.0256 | Val Acc: 0.6382\n",
      "Época 7/80 | Train Loss: 1.1638 | Train Acc: 0.5841 | Val Loss: 1.0267 | Val Acc: 0.6390\n",
      "Época 8/80 | Train Loss: 1.1683 | Train Acc: 0.5889 | Val Loss: 1.0135 | Val Acc: 0.6459\n",
      "Época 9/80 | Train Loss: 1.1390 | Train Acc: 0.6026 | Val Loss: 0.9861 | Val Acc: 0.6585\n",
      "Época 10/80 | Train Loss: 1.1242 | Train Acc: 0.6054 | Val Loss: 0.9397 | Val Acc: 0.6726\n",
      "Época 11/80 | Train Loss: 1.1029 | Train Acc: 0.6131 | Val Loss: 0.9313 | Val Acc: 0.6792\n",
      "Época 12/80 | Train Loss: 1.1181 | Train Acc: 0.6114 | Val Loss: 0.9547 | Val Acc: 0.6667\n",
      "Época 13/80 | Train Loss: 1.0957 | Train Acc: 0.6187 | Val Loss: 0.9301 | Val Acc: 0.6772\n",
      "Época 14/80 | Train Loss: 1.0880 | Train Acc: 0.6242 | Val Loss: 0.9409 | Val Acc: 0.6724\n",
      "Época 15/80 | Train Loss: 1.0874 | Train Acc: 0.6207 | Val Loss: 0.9278 | Val Acc: 0.6817\n",
      "Época 16/80 | Train Loss: 1.0698 | Train Acc: 0.6290 | Val Loss: 0.9127 | Val Acc: 0.6854\n",
      "Época 17/80 | Train Loss: 1.0605 | Train Acc: 0.6323 | Val Loss: 0.9301 | Val Acc: 0.6831\n",
      "Época 18/80 | Train Loss: 1.0601 | Train Acc: 0.6275 | Val Loss: 0.8580 | Val Acc: 0.7033\n",
      "Época 19/80 | Train Loss: 1.0512 | Train Acc: 0.6351 | Val Loss: 0.8941 | Val Acc: 0.6964\n",
      "Época 20/80 | Train Loss: 1.0497 | Train Acc: 0.6366 | Val Loss: 0.8888 | Val Acc: 0.7003\n",
      "Época 21/80 | Train Loss: 0.9097 | Train Acc: 0.6815 | Val Loss: 0.7515 | Val Acc: 0.7477\n",
      "Época 22/80 | Train Loss: 0.8753 | Train Acc: 0.6944 | Val Loss: 0.7385 | Val Acc: 0.7534\n",
      "Época 23/80 | Train Loss: 0.8630 | Train Acc: 0.7017 | Val Loss: 0.7484 | Val Acc: 0.7481\n",
      "Época 24/80 | Train Loss: 0.8460 | Train Acc: 0.7058 | Val Loss: 0.7235 | Val Acc: 0.7517\n",
      "Época 25/80 | Train Loss: 0.8439 | Train Acc: 0.7088 | Val Loss: 0.7117 | Val Acc: 0.7576\n",
      "Época 26/80 | Train Loss: 0.8466 | Train Acc: 0.7077 | Val Loss: 0.7199 | Val Acc: 0.7586\n",
      "Época 27/80 | Train Loss: 0.8264 | Train Acc: 0.7149 | Val Loss: 0.7229 | Val Acc: 0.7550\n",
      "Época 28/80 | Train Loss: 0.8187 | Train Acc: 0.7155 | Val Loss: 0.6821 | Val Acc: 0.7725\n",
      "Época 29/80 | Train Loss: 0.8157 | Train Acc: 0.7164 | Val Loss: 0.6748 | Val Acc: 0.7715\n",
      "Época 30/80 | Train Loss: 0.8131 | Train Acc: 0.7163 | Val Loss: 0.6778 | Val Acc: 0.7676\n",
      "Época 31/80 | Train Loss: 0.8060 | Train Acc: 0.7199 | Val Loss: 0.7078 | Val Acc: 0.7604\n",
      "Época 32/80 | Train Loss: 0.8161 | Train Acc: 0.7161 | Val Loss: 0.6798 | Val Acc: 0.7730\n",
      "Época 33/80 | Train Loss: 0.8073 | Train Acc: 0.7179 | Val Loss: 0.6645 | Val Acc: 0.7729\n",
      "Época 34/80 | Train Loss: 0.8060 | Train Acc: 0.7178 | Val Loss: 0.7038 | Val Acc: 0.7604\n",
      "Época 35/80 | Train Loss: 0.7951 | Train Acc: 0.7249 | Val Loss: 0.7051 | Val Acc: 0.7588\n",
      "Época 36/80 | Train Loss: 0.8087 | Train Acc: 0.7217 | Val Loss: 0.7095 | Val Acc: 0.7609\n",
      "Época 37/80 | Train Loss: 0.7902 | Train Acc: 0.7228 | Val Loss: 0.6441 | Val Acc: 0.7766\n",
      "Época 38/80 | Train Loss: 0.7796 | Train Acc: 0.7279 | Val Loss: 0.6983 | Val Acc: 0.7690\n",
      "Época 39/80 | Train Loss: 0.8004 | Train Acc: 0.7241 | Val Loss: 0.6901 | Val Acc: 0.7584\n",
      "Época 40/80 | Train Loss: 0.7889 | Train Acc: 0.7244 | Val Loss: 0.6732 | Val Acc: 0.7712\n",
      "Época 41/80 | Train Loss: 0.7148 | Train Acc: 0.7519 | Val Loss: 0.6279 | Val Acc: 0.7868\n",
      "Época 42/80 | Train Loss: 0.7017 | Train Acc: 0.7574 | Val Loss: 0.6334 | Val Acc: 0.7817\n",
      "Época 43/80 | Train Loss: 0.6908 | Train Acc: 0.7609 | Val Loss: 0.5906 | Val Acc: 0.8003\n",
      "Época 44/80 | Train Loss: 0.6890 | Train Acc: 0.7599 | Val Loss: 0.5824 | Val Acc: 0.8008\n",
      "Época 45/80 | Train Loss: 0.6831 | Train Acc: 0.7618 | Val Loss: 0.6300 | Val Acc: 0.7866\n",
      "Época 46/80 | Train Loss: 0.6760 | Train Acc: 0.7638 | Val Loss: 0.5783 | Val Acc: 0.8053\n",
      "Época 47/80 | Train Loss: 0.6766 | Train Acc: 0.7639 | Val Loss: 0.5897 | Val Acc: 0.8013\n",
      "Época 48/80 | Train Loss: 0.6779 | Train Acc: 0.7643 | Val Loss: 0.5798 | Val Acc: 0.7992\n",
      "Época 49/80 | Train Loss: 0.6656 | Train Acc: 0.7676 | Val Loss: 0.5878 | Val Acc: 0.8004\n",
      "Época 50/80 | Train Loss: 0.6726 | Train Acc: 0.7660 | Val Loss: 0.5762 | Val Acc: 0.8015\n",
      "Época 51/80 | Train Loss: 0.6709 | Train Acc: 0.7657 | Val Loss: 0.5697 | Val Acc: 0.8035\n",
      "Época 52/80 | Train Loss: 0.6648 | Train Acc: 0.7677 | Val Loss: 0.5779 | Val Acc: 0.8036\n",
      "Época 53/80 | Train Loss: 0.6665 | Train Acc: 0.7647 | Val Loss: 0.5572 | Val Acc: 0.8097\n",
      "Época 54/80 | Train Loss: 0.6688 | Train Acc: 0.7667 | Val Loss: 0.5860 | Val Acc: 0.8031\n",
      "Época 55/80 | Train Loss: 0.6658 | Train Acc: 0.7661 | Val Loss: 0.5904 | Val Acc: 0.7992\n",
      "Época 56/80 | Train Loss: 0.6661 | Train Acc: 0.7676 | Val Loss: 0.5477 | Val Acc: 0.8135\n",
      "Época 57/80 | Train Loss: 0.6567 | Train Acc: 0.7711 | Val Loss: 0.5771 | Val Acc: 0.8059\n",
      "Época 58/80 | Train Loss: 0.6624 | Train Acc: 0.7691 | Val Loss: 0.5483 | Val Acc: 0.8159\n",
      "Época 59/80 | Train Loss: 0.6560 | Train Acc: 0.7721 | Val Loss: 0.5500 | Val Acc: 0.8121\n",
      "Época 60/80 | Train Loss: 0.6522 | Train Acc: 0.7700 | Val Loss: 0.5747 | Val Acc: 0.8048\n",
      "Época 61/80 | Train Loss: 0.6143 | Train Acc: 0.7872 | Val Loss: 0.5265 | Val Acc: 0.8186\n",
      "Época 62/80 | Train Loss: 0.6065 | Train Acc: 0.7899 | Val Loss: 0.5192 | Val Acc: 0.8243\n",
      "Época 63/80 | Train Loss: 0.5989 | Train Acc: 0.7911 | Val Loss: 0.5206 | Val Acc: 0.8242\n",
      "Época 64/80 | Train Loss: 0.5985 | Train Acc: 0.7911 | Val Loss: 0.5249 | Val Acc: 0.8181\n",
      "Época 65/80 | Train Loss: 0.5974 | Train Acc: 0.7916 | Val Loss: 0.5217 | Val Acc: 0.8234\n",
      "Época 66/80 | Train Loss: 0.5907 | Train Acc: 0.7941 | Val Loss: 0.5202 | Val Acc: 0.8232\n",
      "Época 67/80 | Train Loss: 0.5894 | Train Acc: 0.7955 | Val Loss: 0.5176 | Val Acc: 0.8195\n",
      "Época 68/80 | Train Loss: 0.5876 | Train Acc: 0.7939 | Val Loss: 0.5225 | Val Acc: 0.8220\n",
      "Época 69/80 | Train Loss: 0.5978 | Train Acc: 0.7930 | Val Loss: 0.5141 | Val Acc: 0.8263\n",
      "Época 70/80 | Train Loss: 0.5887 | Train Acc: 0.7953 | Val Loss: 0.5139 | Val Acc: 0.8268\n",
      "Época 71/80 | Train Loss: 0.5887 | Train Acc: 0.7939 | Val Loss: 0.5058 | Val Acc: 0.8278\n",
      "Época 72/80 | Train Loss: 0.5851 | Train Acc: 0.7963 | Val Loss: 0.5238 | Val Acc: 0.8230\n",
      "Época 73/80 | Train Loss: 0.5825 | Train Acc: 0.7965 | Val Loss: 0.5104 | Val Acc: 0.8279\n",
      "Época 74/80 | Train Loss: 0.5803 | Train Acc: 0.7970 | Val Loss: 0.5024 | Val Acc: 0.8315\n",
      "Época 75/80 | Train Loss: 0.5823 | Train Acc: 0.7966 | Val Loss: 0.5167 | Val Acc: 0.8277\n",
      "Época 76/80 | Train Loss: 0.5840 | Train Acc: 0.7953 | Val Loss: 0.5172 | Val Acc: 0.8224\n",
      "Época 77/80 | Train Loss: 0.5814 | Train Acc: 0.7969 | Val Loss: 0.5066 | Val Acc: 0.8274\n",
      "Época 78/80 | Train Loss: 0.5799 | Train Acc: 0.7976 | Val Loss: 0.5115 | Val Acc: 0.8252\n",
      "Época 79/80 | Train Loss: 0.5776 | Train Acc: 0.7949 | Val Loss: 0.5194 | Val Acc: 0.8229\n",
      "Época 80/80 | Train Loss: 0.5840 | Train Acc: 0.7976 | Val Loss: 0.4985 | Val Acc: 0.8300\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
