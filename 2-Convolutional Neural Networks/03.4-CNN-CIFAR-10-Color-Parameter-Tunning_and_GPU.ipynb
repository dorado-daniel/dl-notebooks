{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad4aa8",
   "metadata": {},
   "source": [
    "## Parameter Tunning\n",
    "\n",
    "Vamos a mejorar el entrenamiento del anterior todavía más, hasta conseguir una convergencia que nos sirva.\n",
    "\n",
    "Aqui experimentaremos:\n",
    "\n",
    "- Subiendo EPOCHS\n",
    "\n",
    "- Aumentando el tamaño de LR\n",
    "\n",
    "- Tocando step_size y gamma de StepLR\n",
    "\n",
    "- Manteniendo Data Agumentation\n",
    "\n",
    "\n",
    "Además introducimos aqui Tensorboard para visualización de scalars y lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e462f3c",
   "metadata": {},
   "source": [
    "Comenzamos igual que en la sección anterior, lo que cambianmos aqui serán parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb06d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69f483",
   "metadata": {},
   "source": [
    "Vamos ahora a coger el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4857847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loader_train))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66939dd6",
   "metadata": {},
   "source": [
    "# Declaración de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #Primera capa de convolucion, entra una imagen de 3 canales y sale una imagen de 32 features\n",
    "            #El kernel es de 3x3 y el padding es 1 (same padding)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Segunda capa de convolucion, entra una imagen de 16 features y sale una imagen de 32 features\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Tercera capa de convolucion, entra una imagen de 32 features y sale una imagen de 64 features\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "             nn.Dropout(p=0.3), #Aplicamos dropout para evitar el overfitting\n",
    "        )\n",
    "\n",
    "        n_flat = get_flatten_size(self.features)\n",
    "\n",
    "        \n",
    "\n",
    "        #Añadimos una capa lineal para clasificar\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), #Aplanamos la imagen para que sea un vector\n",
    "          \n",
    "            \n",
    "            nn.Linear(in_features=n_flat, out_features=500), #Una capa lineal con n_flat entradas y 500 salidas\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.Dropout(p=0.4), #Aplicamos dropout para evitar el overfitting\n",
    "            \n",
    "            nn.Linear(in_features=500, out_features=10), #Una capa lineal con 500 entradas y 10 salidas\n",
    "            # No aplicamos softmax, ya que la funcion de perdida que usamos (CrossEntropyLoss) lo aplica por defecto\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instancia\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = CIFAR10CNN()\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd307e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "              ReLU-2           [-1, 16, 32, 32]               0\n",
      "         MaxPool2d-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
      "              ReLU-5           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-6             [-1, 32, 8, 8]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          18,496\n",
      "              ReLU-8             [-1, 64, 8, 8]               0\n",
      "         MaxPool2d-9             [-1, 64, 4, 4]               0\n",
      "          Dropout-10             [-1, 64, 4, 4]               0\n",
      "          Flatten-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 500]         512,500\n",
      "             ReLU-13                  [-1, 500]               0\n",
      "          Dropout-14                  [-1, 500]               0\n",
      "           Linear-15                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 541,094\n",
      "Trainable params: 541,094\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.52\n",
      "Params size (MB): 2.06\n",
      "Estimated Total Size (MB): 2.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45588c0e",
   "metadata": {},
   "source": [
    "### Vamos a declarar el bucle de entramiento\n",
    "\n",
    "Vamos a recordar los parametros tipicos de SDG\n",
    "\n",
    "| Parámetro  | Rol                                                      | Valor típico |\n",
    "| ---------- | -------------------------------------------------------- | ------------ |\n",
    "| `lr`       | tamaño del paso (cuánto cambian los pesos por gradiente) | 0.01–0.1     |\n",
    "| `momentum` | cuánto “recuerda” del gradiente anterior                 | 0.8–0.95     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.05\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86551f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.8367 | Train Acc: 0.3175 | Test Loss: 1.4214 | Test Acc: 0.4762\n",
      "Época 2/80 | Train Loss: 1.5088 | Train Acc: 0.4494 | Test Loss: 1.3316 | Test Acc: 0.5161\n",
      "Época 3/80 | Train Loss: 1.3671 | Train Acc: 0.5041 | Test Loss: 1.1834 | Test Acc: 0.5733\n",
      "Época 4/80 | Train Loss: 1.2835 | Train Acc: 0.5413 | Test Loss: 1.0577 | Test Acc: 0.6193\n",
      "Época 5/80 | Train Loss: 1.2364 | Train Acc: 0.5611 | Test Loss: 1.0282 | Test Acc: 0.6361\n",
      "Época 6/80 | Train Loss: 1.1980 | Train Acc: 0.5740 | Test Loss: 1.0262 | Test Acc: 0.6448\n",
      "Época 7/80 | Train Loss: 1.1787 | Train Acc: 0.5831 | Test Loss: 1.0124 | Test Acc: 0.6483\n",
      "Época 8/80 | Train Loss: 1.1607 | Train Acc: 0.5915 | Test Loss: 1.0397 | Test Acc: 0.6478\n",
      "Época 9/80 | Train Loss: 1.1318 | Train Acc: 0.6023 | Test Loss: 0.9483 | Test Acc: 0.6640\n",
      "Época 10/80 | Train Loss: 1.1193 | Train Acc: 0.6062 | Test Loss: 0.9070 | Test Acc: 0.6873\n",
      "Época 11/80 | Train Loss: 1.1071 | Train Acc: 0.6145 | Test Loss: 0.9522 | Test Acc: 0.6593\n",
      "Época 12/80 | Train Loss: 1.1082 | Train Acc: 0.6136 | Test Loss: 1.0016 | Test Acc: 0.6529\n",
      "Época 13/80 | Train Loss: 1.0956 | Train Acc: 0.6163 | Test Loss: 0.9050 | Test Acc: 0.6889\n",
      "Época 14/80 | Train Loss: 1.0921 | Train Acc: 0.6201 | Test Loss: 1.0070 | Test Acc: 0.6503\n",
      "Época 15/80 | Train Loss: 1.0848 | Train Acc: 0.6219 | Test Loss: 0.9125 | Test Acc: 0.6954\n",
      "Época 16/80 | Train Loss: 1.0719 | Train Acc: 0.6273 | Test Loss: 0.9056 | Test Acc: 0.6933\n",
      "Época 17/80 | Train Loss: 1.0650 | Train Acc: 0.6308 | Test Loss: 1.0393 | Test Acc: 0.6413\n",
      "Época 18/80 | Train Loss: 1.0567 | Train Acc: 0.6319 | Test Loss: 0.9028 | Test Acc: 0.6925\n",
      "Época 19/80 | Train Loss: 1.0543 | Train Acc: 0.6310 | Test Loss: 0.8654 | Test Acc: 0.7073\n",
      "Época 20/80 | Train Loss: 1.0591 | Train Acc: 0.6349 | Test Loss: 0.8930 | Test Acc: 0.6886\n",
      "Época 21/80 | Train Loss: 0.9132 | Train Acc: 0.6835 | Test Loss: 0.7764 | Test Acc: 0.7322\n",
      "Época 22/80 | Train Loss: 0.8699 | Train Acc: 0.6959 | Test Loss: 0.7600 | Test Acc: 0.7388\n",
      "Época 23/80 | Train Loss: 0.8634 | Train Acc: 0.7026 | Test Loss: 0.7683 | Test Acc: 0.7391\n",
      "Época 24/80 | Train Loss: 0.8543 | Train Acc: 0.7042 | Test Loss: 0.7415 | Test Acc: 0.7468\n",
      "Época 25/80 | Train Loss: 0.8450 | Train Acc: 0.7068 | Test Loss: 0.7320 | Test Acc: 0.7452\n",
      "Época 26/80 | Train Loss: 0.8315 | Train Acc: 0.7130 | Test Loss: 0.7135 | Test Acc: 0.7523\n",
      "Época 27/80 | Train Loss: 0.8409 | Train Acc: 0.7076 | Test Loss: 0.7251 | Test Acc: 0.7459\n",
      "Época 28/80 | Train Loss: 0.8301 | Train Acc: 0.7127 | Test Loss: 0.6929 | Test Acc: 0.7599\n",
      "Época 29/80 | Train Loss: 0.8103 | Train Acc: 0.7184 | Test Loss: 0.6725 | Test Acc: 0.7709\n",
      "Época 30/80 | Train Loss: 0.8121 | Train Acc: 0.7189 | Test Loss: 0.7117 | Test Acc: 0.7562\n",
      "Época 31/80 | Train Loss: 0.8058 | Train Acc: 0.7199 | Test Loss: 0.6722 | Test Acc: 0.7713\n",
      "Época 32/80 | Train Loss: 0.8253 | Train Acc: 0.7105 | Test Loss: 0.6791 | Test Acc: 0.7735\n",
      "Época 33/80 | Train Loss: 0.8039 | Train Acc: 0.7206 | Test Loss: 0.6606 | Test Acc: 0.7734\n",
      "Época 34/80 | Train Loss: 0.8045 | Train Acc: 0.7199 | Test Loss: 0.6995 | Test Acc: 0.7598\n",
      "Época 35/80 | Train Loss: 0.8004 | Train Acc: 0.7224 | Test Loss: 0.6951 | Test Acc: 0.7606\n",
      "Época 36/80 | Train Loss: 0.7978 | Train Acc: 0.7252 | Test Loss: 0.7009 | Test Acc: 0.7634\n",
      "Época 37/80 | Train Loss: 0.8020 | Train Acc: 0.7206 | Test Loss: 0.6915 | Test Acc: 0.7640\n",
      "Época 38/80 | Train Loss: 0.7868 | Train Acc: 0.7279 | Test Loss: 0.7335 | Test Acc: 0.7503\n",
      "Época 39/80 | Train Loss: 0.7941 | Train Acc: 0.7244 | Test Loss: 0.6863 | Test Acc: 0.7670\n",
      "Época 40/80 | Train Loss: 0.7915 | Train Acc: 0.7250 | Test Loss: 0.6511 | Test Acc: 0.7779\n",
      "Época 41/80 | Train Loss: 0.7176 | Train Acc: 0.7494 | Test Loss: 0.6242 | Test Acc: 0.7858\n",
      "Época 42/80 | Train Loss: 0.7033 | Train Acc: 0.7562 | Test Loss: 0.6241 | Test Acc: 0.7881\n",
      "Época 43/80 | Train Loss: 0.6893 | Train Acc: 0.7589 | Test Loss: 0.5844 | Test Acc: 0.8005\n",
      "Época 44/80 | Train Loss: 0.6880 | Train Acc: 0.7612 | Test Loss: 0.5919 | Test Acc: 0.7967\n",
      "Época 45/80 | Train Loss: 0.6834 | Train Acc: 0.7623 | Test Loss: 0.6037 | Test Acc: 0.7898\n",
      "Época 46/80 | Train Loss: 0.6787 | Train Acc: 0.7619 | Test Loss: 0.5931 | Test Acc: 0.7948\n",
      "Época 47/80 | Train Loss: 0.6779 | Train Acc: 0.7632 | Test Loss: 0.5839 | Test Acc: 0.8033\n",
      "Época 48/80 | Train Loss: 0.6776 | Train Acc: 0.7640 | Test Loss: 0.6033 | Test Acc: 0.7913\n",
      "Época 49/80 | Train Loss: 0.6667 | Train Acc: 0.7665 | Test Loss: 0.5954 | Test Acc: 0.7970\n",
      "Época 50/80 | Train Loss: 0.6757 | Train Acc: 0.7671 | Test Loss: 0.6234 | Test Acc: 0.7823\n",
      "Época 51/80 | Train Loss: 0.6737 | Train Acc: 0.7649 | Test Loss: 0.5798 | Test Acc: 0.8020\n",
      "Época 52/80 | Train Loss: 0.6701 | Train Acc: 0.7688 | Test Loss: 0.5761 | Test Acc: 0.8003\n",
      "Época 53/80 | Train Loss: 0.6678 | Train Acc: 0.7674 | Test Loss: 0.5786 | Test Acc: 0.7982\n",
      "Época 54/80 | Train Loss: 0.6664 | Train Acc: 0.7687 | Test Loss: 0.5874 | Test Acc: 0.7977\n",
      "Época 55/80 | Train Loss: 0.6653 | Train Acc: 0.7702 | Test Loss: 0.5635 | Test Acc: 0.8049\n",
      "Época 56/80 | Train Loss: 0.6615 | Train Acc: 0.7703 | Test Loss: 0.5592 | Test Acc: 0.8079\n",
      "Época 57/80 | Train Loss: 0.6531 | Train Acc: 0.7734 | Test Loss: 0.5703 | Test Acc: 0.8028\n",
      "Época 58/80 | Train Loss: 0.6551 | Train Acc: 0.7724 | Test Loss: 0.5742 | Test Acc: 0.8055\n",
      "Época 59/80 | Train Loss: 0.6621 | Train Acc: 0.7702 | Test Loss: 0.5627 | Test Acc: 0.8052\n",
      "Época 60/80 | Train Loss: 0.6571 | Train Acc: 0.7706 | Test Loss: 0.5548 | Test Acc: 0.8119\n",
      "Época 61/80 | Train Loss: 0.6097 | Train Acc: 0.7882 | Test Loss: 0.5320 | Test Acc: 0.8192\n",
      "Época 62/80 | Train Loss: 0.6059 | Train Acc: 0.7873 | Test Loss: 0.5256 | Test Acc: 0.8198\n",
      "Época 63/80 | Train Loss: 0.6027 | Train Acc: 0.7902 | Test Loss: 0.5287 | Test Acc: 0.8222\n",
      "Época 64/80 | Train Loss: 0.6013 | Train Acc: 0.7912 | Test Loss: 0.5417 | Test Acc: 0.8131\n",
      "Época 65/80 | Train Loss: 0.5960 | Train Acc: 0.7918 | Test Loss: 0.5281 | Test Acc: 0.8203\n",
      "Época 66/80 | Train Loss: 0.5966 | Train Acc: 0.7921 | Test Loss: 0.5236 | Test Acc: 0.8234\n",
      "Época 67/80 | Train Loss: 0.5940 | Train Acc: 0.7933 | Test Loss: 0.5345 | Test Acc: 0.8145\n",
      "Época 68/80 | Train Loss: 0.5884 | Train Acc: 0.7943 | Test Loss: 0.5638 | Test Acc: 0.8072\n",
      "Época 69/80 | Train Loss: 0.5945 | Train Acc: 0.7917 | Test Loss: 0.5174 | Test Acc: 0.8250\n",
      "Época 70/80 | Train Loss: 0.5988 | Train Acc: 0.7930 | Test Loss: 0.5325 | Test Acc: 0.8173\n",
      "Época 71/80 | Train Loss: 0.5950 | Train Acc: 0.7925 | Test Loss: 0.5334 | Test Acc: 0.8164\n",
      "Época 72/80 | Train Loss: 0.5914 | Train Acc: 0.7937 | Test Loss: 0.5371 | Test Acc: 0.8172\n",
      "Época 73/80 | Train Loss: 0.5834 | Train Acc: 0.7963 | Test Loss: 0.5300 | Test Acc: 0.8195\n",
      "Época 74/80 | Train Loss: 0.5797 | Train Acc: 0.7981 | Test Loss: 0.5204 | Test Acc: 0.8223\n",
      "Época 75/80 | Train Loss: 0.5861 | Train Acc: 0.7939 | Test Loss: 0.5163 | Test Acc: 0.8211\n",
      "Época 76/80 | Train Loss: 0.5894 | Train Acc: 0.7935 | Test Loss: 0.5144 | Test Acc: 0.8249\n",
      "Época 77/80 | Train Loss: 0.5871 | Train Acc: 0.7954 | Test Loss: 0.5093 | Test Acc: 0.8248\n",
      "Época 78/80 | Train Loss: 0.5852 | Train Acc: 0.7953 | Test Loss: 0.5239 | Test Acc: 0.8188\n",
      "Época 79/80 | Train Loss: 0.5779 | Train Acc: 0.7970 | Test Loss: 0.5506 | Test Acc: 0.8097\n",
      "Época 80/80 | Train Loss: 0.5820 | Train Acc: 0.7984 | Test Loss: 0.5298 | Test Acc: 0.8206\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/cifar10_cnn_step20_lr0015\")  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "    # <-- añade esto justo al final del bucle de épocas\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/test\",  test_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/test\",   test_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170503be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
