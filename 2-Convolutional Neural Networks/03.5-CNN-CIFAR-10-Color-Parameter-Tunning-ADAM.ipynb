{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad4aa8",
   "metadata": {},
   "source": [
    "## Parameter Tunning\n",
    "\n",
    "Vamos a mejorar el entrenamiento del anterior todavía más, hasta conseguir una convergencia que nos sirva.\n",
    "\n",
    "Aqui experimentaremos:\n",
    "\n",
    "- Cambiando GDR por ADAM, (Adaptative Momentum)\n",
    "\n",
    "\n",
    "Además introducimos aqui Tensorboard para visualización de lr, scalars y prepararemos correctamente para hacer test final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e462f3c",
   "metadata": {},
   "source": [
    "Comenzamos igual que en la sección anterior, lo que cambianmos aqui serán parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb06d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69f483",
   "metadata": {},
   "source": [
    "Vamos ahora a coger el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4857847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loader_train))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66939dd6",
   "metadata": {},
   "source": [
    "# Declaración de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #Primera capa de convolucion, entra una imagen de 3 canales y sale una imagen de 32 features\n",
    "            #El kernel es de 3x3 y el padding es 1 (same padding)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Segunda capa de convolucion, entra una imagen de 16 features y sale una imagen de 32 features\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "            #Tercera capa de convolucion, entra una imagen de 32 features y sale una imagen de 64 features\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "\n",
    "             nn.Dropout(p=0.3), #Aplicamos dropout para evitar el overfitting\n",
    "        )\n",
    "\n",
    "        n_flat = get_flatten_size(self.features)\n",
    "\n",
    "        \n",
    "\n",
    "        #Añadimos una capa lineal para clasificar\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), #Aplanamos la imagen para que sea un vector\n",
    "          \n",
    "            \n",
    "            nn.Linear(in_features=n_flat, out_features=500), #Una capa lineal con n_flat entradas y 500 salidas\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.Dropout(p=0.4), #Aplicamos dropout para evitar el overfitting\n",
    "            \n",
    "            nn.Linear(in_features=500, out_features=10), #Una capa lineal con 500 entradas y 10 salidas\n",
    "            # No aplicamos softmax, ya que la funcion de perdida que usamos (CrossEntropyLoss) lo aplica por defecto\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instancia\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = CIFAR10CNN()\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd307e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "              ReLU-2           [-1, 16, 32, 32]               0\n",
      "         MaxPool2d-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
      "              ReLU-5           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-6             [-1, 32, 8, 8]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          18,496\n",
      "              ReLU-8             [-1, 64, 8, 8]               0\n",
      "         MaxPool2d-9             [-1, 64, 4, 4]               0\n",
      "          Dropout-10             [-1, 64, 4, 4]               0\n",
      "          Flatten-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 500]         512,500\n",
      "             ReLU-13                  [-1, 500]               0\n",
      "          Dropout-14                  [-1, 500]               0\n",
      "           Linear-15                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 541,094\n",
      "Trainable params: 541,094\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.52\n",
      "Params size (MB): 2.06\n",
      "Estimated Total Size (MB): 2.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45588c0e",
   "metadata": {},
   "source": [
    "### Vamos a declarar el bucle de entramiento\n",
    "\n",
    "Vamos a recordar los parametros tipicos de SDG\n",
    "\n",
    "| Parámetro  | Rol                                                      | Valor típico |\n",
    "| ---------- | -------------------------------------------------------- | ------------ |\n",
    "| `lr`       | tamaño del paso (cuánto cambian los pesos por gradiente) | 0.01–0.1     |\n",
    "| `momentum` | cuánto “recuerda” del gradiente anterior                 | 0.8–0.95     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=500, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Definimos los parametros\n",
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay=1e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86551f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.8466 | Train Acc: 0.3120 | Val Loss: 1.5454 | Val Acc: 0.4346\n",
      "Época 2/80 | Train Loss: 1.5137 | Train Acc: 0.4471 | Val Loss: 1.3365 | Val Acc: 0.5137\n",
      "Época 3/80 | Train Loss: 1.3656 | Train Acc: 0.5058 | Val Loss: 1.1858 | Val Acc: 0.5649\n",
      "Época 4/80 | Train Loss: 1.2885 | Train Acc: 0.5356 | Val Loss: 1.0539 | Val Acc: 0.6316\n",
      "Época 5/80 | Train Loss: 1.2276 | Train Acc: 0.5608 | Val Loss: 1.0339 | Val Acc: 0.6369\n",
      "Época 6/80 | Train Loss: 1.1862 | Train Acc: 0.5817 | Val Loss: 1.0391 | Val Acc: 0.6387\n",
      "Época 7/80 | Train Loss: 1.1612 | Train Acc: 0.5880 | Val Loss: 0.9848 | Val Acc: 0.6616\n",
      "Época 8/80 | Train Loss: 1.1472 | Train Acc: 0.5963 | Val Loss: 1.0106 | Val Acc: 0.6527\n",
      "Época 9/80 | Train Loss: 1.1333 | Train Acc: 0.6003 | Val Loss: 0.9919 | Val Acc: 0.6495\n",
      "Época 10/80 | Train Loss: 1.1249 | Train Acc: 0.6043 | Val Loss: 0.9279 | Val Acc: 0.6763\n",
      "Época 11/80 | Train Loss: 1.1074 | Train Acc: 0.6127 | Val Loss: 1.1019 | Val Acc: 0.6138\n",
      "Época 12/80 | Train Loss: 1.1099 | Train Acc: 0.6141 | Val Loss: 1.0210 | Val Acc: 0.6470\n",
      "Época 13/80 | Train Loss: 1.0886 | Train Acc: 0.6193 | Val Loss: 0.9282 | Val Acc: 0.6787\n",
      "Época 14/80 | Train Loss: 1.0945 | Train Acc: 0.6159 | Val Loss: 0.8893 | Val Acc: 0.6952\n",
      "Época 15/80 | Train Loss: 1.0737 | Train Acc: 0.6284 | Val Loss: 0.9026 | Val Acc: 0.6941\n",
      "Época 16/80 | Train Loss: 1.0711 | Train Acc: 0.6257 | Val Loss: 0.9618 | Val Acc: 0.6665\n",
      "Época 17/80 | Train Loss: 1.0850 | Train Acc: 0.6247 | Val Loss: 0.8943 | Val Acc: 0.6992\n",
      "Época 18/80 | Train Loss: 1.0706 | Train Acc: 0.6299 | Val Loss: 0.9031 | Val Acc: 0.6926\n",
      "Época 19/80 | Train Loss: 1.0659 | Train Acc: 0.6308 | Val Loss: 0.8859 | Val Acc: 0.6962\n",
      "Época 20/80 | Train Loss: 1.0527 | Train Acc: 0.6341 | Val Loss: 0.9448 | Val Acc: 0.6692\n",
      "Época 21/80 | Train Loss: 0.9245 | Train Acc: 0.6807 | Val Loss: 0.7531 | Val Acc: 0.7473\n",
      "Época 22/80 | Train Loss: 0.8701 | Train Acc: 0.6989 | Val Loss: 0.7405 | Val Acc: 0.7490\n",
      "Época 23/80 | Train Loss: 0.8637 | Train Acc: 0.7008 | Val Loss: 0.7342 | Val Acc: 0.7498\n",
      "Época 24/80 | Train Loss: 0.8453 | Train Acc: 0.7055 | Val Loss: 0.7564 | Val Acc: 0.7460\n",
      "Época 25/80 | Train Loss: 0.8450 | Train Acc: 0.7063 | Val Loss: 0.7237 | Val Acc: 0.7530\n",
      "Época 26/80 | Train Loss: 0.8342 | Train Acc: 0.7097 | Val Loss: 0.7442 | Val Acc: 0.7473\n",
      "Época 27/80 | Train Loss: 0.8262 | Train Acc: 0.7112 | Val Loss: 0.7325 | Val Acc: 0.7540\n",
      "Época 28/80 | Train Loss: 0.8215 | Train Acc: 0.7133 | Val Loss: 0.6874 | Val Acc: 0.7654\n",
      "Época 29/80 | Train Loss: 0.8165 | Train Acc: 0.7168 | Val Loss: 0.6937 | Val Acc: 0.7609\n",
      "Época 30/80 | Train Loss: 0.8087 | Train Acc: 0.7175 | Val Loss: 0.6741 | Val Acc: 0.7712\n",
      "Época 31/80 | Train Loss: 0.8119 | Train Acc: 0.7179 | Val Loss: 0.6756 | Val Acc: 0.7687\n",
      "Época 32/80 | Train Loss: 0.8135 | Train Acc: 0.7157 | Val Loss: 0.6643 | Val Acc: 0.7705\n",
      "Época 33/80 | Train Loss: 0.7981 | Train Acc: 0.7199 | Val Loss: 0.6829 | Val Acc: 0.7682\n",
      "Época 34/80 | Train Loss: 0.8066 | Train Acc: 0.7167 | Val Loss: 0.6850 | Val Acc: 0.7646\n",
      "Época 35/80 | Train Loss: 0.7946 | Train Acc: 0.7249 | Val Loss: 0.7328 | Val Acc: 0.7526\n",
      "Época 36/80 | Train Loss: 0.7975 | Train Acc: 0.7239 | Val Loss: 0.6692 | Val Acc: 0.7740\n",
      "Época 37/80 | Train Loss: 0.7985 | Train Acc: 0.7224 | Val Loss: 0.6442 | Val Acc: 0.7872\n",
      "Época 38/80 | Train Loss: 0.7828 | Train Acc: 0.7267 | Val Loss: 0.6655 | Val Acc: 0.7753\n",
      "Época 39/80 | Train Loss: 0.7882 | Train Acc: 0.7253 | Val Loss: 0.6789 | Val Acc: 0.7681\n",
      "Época 40/80 | Train Loss: 0.7951 | Train Acc: 0.7241 | Val Loss: 0.6775 | Val Acc: 0.7780\n",
      "Época 41/80 | Train Loss: 0.7085 | Train Acc: 0.7509 | Val Loss: 0.6070 | Val Acc: 0.7957\n",
      "Época 42/80 | Train Loss: 0.6927 | Train Acc: 0.7588 | Val Loss: 0.5826 | Val Acc: 0.8014\n",
      "Época 43/80 | Train Loss: 0.6923 | Train Acc: 0.7600 | Val Loss: 0.5763 | Val Acc: 0.8032\n",
      "Época 44/80 | Train Loss: 0.6815 | Train Acc: 0.7629 | Val Loss: 0.5751 | Val Acc: 0.8042\n",
      "Época 45/80 | Train Loss: 0.6765 | Train Acc: 0.7642 | Val Loss: 0.5868 | Val Acc: 0.7995\n",
      "Época 46/80 | Train Loss: 0.6686 | Train Acc: 0.7662 | Val Loss: 0.5916 | Val Acc: 0.7971\n",
      "Época 47/80 | Train Loss: 0.6823 | Train Acc: 0.7620 | Val Loss: 0.5804 | Val Acc: 0.8018\n",
      "Época 48/80 | Train Loss: 0.6767 | Train Acc: 0.7640 | Val Loss: 0.5670 | Val Acc: 0.8046\n",
      "Época 49/80 | Train Loss: 0.6621 | Train Acc: 0.7700 | Val Loss: 0.5783 | Val Acc: 0.8028\n",
      "Época 50/80 | Train Loss: 0.6663 | Train Acc: 0.7691 | Val Loss: 0.5856 | Val Acc: 0.7984\n",
      "Época 51/80 | Train Loss: 0.6617 | Train Acc: 0.7688 | Val Loss: 0.5799 | Val Acc: 0.8002\n",
      "Época 52/80 | Train Loss: 0.6602 | Train Acc: 0.7683 | Val Loss: 0.5651 | Val Acc: 0.8065\n",
      "Época 53/80 | Train Loss: 0.6606 | Train Acc: 0.7688 | Val Loss: 0.5622 | Val Acc: 0.8080\n",
      "Época 54/80 | Train Loss: 0.6621 | Train Acc: 0.7709 | Val Loss: 0.5640 | Val Acc: 0.8101\n",
      "Época 55/80 | Train Loss: 0.6586 | Train Acc: 0.7697 | Val Loss: 0.5576 | Val Acc: 0.8101\n",
      "Época 56/80 | Train Loss: 0.6603 | Train Acc: 0.7708 | Val Loss: 0.5624 | Val Acc: 0.8078\n",
      "Época 57/80 | Train Loss: 0.6494 | Train Acc: 0.7731 | Val Loss: 0.5605 | Val Acc: 0.8076\n",
      "Época 58/80 | Train Loss: 0.6506 | Train Acc: 0.7728 | Val Loss: 0.5465 | Val Acc: 0.8116\n",
      "Época 59/80 | Train Loss: 0.6539 | Train Acc: 0.7703 | Val Loss: 0.5847 | Val Acc: 0.8023\n",
      "Época 60/80 | Train Loss: 0.6570 | Train Acc: 0.7695 | Val Loss: 0.5511 | Val Acc: 0.8100\n",
      "Época 61/80 | Train Loss: 0.6087 | Train Acc: 0.7875 | Val Loss: 0.5243 | Val Acc: 0.8227\n",
      "Época 62/80 | Train Loss: 0.6076 | Train Acc: 0.7878 | Val Loss: 0.5183 | Val Acc: 0.8225\n",
      "Época 63/80 | Train Loss: 0.5981 | Train Acc: 0.7901 | Val Loss: 0.5146 | Val Acc: 0.8240\n",
      "Época 64/80 | Train Loss: 0.5987 | Train Acc: 0.7911 | Val Loss: 0.5237 | Val Acc: 0.8231\n",
      "Época 65/80 | Train Loss: 0.6016 | Train Acc: 0.7912 | Val Loss: 0.5292 | Val Acc: 0.8181\n",
      "Época 66/80 | Train Loss: 0.5875 | Train Acc: 0.7935 | Val Loss: 0.5161 | Val Acc: 0.8215\n",
      "Época 67/80 | Train Loss: 0.5948 | Train Acc: 0.7922 | Val Loss: 0.5194 | Val Acc: 0.8231\n",
      "Época 68/80 | Train Loss: 0.5847 | Train Acc: 0.7967 | Val Loss: 0.5167 | Val Acc: 0.8218\n",
      "Época 69/80 | Train Loss: 0.5934 | Train Acc: 0.7930 | Val Loss: 0.5128 | Val Acc: 0.8241\n",
      "Época 70/80 | Train Loss: 0.5860 | Train Acc: 0.7977 | Val Loss: 0.5067 | Val Acc: 0.8245\n",
      "Época 71/80 | Train Loss: 0.5859 | Train Acc: 0.7958 | Val Loss: 0.5118 | Val Acc: 0.8234\n",
      "Época 72/80 | Train Loss: 0.5753 | Train Acc: 0.7985 | Val Loss: 0.5097 | Val Acc: 0.8248\n",
      "Época 73/80 | Train Loss: 0.5792 | Train Acc: 0.7961 | Val Loss: 0.5153 | Val Acc: 0.8239\n",
      "Época 74/80 | Train Loss: 0.5760 | Train Acc: 0.7958 | Val Loss: 0.5168 | Val Acc: 0.8241\n",
      "Época 75/80 | Train Loss: 0.5772 | Train Acc: 0.7977 | Val Loss: 0.4979 | Val Acc: 0.8288\n",
      "Época 76/80 | Train Loss: 0.5803 | Train Acc: 0.7974 | Val Loss: 0.5237 | Val Acc: 0.8185\n",
      "Época 77/80 | Train Loss: 0.5818 | Train Acc: 0.7949 | Val Loss: 0.4989 | Val Acc: 0.8319\n",
      "Época 78/80 | Train Loss: 0.5820 | Train Acc: 0.7967 | Val Loss: 0.5077 | Val Acc: 0.8250\n",
      "Época 79/80 | Train Loss: 0.5809 | Train Acc: 0.7982 | Val Loss: 0.4977 | Val Acc: 0.8298\n",
      "Época 80/80 | Train Loss: 0.5775 | Train Acc: 0.7990 | Val Loss: 0.4984 | Val Acc: 0.8290\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
