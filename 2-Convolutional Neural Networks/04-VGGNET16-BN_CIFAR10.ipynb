{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3d7956",
   "metadata": {},
   "source": [
    "Vamos a construir VGGNET para CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83935d",
   "metadata": {},
   "source": [
    "VGG16-BN-CIFAR:\n",
    "\n",
    "16 capas con parámetros entrenables (13 convoluciones + 3 lineales).\n",
    "\n",
    "Se divide en bloques:\n",
    "\n",
    "- 2 Bloques  (Conv + BatchNorm + Relu) x 2 -> MaxPool2\n",
    "\n",
    "- 3 Bloques  (Conv + BatchNorm + Relu) x 3 -> MaxPool2\n",
    "\n",
    "- Classificador Flatten → [Linear → ReLU → Dropout] ×2 → Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751ec4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a803b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "    \n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faca333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/dlvs/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([4, 3, 32, 32]) | Output: torch.Size([4, 10])\n",
      "Params totales: 15249354\n"
     ]
    }
   ],
   "source": [
    "# VGG16-BN-CIFAR — 13 conv (con BN) + 3 FC (con Dropout)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class VGG16_BN_CIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #==============================================\n",
    "            # Block 1: 3->64, 32->16\n",
    "            nn.Conv2d(3,   64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64,  64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 2: 64->128, 16->8\n",
    "            nn.Conv2d(64,  128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #============================================== \n",
    "            # Block 3: 128->256, 8->4 (tres conv)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 4: 256->512, 4->2 (tres conv)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 5: 512->512, 2->1 (tres conv)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "        )\n",
    "\n",
    "        flat = get_flatten_size(self.features, input_shape=(1,3,32,32))  # esperado: 512*1*1 = 512\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat, 512), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(512, 512),  nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "#GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "\n",
    "# Instancia + sanity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGG16_BN_CIFAR(p_drop=0.5).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb = torch.randn(4, 3, 32, 32, device=device)\n",
    "    yb = model(xb)\n",
    "print(\"Input:\", xb.shape, \"| Output:\", yb.shape)  # [4, 10]\n",
    "print(\"Params totales:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf634d",
   "metadata": {},
   "source": [
    "Vamos a definir hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183f1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16_BN_CIFAR(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Definimos los hparams\n",
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=1e-3)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 2.0784 | Train Acc: 0.1842 | Val Loss: 2.0352 | Val Acc: 0.1913\n",
      "Época 2/80 | Train Loss: 1.9994 | Train Acc: 0.2125 | Val Loss: 1.9119 | Val Acc: 0.2586\n",
      "Época 3/80 | Train Loss: 1.9201 | Train Acc: 0.2828 | Val Loss: 1.7730 | Val Acc: 0.3361\n",
      "Época 4/80 | Train Loss: 1.7744 | Train Acc: 0.3477 | Val Loss: 1.6908 | Val Acc: 0.3617\n",
      "Época 5/80 | Train Loss: 1.6612 | Train Acc: 0.4154 | Val Loss: 1.5906 | Val Acc: 0.4506\n",
      "Época 6/80 | Train Loss: 1.5413 | Train Acc: 0.4935 | Val Loss: 1.6568 | Val Acc: 0.4499\n",
      "Época 7/80 | Train Loss: 1.4486 | Train Acc: 0.5383 | Val Loss: 1.6919 | Val Acc: 0.4621\n",
      "Época 8/80 | Train Loss: 1.3527 | Train Acc: 0.6125 | Val Loss: 1.3550 | Val Acc: 0.6191\n",
      "Época 9/80 | Train Loss: 1.2760 | Train Acc: 0.6677 | Val Loss: 1.5226 | Val Acc: 0.5787\n",
      "Época 10/80 | Train Loss: 1.2293 | Train Acc: 0.6953 | Val Loss: 1.3716 | Val Acc: 0.6377\n",
      "Época 11/80 | Train Loss: 1.1754 | Train Acc: 0.7236 | Val Loss: 1.3700 | Val Acc: 0.6547\n",
      "Época 12/80 | Train Loss: 1.1458 | Train Acc: 0.7426 | Val Loss: 1.2191 | Val Acc: 0.7099\n",
      "Época 13/80 | Train Loss: 1.1149 | Train Acc: 0.7560 | Val Loss: 1.1586 | Val Acc: 0.7200\n",
      "Época 14/80 | Train Loss: 1.0890 | Train Acc: 0.7675 | Val Loss: 1.6481 | Val Acc: 0.5791\n",
      "Época 15/80 | Train Loss: 1.0540 | Train Acc: 0.7801 | Val Loss: 1.2306 | Val Acc: 0.7099\n",
      "Época 16/80 | Train Loss: 1.0345 | Train Acc: 0.7876 | Val Loss: 1.1939 | Val Acc: 0.7079\n",
      "Época 17/80 | Train Loss: 1.0356 | Train Acc: 0.7881 | Val Loss: 1.3591 | Val Acc: 0.6745\n",
      "Época 18/80 | Train Loss: 0.9096 | Train Acc: 0.8352 | Val Loss: 0.9318 | Val Acc: 0.8158\n",
      "Época 19/80 | Train Loss: 0.8724 | Train Acc: 0.8487 | Val Loss: 0.9574 | Val Acc: 0.8143\n",
      "Época 20/80 | Train Loss: 0.8648 | Train Acc: 0.8546 | Val Loss: 0.9525 | Val Acc: 0.8177\n",
      "Época 21/80 | Train Loss: 0.8614 | Train Acc: 0.8562 | Val Loss: 0.9816 | Val Acc: 0.8013\n",
      "Época 22/80 | Train Loss: 0.8617 | Train Acc: 0.8562 | Val Loss: 1.2111 | Val Acc: 0.7300\n",
      "Época 23/80 | Train Loss: 0.7822 | Train Acc: 0.8880 | Val Loss: 0.8389 | Val Acc: 0.8577\n",
      "Época 24/80 | Train Loss: 0.7547 | Train Acc: 0.9006 | Val Loss: 0.8818 | Val Acc: 0.8484\n",
      "Época 25/80 | Train Loss: 0.7473 | Train Acc: 0.9016 | Val Loss: 0.8370 | Val Acc: 0.8594\n",
      "Época 26/80 | Train Loss: 0.7432 | Train Acc: 0.9042 | Val Loss: 0.8573 | Val Acc: 0.8512\n",
      "Época 27/80 | Train Loss: 0.7408 | Train Acc: 0.9045 | Val Loss: 0.8161 | Val Acc: 0.8693\n",
      "Época 28/80 | Train Loss: 0.7382 | Train Acc: 0.9052 | Val Loss: 0.8777 | Val Acc: 0.8412\n",
      "Época 29/80 | Train Loss: 0.7344 | Train Acc: 0.9077 | Val Loss: 0.8452 | Val Acc: 0.8557\n",
      "Época 30/80 | Train Loss: 0.7349 | Train Acc: 0.9064 | Val Loss: 0.8100 | Val Acc: 0.8711\n",
      "Época 31/80 | Train Loss: 0.7301 | Train Acc: 0.9095 | Val Loss: 0.8628 | Val Acc: 0.8529\n",
      "Época 32/80 | Train Loss: 0.7320 | Train Acc: 0.9083 | Val Loss: 0.9151 | Val Acc: 0.8329\n",
      "Época 33/80 | Train Loss: 0.7252 | Train Acc: 0.9128 | Val Loss: 0.8917 | Val Acc: 0.8430\n",
      "Época 34/80 | Train Loss: 0.7307 | Train Acc: 0.9090 | Val Loss: 0.8962 | Val Acc: 0.8396\n",
      "Época 35/80 | Train Loss: 0.6634 | Train Acc: 0.9375 | Val Loss: 0.7597 | Val Acc: 0.8978\n",
      "Época 36/80 | Train Loss: 0.6456 | Train Acc: 0.9456 | Val Loss: 0.7841 | Val Acc: 0.8841\n",
      "Época 37/80 | Train Loss: 0.6411 | Train Acc: 0.9466 | Val Loss: 0.7898 | Val Acc: 0.8873\n",
      "Época 38/80 | Train Loss: 0.6355 | Train Acc: 0.9486 | Val Loss: 0.7827 | Val Acc: 0.8858\n",
      "Época 39/80 | Train Loss: 0.6325 | Train Acc: 0.9517 | Val Loss: 0.7865 | Val Acc: 0.8871\n",
      "Época 40/80 | Train Loss: 0.5979 | Train Acc: 0.9648 | Val Loss: 0.7311 | Val Acc: 0.9115\n",
      "Época 41/80 | Train Loss: 0.5898 | Train Acc: 0.9688 | Val Loss: 0.7538 | Val Acc: 0.9052\n",
      "Época 42/80 | Train Loss: 0.5815 | Train Acc: 0.9723 | Val Loss: 0.7403 | Val Acc: 0.9050\n",
      "Época 43/80 | Train Loss: 0.5807 | Train Acc: 0.9721 | Val Loss: 0.7480 | Val Acc: 0.9041\n",
      "Época 44/80 | Train Loss: 0.5757 | Train Acc: 0.9742 | Val Loss: 0.7403 | Val Acc: 0.9066\n",
      "Época 45/80 | Train Loss: 0.5634 | Train Acc: 0.9797 | Val Loss: 0.7257 | Val Acc: 0.9126\n",
      "Época 46/80 | Train Loss: 0.5558 | Train Acc: 0.9831 | Val Loss: 0.7301 | Val Acc: 0.9133\n",
      "Época 47/80 | Train Loss: 0.5519 | Train Acc: 0.9848 | Val Loss: 0.7264 | Val Acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar modelo a ONNX (compatibilidad PyTorch <=2.2)\n",
    "import torch.onnx\n",
    "\n",
    "model.eval()\n",
    "model_cpu = model.to('cpu').to(memory_format=torch.contiguous_format)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    dummy_input,\n",
    "    \"cifar10cnn.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    dynamo=False,  # fuerza el exportador estable\n",
    "    training=torch.onnx.TrainingMode.EVAL\n",
    ")\n",
    "\n",
    "print(\"Modelo exportado correctamente a cifar10cnn.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
