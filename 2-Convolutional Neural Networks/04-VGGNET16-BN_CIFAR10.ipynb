{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3d7956",
   "metadata": {},
   "source": [
    "Vamos a construir VGGNET para CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83935d",
   "metadata": {},
   "source": [
    "VGG16-BN-CIFAR:\n",
    "\n",
    "16 capas con parámetros entrenables (13 convoluciones + 3 lineales).\n",
    "\n",
    "Se divide en bloques:\n",
    "\n",
    "- 2 Bloques  (Conv + BatchNorm + Relu) x 2 -> MaxPool2\n",
    "\n",
    "- 3 Bloques  (Conv + BatchNorm + Relu) x 3 -> MaxPool2\n",
    "\n",
    "- Classificador Flatten → [Linear → ReLU → Dropout] ×2 → Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751ec4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a803b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "    \n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1faca333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/dlvs/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([4, 3, 32, 32]) | Output: torch.Size([4, 10])\n",
      "Params totales: 15249354\n"
     ]
    }
   ],
   "source": [
    "# VGG16-BN-CIFAR — 13 conv (con BN) + 3 FC (con Dropout)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class VGG16_BN_CIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #==============================================\n",
    "            # Block 1: 3->64, 32->16\n",
    "            nn.Conv2d(3,   64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64,  64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 2: 64->128, 16->8\n",
    "            nn.Conv2d(64,  128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #============================================== \n",
    "            # Block 3: 128->256, 8->4 (tres conv)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 4: 256->512, 4->2 (tres conv)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "\n",
    "            #==============================================\n",
    "            # Block 5: 512->512, 2->1 (tres conv)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(2),\n",
    "            #==============================================\n",
    "        )\n",
    "\n",
    "        flat = get_flatten_size(self.features, input_shape=(1,3,32,32))  # esperado: 512*1*1 = 512\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat, 512), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(512, 512),  nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "#GPU\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "\n",
    "# Instancia + sanity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGG16_BN_CIFAR(p_drop=0.5).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb = torch.randn(4, 3, 32, 32, device=device)\n",
    "    yb = model(xb)\n",
    "print(\"Input:\", xb.shape, \"| Output:\", yb.shape)  # [4, 10]\n",
    "print(\"Params totales:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf634d",
   "metadata": {},
   "source": [
    "Vamos a definir hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183f1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16_BN_CIFAR(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Definimos los hparams\n",
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=1e-3)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a5e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 2.0831 | Train Acc: 0.1842 | Val Loss: 1.9979 | Val Acc: 0.1865\n",
      "Época 2/80 | Train Loss: 1.9970 | Train Acc: 0.2228 | Val Loss: 1.9313 | Val Acc: 0.2457\n",
      "Época 3/80 | Train Loss: 1.9000 | Train Acc: 0.2949 | Val Loss: 1.7818 | Val Acc: 0.3421\n",
      "Época 4/80 | Train Loss: 1.7635 | Train Acc: 0.3560 | Val Loss: 1.7530 | Val Acc: 0.3550\n",
      "Época 5/80 | Train Loss: 1.6270 | Train Acc: 0.4429 | Val Loss: 1.6119 | Val Acc: 0.4661\n",
      "Época 6/80 | Train Loss: 1.5077 | Train Acc: 0.5111 | Val Loss: 1.5256 | Val Acc: 0.5157\n",
      "Época 7/80 | Train Loss: 1.4162 | Train Acc: 0.5759 | Val Loss: 1.7369 | Val Acc: 0.4964\n",
      "Época 8/80 | Train Loss: 1.3124 | Train Acc: 0.6541 | Val Loss: 1.3699 | Val Acc: 0.6211\n",
      "Época 9/80 | Train Loss: 1.2411 | Train Acc: 0.6875 | Val Loss: 1.5082 | Val Acc: 0.6032\n",
      "Época 10/80 | Train Loss: 1.1968 | Train Acc: 0.7134 | Val Loss: 1.2733 | Val Acc: 0.6701\n",
      "Época 11/80 | Train Loss: 1.1521 | Train Acc: 0.7370 | Val Loss: 1.2220 | Val Acc: 0.6888\n",
      "Época 12/80 | Train Loss: 1.1264 | Train Acc: 0.7500 | Val Loss: 1.2240 | Val Acc: 0.7134\n",
      "Época 13/80 | Train Loss: 1.0972 | Train Acc: 0.7615 | Val Loss: 1.2581 | Val Acc: 0.6852\n",
      "Época 14/80 | Train Loss: 1.0676 | Train Acc: 0.7729 | Val Loss: 1.1699 | Val Acc: 0.7219\n",
      "Época 15/80 | Train Loss: 1.0468 | Train Acc: 0.7843 | Val Loss: 1.3162 | Val Acc: 0.6532\n",
      "Época 16/80 | Train Loss: 1.0322 | Train Acc: 0.7894 | Val Loss: 1.1098 | Val Acc: 0.7460\n",
      "Época 17/80 | Train Loss: 1.0281 | Train Acc: 0.7915 | Val Loss: 1.0534 | Val Acc: 0.7761\n",
      "Época 18/80 | Train Loss: 1.0130 | Train Acc: 0.7976 | Val Loss: 1.0737 | Val Acc: 0.7643\n",
      "Época 19/80 | Train Loss: 0.9952 | Train Acc: 0.8060 | Val Loss: 1.3808 | Val Acc: 0.6671\n",
      "Época 20/80 | Train Loss: 0.9899 | Train Acc: 0.8076 | Val Loss: 1.2240 | Val Acc: 0.7091\n",
      "Época 21/80 | Train Loss: 0.9822 | Train Acc: 0.8097 | Val Loss: 1.0469 | Val Acc: 0.7724\n",
      "Época 22/80 | Train Loss: 0.9775 | Train Acc: 0.8120 | Val Loss: 0.9716 | Val Acc: 0.8049\n",
      "Época 23/80 | Train Loss: 0.9663 | Train Acc: 0.8161 | Val Loss: 1.0961 | Val Acc: 0.7513\n",
      "Época 24/80 | Train Loss: 0.9604 | Train Acc: 0.8185 | Val Loss: 1.0908 | Val Acc: 0.7606\n",
      "Época 25/80 | Train Loss: 0.9567 | Train Acc: 0.8213 | Val Loss: 1.0683 | Val Acc: 0.7707\n",
      "Época 26/80 | Train Loss: 0.9479 | Train Acc: 0.8243 | Val Loss: 1.2371 | Val Acc: 0.7079\n",
      "Época 27/80 | Train Loss: 0.8295 | Train Acc: 0.8703 | Val Loss: 0.8929 | Val Acc: 0.8424\n",
      "Época 28/80 | Train Loss: 0.8075 | Train Acc: 0.8780 | Val Loss: 0.9084 | Val Acc: 0.8277\n",
      "Época 29/80 | Train Loss: 0.8026 | Train Acc: 0.8794 | Val Loss: 0.8643 | Val Acc: 0.8460\n",
      "Época 30/80 | Train Loss: 0.8041 | Train Acc: 0.8772 | Val Loss: 0.8680 | Val Acc: 0.8472\n",
      "Época 31/80 | Train Loss: 0.8011 | Train Acc: 0.8794 | Val Loss: 0.8827 | Val Acc: 0.8410\n",
      "Época 32/80 | Train Loss: 0.8040 | Train Acc: 0.8778 | Val Loss: 0.9267 | Val Acc: 0.8269\n",
      "Época 33/80 | Train Loss: 0.8033 | Train Acc: 0.8796 | Val Loss: 0.8479 | Val Acc: 0.8573\n",
      "Época 34/80 | Train Loss: 0.8000 | Train Acc: 0.8802 | Val Loss: 0.9243 | Val Acc: 0.8249\n",
      "Época 35/80 | Train Loss: 0.8086 | Train Acc: 0.8770 | Val Loss: 0.9891 | Val Acc: 0.8065\n",
      "Época 36/80 | Train Loss: 0.8000 | Train Acc: 0.8785 | Val Loss: 0.9238 | Val Acc: 0.8281\n",
      "Época 37/80 | Train Loss: 0.7904 | Train Acc: 0.8841 | Val Loss: 0.9641 | Val Acc: 0.8150\n",
      "Época 38/80 | Train Loss: 0.7052 | Train Acc: 0.9198 | Val Loss: 0.7956 | Val Acc: 0.8781\n",
      "Época 39/80 | Train Loss: 0.6866 | Train Acc: 0.9270 | Val Loss: 0.7716 | Val Acc: 0.8863\n",
      "Época 40/80 | Train Loss: 0.6786 | Train Acc: 0.9285 | Val Loss: 0.7937 | Val Acc: 0.8784\n",
      "Época 41/80 | Train Loss: 0.6781 | Train Acc: 0.9311 | Val Loss: 0.8155 | Val Acc: 0.8708\n",
      "Época 42/80 | Train Loss: 0.6799 | Train Acc: 0.9287 | Val Loss: 0.8081 | Val Acc: 0.8723\n",
      "Época 43/80 | Train Loss: 0.6792 | Train Acc: 0.9294 | Val Loss: 0.8500 | Val Acc: 0.8605\n",
      "Época 44/80 | Train Loss: 0.6251 | Train Acc: 0.9531 | Val Loss: 0.7181 | Val Acc: 0.9070\n",
      "Época 45/80 | Train Loss: 0.6125 | Train Acc: 0.9585 | Val Loss: 0.7650 | Val Acc: 0.8971\n",
      "Época 46/80 | Train Loss: 0.6109 | Train Acc: 0.9587 | Val Loss: 0.7377 | Val Acc: 0.9053\n",
      "Época 47/80 | Train Loss: 0.6037 | Train Acc: 0.9616 | Val Loss: 0.7361 | Val Acc: 0.9056\n",
      "Época 48/80 | Train Loss: 0.6022 | Train Acc: 0.9623 | Val Loss: 0.7557 | Val Acc: 0.8948\n",
      "Época 49/80 | Train Loss: 0.5759 | Train Acc: 0.9736 | Val Loss: 0.7304 | Val Acc: 0.9102\n",
      "Época 50/80 | Train Loss: 0.5640 | Train Acc: 0.9785 | Val Loss: 0.7231 | Val Acc: 0.9147\n",
      "Época 51/80 | Train Loss: 0.5621 | Train Acc: 0.9805 | Val Loss: 0.7265 | Val Acc: 0.9104\n",
      "Época 52/80 | Train Loss: 0.5611 | Train Acc: 0.9806 | Val Loss: 0.7329 | Val Acc: 0.9127\n",
      "Época 53/80 | Train Loss: 0.5487 | Train Acc: 0.9857 | Val Loss: 0.7175 | Val Acc: 0.9196\n",
      "Época 54/80 | Train Loss: 0.5452 | Train Acc: 0.9867 | Val Loss: 0.7128 | Val Acc: 0.9209\n",
      "Época 55/80 | Train Loss: 0.5416 | Train Acc: 0.9880 | Val Loss: 0.7140 | Val Acc: 0.9216\n",
      "Época 56/80 | Train Loss: 0.5395 | Train Acc: 0.9892 | Val Loss: 0.7193 | Val Acc: 0.9190\n",
      "Época 57/80 | Train Loss: 0.5376 | Train Acc: 0.9900 | Val Loss: 0.7195 | Val Acc: 0.9205\n",
      "Época 58/80 | Train Loss: 0.5373 | Train Acc: 0.9902 | Val Loss: 0.7228 | Val Acc: 0.9205\n",
      "Época 59/80 | Train Loss: 0.5338 | Train Acc: 0.9916 | Val Loss: 0.7195 | Val Acc: 0.9223\n",
      "Época 60/80 | Train Loss: 0.5319 | Train Acc: 0.9927 | Val Loss: 0.7163 | Val Acc: 0.9233\n",
      "Época 61/80 | Train Loss: 0.5320 | Train Acc: 0.9921 | Val Loss: 0.7160 | Val Acc: 0.9227\n",
      "Época 62/80 | Train Loss: 0.5303 | Train Acc: 0.9932 | Val Loss: 0.7199 | Val Acc: 0.9229\n",
      "Época 63/80 | Train Loss: 0.5286 | Train Acc: 0.9935 | Val Loss: 0.7193 | Val Acc: 0.9225\n",
      "Época 64/80 | Train Loss: 0.5278 | Train Acc: 0.9941 | Val Loss: 0.7186 | Val Acc: 0.9225\n",
      "Época 65/80 | Train Loss: 0.5274 | Train Acc: 0.9941 | Val Loss: 0.7170 | Val Acc: 0.9221\n",
      "Época 66/80 | Train Loss: 0.5273 | Train Acc: 0.9943 | Val Loss: 0.7184 | Val Acc: 0.9222\n",
      "Época 67/80 | Train Loss: 0.5265 | Train Acc: 0.9944 | Val Loss: 0.7173 | Val Acc: 0.9233\n",
      "Época 68/80 | Train Loss: 0.5261 | Train Acc: 0.9947 | Val Loss: 0.7166 | Val Acc: 0.9232\n",
      "Época 69/80 | Train Loss: 0.5254 | Train Acc: 0.9954 | Val Loss: 0.7164 | Val Acc: 0.9230\n",
      "Época 70/80 | Train Loss: 0.5242 | Train Acc: 0.9957 | Val Loss: 0.7176 | Val Acc: 0.9229\n",
      "Época 71/80 | Train Loss: 0.5246 | Train Acc: 0.9956 | Val Loss: 0.7163 | Val Acc: 0.9235\n",
      "Época 72/80 | Train Loss: 0.5247 | Train Acc: 0.9956 | Val Loss: 0.7167 | Val Acc: 0.9240\n",
      "Época 73/80 | Train Loss: 0.5249 | Train Acc: 0.9957 | Val Loss: 0.7150 | Val Acc: 0.9240\n",
      "Época 74/80 | Train Loss: 0.5246 | Train Acc: 0.9956 | Val Loss: 0.7157 | Val Acc: 0.9233\n",
      "Época 75/80 | Train Loss: 0.5236 | Train Acc: 0.9957 | Val Loss: 0.7170 | Val Acc: 0.9228\n",
      "Época 76/80 | Train Loss: 0.5245 | Train Acc: 0.9953 | Val Loss: 0.7162 | Val Acc: 0.9236\n",
      "Época 77/80 | Train Loss: 0.5236 | Train Acc: 0.9957 | Val Loss: 0.7162 | Val Acc: 0.9236\n",
      "Época 78/80 | Train Loss: 0.5244 | Train Acc: 0.9956 | Val Loss: 0.7157 | Val Acc: 0.9237\n",
      "Época 79/80 | Train Loss: 0.5240 | Train Acc: 0.9957 | Val Loss: 0.7164 | Val Acc: 0.9226\n",
      "Época 80/80 | Train Loss: 0.5240 | Train Acc: 0.9957 | Val Loss: 0.7174 | Val Acc: 0.9229\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d8d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5133/372281290.py:8: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado correctamente a cifar10cnn.onnx\n"
     ]
    }
   ],
   "source": [
    "# Exportar modelo a ONNX (compatibilidad PyTorch <=2.2)\n",
    "import torch.onnx\n",
    "\n",
    "model.eval()\n",
    "model_cpu = model.to('cpu').to(memory_format=torch.contiguous_format)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    dummy_input,\n",
    "    \"cifar10cnn_vggnet.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    dynamo=False,  # fuerza el exportador estable\n",
    "    training=torch.onnx.TrainingMode.EVAL\n",
    ")\n",
    "\n",
    "print(\"Modelo exportado correctamente a cifar10cnn.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
