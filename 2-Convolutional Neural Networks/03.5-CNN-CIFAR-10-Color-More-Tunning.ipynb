{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad4aa8",
   "metadata": {},
   "source": [
    "## More Tunning\n",
    "\n",
    "Aqui hemos tocado más cosas:\n",
    "\n",
    "- Duplicado los parametros y añadido otra capa CONV\n",
    "\n",
    "    4 capas convolucionales: 3→64→128→256→512 canales (duplicando los filtros)\n",
    "\n",
    "- Añadimos BatchNorm2d después de cada capa convolucional (reduce covariant shift)\n",
    "- Cambiado el scheduler a ReduceLROnPlateau, que reduce el LR en vase al val_acc automáticamente\n",
    "- Usamos Global Average Pooling (AdaptiveAvgPool2d) en lugar de Flatten\n",
    "- Clasificador simplificado: solo Linear(512→10)\n",
    "- Solo Dropout(0.3) después de la 3ª capa convolucional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceb06d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d9a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "    \n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69f483",
   "metadata": {},
   "source": [
    "Vamos ahora a coger el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4857847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loader_train))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66939dd6",
   "metadata": {},
   "source": [
    "# Declaración de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ba618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Dropout(p=0.3, inplace=False)\n",
      "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #Primera capa de convolucion, entra una imagen de 3 canales y sale una imagen de 32 features\n",
    "            #El kernel es de 3x3 y el padding es 1 (same padding)\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            \n",
    "            #Segunda capa de convolucion, entra una imagen de 16 features y sale una imagen de 32 features\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            \n",
    "            #Tercera capa de convolucion, entra una imagen de 32 features y sale una imagen de 64 features\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            nn.Dropout(p=0.3), #Aplicamos dropout para evitar el overfitting\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instancia\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = CIFAR10CNN()\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bd307e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
      "            Conv2d-9            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-10            [-1, 256, 8, 8]             512\n",
      "             ReLU-11            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-12            [-1, 256, 4, 4]               0\n",
      "          Dropout-13            [-1, 256, 4, 4]               0\n",
      "           Conv2d-14            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-15            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-16            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-17            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-18            [-1, 512, 1, 1]               0\n",
      "           Linear-19                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 1,557,066\n",
      "Trainable params: 1,557,066\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.08\n",
      "Params size (MB): 5.94\n",
      "Estimated Total Size (MB): 9.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45588c0e",
   "metadata": {},
   "source": [
    "### Vamos a declarar el bucle de entramiento\n",
    "\n",
    "Vamos a recordar los parametros tipicos de SDG\n",
    "\n",
    "| Parámetro  | Rol                                                      | Valor típico |\n",
    "| ---------- | -------------------------------------------------------- | ------------ |\n",
    "| `lr`       | tamaño del paso (cuánto cambian los pesos por gradiente) | 0.01–0.1     |\n",
    "| `momentum` | cuánto “recuerda” del gradiente anterior                 | 0.8–0.95     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a0e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU()\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Definimos los parametros\n",
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.05\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=1e-3)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a54e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86551f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.6915 | Train Acc: 0.3973 | Val Loss: 1.4307 | Val Acc: 0.5009\n",
      "Época 2/80 | Train Loss: 1.2086 | Train Acc: 0.5655 | Val Loss: 1.2092 | Val Acc: 0.5801\n",
      "Época 3/80 | Train Loss: 1.0253 | Train Acc: 0.6362 | Val Loss: 1.0438 | Val Acc: 0.6394\n",
      "Época 4/80 | Train Loss: 0.9074 | Train Acc: 0.6774 | Val Loss: 0.8206 | Val Acc: 0.7101\n",
      "Época 5/80 | Train Loss: 0.8040 | Train Acc: 0.7147 | Val Loss: 0.8265 | Val Acc: 0.7199\n",
      "Época 6/80 | Train Loss: 0.7465 | Train Acc: 0.7391 | Val Loss: 0.7580 | Val Acc: 0.7407\n",
      "Época 7/80 | Train Loss: 0.7024 | Train Acc: 0.7541 | Val Loss: 0.7614 | Val Acc: 0.7348\n",
      "Época 8/80 | Train Loss: 0.6534 | Train Acc: 0.7714 | Val Loss: 0.7250 | Val Acc: 0.7565\n",
      "Época 9/80 | Train Loss: 0.6275 | Train Acc: 0.7821 | Val Loss: 0.6384 | Val Acc: 0.7784\n",
      "Época 10/80 | Train Loss: 0.6070 | Train Acc: 0.7915 | Val Loss: 0.5968 | Val Acc: 0.7931\n",
      "Época 11/80 | Train Loss: 0.5813 | Train Acc: 0.7975 | Val Loss: 0.6093 | Val Acc: 0.7958\n",
      "Época 12/80 | Train Loss: 0.5718 | Train Acc: 0.8016 | Val Loss: 0.6861 | Val Acc: 0.7638\n",
      "Época 13/80 | Train Loss: 0.5562 | Train Acc: 0.8081 | Val Loss: 0.6243 | Val Acc: 0.7875\n",
      "Época 14/80 | Train Loss: 0.5404 | Train Acc: 0.8122 | Val Loss: 0.5629 | Val Acc: 0.8077\n",
      "Época 15/80 | Train Loss: 0.5345 | Train Acc: 0.8151 | Val Loss: 0.6481 | Val Acc: 0.7773\n",
      "Época 16/80 | Train Loss: 0.5162 | Train Acc: 0.8219 | Val Loss: 0.5628 | Val Acc: 0.8062\n",
      "Época 17/80 | Train Loss: 0.5106 | Train Acc: 0.8224 | Val Loss: 0.5723 | Val Acc: 0.8095\n",
      "Época 18/80 | Train Loss: 0.5037 | Train Acc: 0.8269 | Val Loss: 0.5741 | Val Acc: 0.8070\n",
      "Época 19/80 | Train Loss: 0.4132 | Train Acc: 0.8573 | Val Loss: 0.4372 | Val Acc: 0.8505\n",
      "Época 20/80 | Train Loss: 0.3964 | Train Acc: 0.8639 | Val Loss: 0.4282 | Val Acc: 0.8526\n",
      "Época 21/80 | Train Loss: 0.3998 | Train Acc: 0.8629 | Val Loss: 0.4793 | Val Acc: 0.8412\n",
      "Época 22/80 | Train Loss: 0.3789 | Train Acc: 0.8683 | Val Loss: 0.4359 | Val Acc: 0.8522\n",
      "Época 23/80 | Train Loss: 0.3814 | Train Acc: 0.8666 | Val Loss: 0.4315 | Val Acc: 0.8546\n",
      "Época 24/80 | Train Loss: 0.3785 | Train Acc: 0.8689 | Val Loss: 0.4426 | Val Acc: 0.8502\n",
      "Época 25/80 | Train Loss: 0.3218 | Train Acc: 0.8888 | Val Loss: 0.3931 | Val Acc: 0.8655\n",
      "Época 26/80 | Train Loss: 0.3013 | Train Acc: 0.8951 | Val Loss: 0.3795 | Val Acc: 0.8681\n",
      "Época 27/80 | Train Loss: 0.3038 | Train Acc: 0.8958 | Val Loss: 0.3914 | Val Acc: 0.8666\n",
      "Época 28/80 | Train Loss: 0.2900 | Train Acc: 0.9008 | Val Loss: 0.3839 | Val Acc: 0.8694\n",
      "Época 29/80 | Train Loss: 0.2904 | Train Acc: 0.8989 | Val Loss: 0.3922 | Val Acc: 0.8685\n",
      "Época 30/80 | Train Loss: 0.2896 | Train Acc: 0.8984 | Val Loss: 0.3691 | Val Acc: 0.8751\n",
      "Época 31/80 | Train Loss: 0.2818 | Train Acc: 0.9030 | Val Loss: 0.3732 | Val Acc: 0.8729\n",
      "Época 32/80 | Train Loss: 0.2858 | Train Acc: 0.9005 | Val Loss: 0.4198 | Val Acc: 0.8608\n",
      "Época 33/80 | Train Loss: 0.2836 | Train Acc: 0.9011 | Val Loss: 0.4062 | Val Acc: 0.8637\n",
      "Época 34/80 | Train Loss: 0.2795 | Train Acc: 0.9019 | Val Loss: 0.3909 | Val Acc: 0.8737\n",
      "Época 35/80 | Train Loss: 0.2396 | Train Acc: 0.9175 | Val Loss: 0.3487 | Val Acc: 0.8856\n",
      "Época 36/80 | Train Loss: 0.2291 | Train Acc: 0.9211 | Val Loss: 0.3549 | Val Acc: 0.8839\n",
      "Época 37/80 | Train Loss: 0.2251 | Train Acc: 0.9224 | Val Loss: 0.3436 | Val Acc: 0.8888\n",
      "Época 38/80 | Train Loss: 0.2202 | Train Acc: 0.9258 | Val Loss: 0.3354 | Val Acc: 0.8899\n",
      "Época 39/80 | Train Loss: 0.2168 | Train Acc: 0.9254 | Val Loss: 0.3445 | Val Acc: 0.8899\n",
      "Época 40/80 | Train Loss: 0.2188 | Train Acc: 0.9253 | Val Loss: 0.3365 | Val Acc: 0.8883\n",
      "Época 41/80 | Train Loss: 0.2102 | Train Acc: 0.9277 | Val Loss: 0.3400 | Val Acc: 0.8873\n",
      "Época 42/80 | Train Loss: 0.2095 | Train Acc: 0.9291 | Val Loss: 0.3495 | Val Acc: 0.8845\n",
      "Época 43/80 | Train Loss: 0.1916 | Train Acc: 0.9345 | Val Loss: 0.3238 | Val Acc: 0.8939\n",
      "Época 44/80 | Train Loss: 0.1799 | Train Acc: 0.9391 | Val Loss: 0.3220 | Val Acc: 0.8918\n",
      "Época 45/80 | Train Loss: 0.1764 | Train Acc: 0.9402 | Val Loss: 0.3268 | Val Acc: 0.8927\n",
      "Época 46/80 | Train Loss: 0.1796 | Train Acc: 0.9398 | Val Loss: 0.3239 | Val Acc: 0.8959\n",
      "Época 47/80 | Train Loss: 0.1753 | Train Acc: 0.9416 | Val Loss: 0.3207 | Val Acc: 0.8936\n",
      "Época 48/80 | Train Loss: 0.1709 | Train Acc: 0.9425 | Val Loss: 0.3219 | Val Acc: 0.8960\n",
      "Época 49/80 | Train Loss: 0.1716 | Train Acc: 0.9426 | Val Loss: 0.3273 | Val Acc: 0.8949\n",
      "Época 50/80 | Train Loss: 0.1693 | Train Acc: 0.9435 | Val Loss: 0.3182 | Val Acc: 0.8954\n",
      "Época 51/80 | Train Loss: 0.1664 | Train Acc: 0.9442 | Val Loss: 0.3252 | Val Acc: 0.8946\n",
      "Época 52/80 | Train Loss: 0.1684 | Train Acc: 0.9431 | Val Loss: 0.3300 | Val Acc: 0.8964\n",
      "Época 53/80 | Train Loss: 0.1657 | Train Acc: 0.9437 | Val Loss: 0.3282 | Val Acc: 0.8940\n",
      "Época 54/80 | Train Loss: 0.1620 | Train Acc: 0.9458 | Val Loss: 0.3259 | Val Acc: 0.8946\n",
      "Época 55/80 | Train Loss: 0.1501 | Train Acc: 0.9498 | Val Loss: 0.3189 | Val Acc: 0.8966\n",
      "Época 56/80 | Train Loss: 0.1504 | Train Acc: 0.9494 | Val Loss: 0.3206 | Val Acc: 0.8962\n",
      "Época 57/80 | Train Loss: 0.1494 | Train Acc: 0.9492 | Val Loss: 0.3158 | Val Acc: 0.8992\n",
      "Época 58/80 | Train Loss: 0.1456 | Train Acc: 0.9520 | Val Loss: 0.3139 | Val Acc: 0.8968\n",
      "Época 59/80 | Train Loss: 0.1465 | Train Acc: 0.9522 | Val Loss: 0.3174 | Val Acc: 0.8966\n",
      "Época 60/80 | Train Loss: 0.1456 | Train Acc: 0.9520 | Val Loss: 0.3170 | Val Acc: 0.8994\n",
      "Época 61/80 | Train Loss: 0.1431 | Train Acc: 0.9522 | Val Loss: 0.3186 | Val Acc: 0.8972\n",
      "Época 62/80 | Train Loss: 0.1412 | Train Acc: 0.9529 | Val Loss: 0.3196 | Val Acc: 0.8958\n",
      "Época 63/80 | Train Loss: 0.1340 | Train Acc: 0.9555 | Val Loss: 0.3156 | Val Acc: 0.9001\n",
      "Época 64/80 | Train Loss: 0.1325 | Train Acc: 0.9575 | Val Loss: 0.3129 | Val Acc: 0.8985\n",
      "Época 65/80 | Train Loss: 0.1321 | Train Acc: 0.9575 | Val Loss: 0.3123 | Val Acc: 0.9008\n",
      "Época 66/80 | Train Loss: 0.1333 | Train Acc: 0.9568 | Val Loss: 0.3111 | Val Acc: 0.9007\n",
      "Época 67/80 | Train Loss: 0.1303 | Train Acc: 0.9576 | Val Loss: 0.3154 | Val Acc: 0.8998\n",
      "Época 68/80 | Train Loss: 0.1301 | Train Acc: 0.9578 | Val Loss: 0.3130 | Val Acc: 0.8996\n",
      "Época 69/80 | Train Loss: 0.1308 | Train Acc: 0.9567 | Val Loss: 0.3120 | Val Acc: 0.8996\n",
      "Época 70/80 | Train Loss: 0.1305 | Train Acc: 0.9586 | Val Loss: 0.3150 | Val Acc: 0.8987\n",
      "Época 71/80 | Train Loss: 0.1279 | Train Acc: 0.9585 | Val Loss: 0.3124 | Val Acc: 0.8998\n",
      "Época 72/80 | Train Loss: 0.1262 | Train Acc: 0.9587 | Val Loss: 0.3113 | Val Acc: 0.9004\n",
      "Época 73/80 | Train Loss: 0.1236 | Train Acc: 0.9592 | Val Loss: 0.3107 | Val Acc: 0.9004\n",
      "Época 74/80 | Train Loss: 0.1241 | Train Acc: 0.9605 | Val Loss: 0.3115 | Val Acc: 0.9005\n",
      "Época 75/80 | Train Loss: 0.1248 | Train Acc: 0.9603 | Val Loss: 0.3109 | Val Acc: 0.9002\n",
      "Época 76/80 | Train Loss: 0.1247 | Train Acc: 0.9588 | Val Loss: 0.3119 | Val Acc: 0.9022\n",
      "Época 77/80 | Train Loss: 0.1233 | Train Acc: 0.9596 | Val Loss: 0.3117 | Val Acc: 0.9003\n",
      "Época 78/80 | Train Loss: 0.1220 | Train Acc: 0.9603 | Val Loss: 0.3124 | Val Acc: 0.9001\n",
      "Época 79/80 | Train Loss: 0.1221 | Train Acc: 0.9598 | Val Loss: 0.3088 | Val Acc: 0.9011\n",
      "Época 80/80 | Train Loss: 0.1198 | Train Acc: 0.9621 | Val Loss: 0.3100 | Val Acc: 0.9008\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1054a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado correctamente a cifar10cnn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34140/2274461777.py:8: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "# Exportar modelo a ONNX (compatibilidad PyTorch <=2.2)\n",
    "import torch.onnx\n",
    "\n",
    "model.eval()\n",
    "model_cpu = model.to('cpu').to(memory_format=torch.contiguous_format)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    dummy_input,\n",
    "    \"cifar10cnn.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    dynamo=False,  # fuerza el exportador estable\n",
    "    training=torch.onnx.TrainingMode.EVAL\n",
    ")\n",
    "\n",
    "print(\"Modelo exportado correctamente a cifar10cnn.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
