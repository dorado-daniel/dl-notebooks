{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad4aa8",
   "metadata": {},
   "source": [
    "## More Tunning\n",
    "\n",
    "Aqui hemos tocado más cosas:\n",
    "\n",
    "- Duplicado los parametros y añadido otra capa CONV\n",
    "\n",
    "    4 capas convolucionales: 3→64→128→256→512 canales (duplicando los filtros)\n",
    "\n",
    "- Añadimos BatchNorm2d después de cada capa convolucional (reduce covariant shift)\n",
    "- Cambiado el scheduler a ReduceLROnPlateau, que reduce el LR en vase al val_acc automáticamente\n",
    "- Usamos Global Average Pooling (AdaptiveAvgPool2d) en lugar de Flatten\n",
    "- Clasificador simplificado: solo Linear(512→10)\n",
    "- Solo Dropout(0.3) después de la 3ª capa convolucional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb06d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "tensor([0.4914, 0.4822, 0.4465], dtype=torch.float64) tensor([0.2470, 0.2435, 0.2616], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "TENSORBOARD_EXP = f\"runs/cifar10_cnn_step20_lr0015_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Obtenemos el dataset train completo\n",
    "loader_train = DataLoader(train_full, batch_size=len(train_full), shuffle=False)                       \n",
    "#Obtenemos un batch de datos\n",
    "imgs, _ = next(iter(loader_train))             \n",
    "imgs = imgs.to(torch.float64)\n",
    "print(imgs.shape) # [50000,3,32,32]\n",
    "\n",
    "CIFAR10_MEAN = imgs.mean(dim=(0,2,3))\n",
    "CIFAR10_STD  = imgs.std(dim=(0,2,3))\n",
    "print(CIFAR10_MEAN, CIFAR10_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Appx: 0): tensor([ 6.3542e-08, -1.9292e-08, -4.8446e-08])\n",
      "Std Appx: 1): tensor([1.0000, 1.0000, 1.0000])\n",
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "#CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el transform para data augmentation y normalización de TRAIN_SET\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "    \n",
    "])\n",
    "\n",
    "#Creamos el transform para normalización de TRAIN_SET y EVAL sin aug\n",
    "no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN.tolist(), CIFAR10_STD.tolist())\n",
    "])\n",
    "\n",
    "# Re-creamos los datasets con el nuevo transform + uno nuevo para validacion y que no pase por aug\n",
    "train_full_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "train_full_no_aug = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=no_aug)\n",
    "\n",
    "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=no_aug)\n",
    "\n",
    "\n",
    "#Comprobamos que los datos estan normalizados\n",
    "check_loader_train_full = DataLoader(train_full_no_aug, batch_size=len(train_full_no_aug), shuffle=False) #Dataset completo para calcular la media y la desviación estándar de los datos ya normalizados\n",
    "x, _ = next(iter(check_loader_train_full))\n",
    "mean_check = x.mean(dim=(0, 2, 3))\n",
    "std_check  = x.std(dim=(0, 2, 3))\n",
    "print(\"Mean Appx: 0):\", mean_check)\n",
    "print(\"Std Appx: 1):\", std_check)\n",
    "assert mean_check.abs().max() < 0.05\n",
    "assert (std_check - 1).abs().max() < 0.05\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos el dataset en train y validation para no_aug\n",
    "train_set, val_set = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_no_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Dividimos el dataset en train y validation para AUG\n",
    "train_set_aug, val_set_aug = torch.utils.data.random_split(generator=torch.Generator().manual_seed(3), dataset=train_full_aug, lengths=[40000, 10000])\n",
    "\n",
    "#Comprobamos que el dataset se ha dividido correctamente\n",
    "print(len(train_set), len(val_set))\n",
    "assert train_set.indices == train_set_aug.indices\n",
    "assert val_set.indices   == val_set_aug.indices\n",
    "\n",
    "\n",
    "#Nuestros loaders para entrenar, validar y testear, ya normalizados.\n",
    "\n",
    "#Vamos a mejorar rendimiento de la GPU\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "loader_train = DataLoader(train_set_aug, batch_size=128, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2) #Barajamos porque es train y mejora la generalización\n",
    "loader_val = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "loader_test = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69f483",
   "metadata": {},
   "source": [
    "Vamos ahora a coger el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4857847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(loader_train))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66939dd6",
   "metadata": {},
   "source": [
    "# Declaración de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ba618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Dropout(p=0.3, inplace=False)\n",
      "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_flatten_size(model_features, input_shape=(1, 3, 32, 32)):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(input_shape)\n",
    "        out = model_features(x)\n",
    "        return out.view(out.size(0), -1).size(1)\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            #Primera capa de convolucion, entra una imagen de 3 canales y sale una imagen de 32 features\n",
    "            #El kernel es de 3x3 y el padding es 1 (same padding)\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            \n",
    "            #Segunda capa de convolucion, entra una imagen de 16 features y sale una imagen de 32 features\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            \n",
    "            #Tercera capa de convolucion, entra una imagen de 32 features y sale una imagen de 64 features\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "            nn.Dropout(p=0.3), #Aplicamos dropout para evitar el overfitting\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(), #Aplicamos la funcion de activacion ReLU\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #Aplicamos pooling max para reducir la dimensionalidad de la imagen\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instancia\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = CIFAR10CNN()\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bd307e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
      "            Conv2d-9            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-10            [-1, 256, 8, 8]             512\n",
      "             ReLU-11            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-12            [-1, 256, 4, 4]               0\n",
      "          Dropout-13            [-1, 256, 4, 4]               0\n",
      "           Conv2d-14            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-15            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-16            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-17            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-18            [-1, 512, 1, 1]               0\n",
      "           Linear-19                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 1,557,066\n",
      "Trainable params: 1,557,066\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.08\n",
      "Params size (MB): 5.94\n",
      "Estimated Total Size (MB): 9.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45588c0e",
   "metadata": {},
   "source": [
    "### Vamos a declarar el bucle de entramiento\n",
    "\n",
    "Vamos a recordar los parametros tipicos de SDG\n",
    "\n",
    "| Parámetro  | Rol                                                      | Valor típico |\n",
    "| ---------- | -------------------------------------------------------- | ------------ |\n",
    "| `lr`       | tamaño del paso (cuánto cambian los pesos por gradiente) | 0.01–0.1     |\n",
    "| `momentum` | cuánto “recuerda” del gradiente anterior                 | 0.8–0.95     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU()\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TorchMetrics\n",
    "NUM_CLASSES = 10  # CIFAR-10\n",
    "\n",
    "precision_metric = MulticlassPrecision(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "recall_metric    = MulticlassRecall(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "f1_metric        = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "#Definimos los parametros\n",
    "EPOCHS = 80  # número de pasadas por el dataset\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "#Definimos la funcion de perdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay=5e-4\n",
    "#lr es la tasa de aprendizaje, momentum es el factor de inercia, es decir, cuanto se mueve el optimizador en la direccion del gradiente y \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#DECLARAMOS EL SCHEDULER\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=1e-3)\n",
    "lrs = []\n",
    "\n",
    "model.to(device)  # mueve el modelo a GPU\n",
    "model.train()     # pone el modelo en modo entrenamiento (activa dropout, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86551f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/80 | Train Loss: 1.8128 | Train Acc: 0.3429 | Val Loss: 1.4607 | Val Acc: 0.4702\n",
      "Época 2/80 | Train Loss: 1.4809 | Train Acc: 0.4693 | Val Loss: 1.1552 | Val Acc: 0.5936\n",
      "Época 3/80 | Train Loss: 1.3300 | Train Acc: 0.5245 | Val Loss: 1.0840 | Val Acc: 0.6166\n",
      "Época 4/80 | Train Loss: 1.2420 | Train Acc: 0.5575 | Val Loss: 0.9177 | Val Acc: 0.6750\n",
      "Época 5/80 | Train Loss: 1.1525 | Train Acc: 0.5923 | Val Loss: 0.9122 | Val Acc: 0.6809\n",
      "Época 6/80 | Train Loss: 1.1015 | Train Acc: 0.6114 | Val Loss: 0.8097 | Val Acc: 0.7179\n",
      "Época 7/80 | Train Loss: 1.0597 | Train Acc: 0.6271 | Val Loss: 0.8401 | Val Acc: 0.7129\n",
      "Época 8/80 | Train Loss: 1.0346 | Train Acc: 0.6351 | Val Loss: 0.7514 | Val Acc: 0.7440\n",
      "Época 9/80 | Train Loss: 0.9848 | Train Acc: 0.6531 | Val Loss: 0.7535 | Val Acc: 0.7444\n",
      "Época 10/80 | Train Loss: 0.9611 | Train Acc: 0.6619 | Val Loss: 0.7247 | Val Acc: 0.7509\n",
      "Época 11/80 | Train Loss: 0.9328 | Train Acc: 0.6724 | Val Loss: 0.6899 | Val Acc: 0.7640\n",
      "Época 12/80 | Train Loss: 0.9069 | Train Acc: 0.6819 | Val Loss: 0.6876 | Val Acc: 0.7619\n",
      "Época 13/80 | Train Loss: 0.8963 | Train Acc: 0.6859 | Val Loss: 0.6647 | Val Acc: 0.7732\n",
      "Época 14/80 | Train Loss: 0.8799 | Train Acc: 0.6933 | Val Loss: 0.6178 | Val Acc: 0.7842\n",
      "Época 15/80 | Train Loss: 0.8592 | Train Acc: 0.7006 | Val Loss: 0.6274 | Val Acc: 0.7818\n",
      "Época 16/80 | Train Loss: 0.8500 | Train Acc: 0.6995 | Val Loss: 0.6068 | Val Acc: 0.7919\n",
      "Época 17/80 | Train Loss: 0.8235 | Train Acc: 0.7124 | Val Loss: 0.5843 | Val Acc: 0.8003\n",
      "Época 18/80 | Train Loss: 0.8154 | Train Acc: 0.7130 | Val Loss: 0.5718 | Val Acc: 0.8038\n",
      "Época 19/80 | Train Loss: 0.7863 | Train Acc: 0.7240 | Val Loss: 0.5610 | Val Acc: 0.8076\n",
      "Época 20/80 | Train Loss: 0.7806 | Train Acc: 0.7274 | Val Loss: 0.5493 | Val Acc: 0.8144\n",
      "Época 21/80 | Train Loss: 0.7768 | Train Acc: 0.7301 | Val Loss: 0.6014 | Val Acc: 0.7931\n",
      "Época 22/80 | Train Loss: 0.7624 | Train Acc: 0.7352 | Val Loss: 0.5522 | Val Acc: 0.8125\n",
      "Época 23/80 | Train Loss: 0.7471 | Train Acc: 0.7385 | Val Loss: 0.5210 | Val Acc: 0.8197\n",
      "Época 24/80 | Train Loss: 0.7499 | Train Acc: 0.7399 | Val Loss: 0.5400 | Val Acc: 0.8140\n",
      "Época 25/80 | Train Loss: 0.7307 | Train Acc: 0.7461 | Val Loss: 0.4812 | Val Acc: 0.8364\n",
      "Época 26/80 | Train Loss: 0.7247 | Train Acc: 0.7482 | Val Loss: 0.5264 | Val Acc: 0.8214\n",
      "Época 27/80 | Train Loss: 0.7208 | Train Acc: 0.7481 | Val Loss: 0.4950 | Val Acc: 0.8311\n",
      "Época 28/80 | Train Loss: 0.7072 | Train Acc: 0.7525 | Val Loss: 0.5037 | Val Acc: 0.8265\n",
      "Época 29/80 | Train Loss: 0.6972 | Train Acc: 0.7575 | Val Loss: 0.4807 | Val Acc: 0.8317\n",
      "Época 30/80 | Train Loss: 0.6931 | Train Acc: 0.7587 | Val Loss: 0.4773 | Val Acc: 0.8372\n",
      "Época 31/80 | Train Loss: 0.6883 | Train Acc: 0.7630 | Val Loss: 0.4853 | Val Acc: 0.8331\n",
      "Época 32/80 | Train Loss: 0.6750 | Train Acc: 0.7627 | Val Loss: 0.4419 | Val Acc: 0.8461\n",
      "Época 33/80 | Train Loss: 0.6665 | Train Acc: 0.7671 | Val Loss: 0.5122 | Val Acc: 0.8306\n",
      "Época 34/80 | Train Loss: 0.6492 | Train Acc: 0.7720 | Val Loss: 0.4844 | Val Acc: 0.8391\n",
      "Época 35/80 | Train Loss: 0.6545 | Train Acc: 0.7740 | Val Loss: 0.4544 | Val Acc: 0.8455\n",
      "Época 36/80 | Train Loss: 0.6462 | Train Acc: 0.7759 | Val Loss: 0.4458 | Val Acc: 0.8477\n",
      "Época 37/80 | Train Loss: 0.6415 | Train Acc: 0.7762 | Val Loss: 0.4466 | Val Acc: 0.8467\n",
      "Época 38/80 | Train Loss: 0.6351 | Train Acc: 0.7772 | Val Loss: 0.4408 | Val Acc: 0.8455\n",
      "Época 39/80 | Train Loss: 0.6284 | Train Acc: 0.7822 | Val Loss: 0.4397 | Val Acc: 0.8518\n",
      "Época 40/80 | Train Loss: 0.6278 | Train Acc: 0.7823 | Val Loss: 0.4808 | Val Acc: 0.8338\n",
      "Época 41/80 | Train Loss: 0.6193 | Train Acc: 0.7850 | Val Loss: 0.4747 | Val Acc: 0.8362\n",
      "Época 42/80 | Train Loss: 0.6164 | Train Acc: 0.7889 | Val Loss: 0.4265 | Val Acc: 0.8545\n",
      "Época 43/80 | Train Loss: 0.6127 | Train Acc: 0.7887 | Val Loss: 0.4047 | Val Acc: 0.8615\n",
      "Época 44/80 | Train Loss: 0.6163 | Train Acc: 0.7856 | Val Loss: 0.4301 | Val Acc: 0.8516\n",
      "Época 45/80 | Train Loss: 0.6037 | Train Acc: 0.7922 | Val Loss: 0.4109 | Val Acc: 0.8559\n",
      "Época 46/80 | Train Loss: 0.6057 | Train Acc: 0.7898 | Val Loss: 0.4436 | Val Acc: 0.8512\n",
      "Época 47/80 | Train Loss: 0.6001 | Train Acc: 0.7912 | Val Loss: 0.4077 | Val Acc: 0.8611\n",
      "Época 48/80 | Train Loss: 0.5900 | Train Acc: 0.7969 | Val Loss: 0.4116 | Val Acc: 0.8570\n",
      "Época 49/80 | Train Loss: 0.5927 | Train Acc: 0.7942 | Val Loss: 0.4215 | Val Acc: 0.8550\n",
      "Época 50/80 | Train Loss: 0.5880 | Train Acc: 0.7963 | Val Loss: 0.4137 | Val Acc: 0.8561\n",
      "Época 51/80 | Train Loss: 0.5915 | Train Acc: 0.7968 | Val Loss: 0.3951 | Val Acc: 0.8659\n",
      "Época 52/80 | Train Loss: 0.5895 | Train Acc: 0.7959 | Val Loss: 0.4467 | Val Acc: 0.8425\n",
      "Época 53/80 | Train Loss: 0.5790 | Train Acc: 0.8002 | Val Loss: 0.4327 | Val Acc: 0.8550\n",
      "Época 54/80 | Train Loss: 0.5676 | Train Acc: 0.8031 | Val Loss: 0.3991 | Val Acc: 0.8620\n",
      "Época 55/80 | Train Loss: 0.5772 | Train Acc: 0.7978 | Val Loss: 0.4159 | Val Acc: 0.8546\n",
      "Época 56/80 | Train Loss: 0.5697 | Train Acc: 0.8025 | Val Loss: 0.4239 | Val Acc: 0.8563\n",
      "Época 57/80 | Train Loss: 0.5694 | Train Acc: 0.8036 | Val Loss: 0.3834 | Val Acc: 0.8659\n",
      "Época 58/80 | Train Loss: 0.5666 | Train Acc: 0.8030 | Val Loss: 0.4199 | Val Acc: 0.8585\n",
      "Época 59/80 | Train Loss: 0.5635 | Train Acc: 0.8045 | Val Loss: 0.3887 | Val Acc: 0.8675\n",
      "Época 60/80 | Train Loss: 0.5605 | Train Acc: 0.8072 | Val Loss: 0.3766 | Val Acc: 0.8710\n",
      "Época 61/80 | Train Loss: 0.5604 | Train Acc: 0.8071 | Val Loss: 0.3937 | Val Acc: 0.8667\n",
      "Época 62/80 | Train Loss: 0.5606 | Train Acc: 0.8071 | Val Loss: 0.4002 | Val Acc: 0.8614\n",
      "Época 63/80 | Train Loss: 0.5607 | Train Acc: 0.8061 | Val Loss: 0.3963 | Val Acc: 0.8663\n",
      "Época 64/80 | Train Loss: 0.5527 | Train Acc: 0.8098 | Val Loss: 0.3948 | Val Acc: 0.8658\n",
      "Época 65/80 | Train Loss: 0.5542 | Train Acc: 0.8084 | Val Loss: 0.3846 | Val Acc: 0.8691\n",
      "Época 66/80 | Train Loss: 0.5547 | Train Acc: 0.8079 | Val Loss: 0.3731 | Val Acc: 0.8723\n",
      "Época 67/80 | Train Loss: 0.5559 | Train Acc: 0.8075 | Val Loss: 0.4005 | Val Acc: 0.8603\n",
      "Época 68/80 | Train Loss: 0.5555 | Train Acc: 0.8080 | Val Loss: 0.4051 | Val Acc: 0.8585\n",
      "Época 69/80 | Train Loss: 0.5467 | Train Acc: 0.8106 | Val Loss: 0.3795 | Val Acc: 0.8740\n",
      "Época 70/80 | Train Loss: 0.5405 | Train Acc: 0.8129 | Val Loss: 0.3865 | Val Acc: 0.8655\n",
      "Época 71/80 | Train Loss: 0.5488 | Train Acc: 0.8109 | Val Loss: 0.4034 | Val Acc: 0.8605\n",
      "Época 72/80 | Train Loss: 0.5445 | Train Acc: 0.8129 | Val Loss: 0.3847 | Val Acc: 0.8695\n",
      "Época 73/80 | Train Loss: 0.5460 | Train Acc: 0.8108 | Val Loss: 0.3740 | Val Acc: 0.8728\n",
      "Época 74/80 | Train Loss: 0.5362 | Train Acc: 0.8155 | Val Loss: 0.4114 | Val Acc: 0.8625\n",
      "Época 75/80 | Train Loss: 0.5391 | Train Acc: 0.8130 | Val Loss: 0.3720 | Val Acc: 0.8710\n",
      "Época 76/80 | Train Loss: 0.5296 | Train Acc: 0.8159 | Val Loss: 0.3795 | Val Acc: 0.8709\n",
      "Época 77/80 | Train Loss: 0.5333 | Train Acc: 0.8157 | Val Loss: 0.3757 | Val Acc: 0.8714\n",
      "Época 78/80 | Train Loss: 0.5338 | Train Acc: 0.8153 | Val Loss: 0.3711 | Val Acc: 0.8733\n",
      "Época 79/80 | Train Loss: 0.5328 | Train Acc: 0.8161 | Val Loss: 0.3655 | Val Acc: 0.8737\n",
      "Época 80/80 | Train Loss: 0.5319 | Train Acc: 0.8163 | Val Loss: 0.4098 | Val Acc: 0.8649\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_EXP)  # <-- añadido\n",
    "\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(model, loader_val, device, criterion)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_val:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "\n",
    "    precision_val = precision_metric.compute().item()\n",
    "    recall_val    = recall_metric.compute().item()\n",
    "    f1_val        = f1_metric.compute().item()\n",
    "\n",
    "    writer.add_scalar(\"Val/Precision_macro\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Val/Recall_macro\",    recall_val,  epoch)\n",
    "    writer.add_scalar(\"Val/F1_macro\",        f1_val,      epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Época {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    #Escribimos los resultados de cada epoch en tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",  val_loss,  epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  train_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/val\",   val_acc,   epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.flush()  # <-- fuerza escritura inmediata para ver en tiempo real\n",
    "\n",
    "#Evaluamos el modelo en el test set y añadimos los resultados a tensorboard\n",
    "test_loss, test_acc = evaluate(model, loader_test, device, criterion)\n",
    "\n",
    "hparams = {\n",
    "    'model': model.__class__.__name__,\n",
    "    'seed': 3,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'lr_init': float(lr),\n",
    "    'momentum': float(momentum),\n",
    "    'batch_size': int(loader_train.batch_size),\n",
    "    'weight_decay': float(optimizer.param_groups[0].get('weight_decay', 0.0)),\n",
    "    'scheduler': type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'metrics/test_acc': float(test_acc),\n",
    "    'metrics/test_loss': float(test_loss),\n",
    "    'metrics/val_acc_last': float(val_accuracies[-1]),\n",
    "    'metrics/val_loss_last': float(val_losses[-1]),\n",
    "    'metrics/train_acc_last': float(train_accuracies[-1]),\n",
    "    'metrics/train_loss_last': float(train_losses[-1]),\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparams, metrics)\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
