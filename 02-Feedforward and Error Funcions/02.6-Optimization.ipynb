{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5956405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formas:\n",
      "X: torch.Size([3, 2]) | W: torch.Size([1, 2]) | b: torch.Size([1]) | y_true: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# === 1. Datos de entrada ===\n",
    "X = torch.tensor([\n",
    "    [2.0, 1.0],   # muestra 1\n",
    "    [5.0, 1.0],   # muestra 2\n",
    "    [1.0, 3.0]    # muestra 3\n",
    "], device=device)\n",
    "\n",
    "# === 2. Etiquetas verdaderas (objetivos) ===\n",
    "# Digamos que la salida esperada (regresión simple) es:\n",
    "y_true = torch.tensor([[5.0], [10.0], [7.0]], device=device)\n",
    "\n",
    "# === 3. Pesos y bias iniciales (entrenables) ===\n",
    "W = torch.tensor([[0.5, 1.0]], device=device, requires_grad=True)  # (1, 2)\n",
    "b = torch.tensor([0.0], device=device, requires_grad=True)          # (1,)\n",
    "\n",
    "# === 4. Definir la función de pérdida ===\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"Formas:\")\n",
    "print(\"X:\", X.shape, \"| W:\", W.shape, \"| b:\", b.shape, \"| y_true:\", y_true.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c995c5",
   "metadata": {},
   "source": [
    "## Aplicando (!Stochastic) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 5. Definir el optimizador ===\n",
    "#aqui usamos SGD (gradiente descendente estocástico) \n",
    "# con una tasa de aprendizaje de 0.05\n",
    "# estocastico = aleatorio\n",
    "# el GD que no es estocastico usa todo el dataset para actualizar los pesos\n",
    "# pero es menos eficiente computacionalmente\n",
    "\n",
    "#De todas formas aqui no estamos usando mini-batches,\n",
    "# asi que en este caso SGD y GD serian equivalentes\n",
    "# NO HAY ALEATORIEDAD EN ESTE EJEMPLO\n",
    "#PARA ELLO TENDRIAMOS QUE USAR MINI-BATCHES (MAS ADELANTE)\n",
    "\n",
    "#Se suele usar aleatoriedad porque\n",
    "#se corre el riesgo de quedar atrapado en minimos locales\n",
    "\n",
    "# Usar SGD sin definir mini-batches, es usar GD.\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
